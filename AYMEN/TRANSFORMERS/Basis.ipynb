{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ee2ce3",
   "metadata": {},
   "source": [
    "# Est ce qu'on peut utiliser des méthodes de sélection de variables avec des transformers ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3faa252",
   "metadata": {},
   "source": [
    "**Aymen. B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b12b95",
   "metadata": {},
   "source": [
    "## Références"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783fa09",
   "metadata": {},
   "source": [
    "[Comprendre les Transformers en 10 minutes | Alexandre TL](https://www.youtube.com/watch?v=46XbjplgwOw)\n",
    "\n",
    "[Transformers : têtes d'attention et couches | Alexandre TL](https://www.youtube.com/watch?v=Xe--F4RGN_0)\n",
    "\n",
    "[Architecture encodeur-décodeur du Transformer | Alexandre TL](https://www.youtube.com/watch?v=lCMHkS1KT-c)\n",
    "\n",
    "Je me base sur l'article Learned Token Pruning for Transformers de **Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c42ed",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b45b8f",
   "metadata": {},
   "source": [
    "Un simple transformer apprend déjà à ignorer les groupes de variables inutiles grâce à l’attention.\n",
    "\n",
    "Mais même si un token reçoit un poids d’attention quasi nul, il participe quand-même à toutes les multiplications ; la complexité reste inchangée.\n",
    "\n",
    "Les poids d’attention ne sont pas des explications fiables ; on peut réarranger l’attention sans changer la prédiction. Un masque dur (« cette colonne est supprimée ») produit une liste claire de variables utiles.\n",
    "\n",
    "En éliminant les tokens peu pertinents, les modèles évitent de suranalyser des informations insignifiantes, réduisant ainsi les réponses incohérentes ou inventées.\n",
    "\n",
    "La réduction de tokens aide les modèles à se concentrer sur les informations essentielles dans de longues entrées, préservant ainsi la cohérence contextuelle.\n",
    "\n",
    "En filtrant les tokens bruyants ou non informatifs dès la phase d'entraînement, les modèles peuvent converger plus rapidement et de manière plus stable."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

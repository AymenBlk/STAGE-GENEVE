{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f97a35",
   "metadata": {},
   "source": [
    "# Quantile universal threshold\n",
    "---\n",
    "**Aymen.B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a804c2c",
   "metadata": {},
   "source": [
    "Je me repose sur l'article *\"Quantile universal threshold: model selection at the detection edge for high-dimensional linear regression\"* de **Jairo Diaz Rodriguez** et **Sylvain Sardy**\n",
    "\n",
    "Ainsi que sur celui-ci *\"Quantile universal threshold for model selection\"* de **Caroline Giacobino**, **Sylvain Sardy**, **Jairo Diaz Rodriguez** et **Nick Hengartner**\n",
    "\n",
    "et enfin ce dernier *\"Training a neural network for data interpretation and better generalization: towards intelligent artificial intelligence\"* de **Sylvain Sardy**, **Maxime van Custsem** et **Xiaoyu Ma**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d34ab6",
   "metadata": {},
   "source": [
    "## Théorie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca2ba6",
   "metadata": {},
   "source": [
    "### La fonction zero-thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a2441e",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    ">**Définition 1.** \n",
    ">\n",
    ">Supposons que $Y \\sim f_{\\xi_0}$. Un estimateur $\\hat{\\xi}_\\lambda(Y)$ indexé par $\\lambda \\geq 0$ est appelé un estimateur thresholding si\n",
    ">\n",
    ">$$\n",
    ">\\mathbb{P}(\\hat{\\xi}_\\lambda(Y) = 0) > 0 \\quad \\text{pour un certain } \\lambda \\text{ fini}.\n",
    ">$$\n",
    "\n",
    "Notez que la régression ridge n'est pas un estimateur thresholding car $\\hat{\\xi}_\\lambda(y) \\neq 0$ pour tout $\\lambda$ fini et tout $\\mathbf{y} \\in \\mathbb{R}^n$. C’est dû au fait que la pénalité $\\ell_2$ ne force pas de composantes à zéro.\n",
    "\n",
    ">**Définition 2.** \n",
    ">\n",
    ">Un estimateur thresholding $\\hat{\\xi}_\\lambda(Y)$ admet une fonction zero-thresholding $\\lambda_0(Y)$ si\n",
    ">\n",
    ">$$\n",
    ">\\hat{\\xi}_\\lambda(Y) = 0 \\quad \\Leftrightarrow \\quad \\lambda \\geq \\lambda_0(Y) \\quad \\text{presque partout}.\n",
    ">$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63d099",
   "metadata": {},
   "source": [
    ">**Définition 3.**  \n",
    ">\n",
    ">On définit la **fonction de zero-thresholding locale** $\\lambda_0^{\\text{local}}(Y)$ étant le plus petit $\\lambda$ tel que la solution nulle soit **un minimum local** du critère régularisé (et non forcément global $\\lambda_0(Y)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8252ba0",
   "metadata": {},
   "source": [
    "Dans le cas convexe, on a $\\boxed{\\lambda_0^{\\text{local}}(Y) = \\lambda_0(Y)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb148cc",
   "metadata": {},
   "source": [
    "#### Trouver $\\lambda_0(\\cdot)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170c2b7",
   "metadata": {},
   "source": [
    "Donc $\\lambda_0(Y)$ est le plus petit $\\lambda$ pour lequel la solution $\\hat{\\xi}_\\lambda(Y) = 0$\n",
    "\n",
    "Il s'agit donc d'identifier la plus petite valeur $\\lambda$ telle que l’estimateur annule tous les paramètres, c’est-à-dire que la solution $\\hat{\\xi}_\\lambda(Y) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9153d51a",
   "metadata": {},
   "source": [
    "##### **Exemple LASSO :** ($\\xi = \\beta$)\n",
    "\n",
    "On cherche pour quel $\\lambda$ la solution du problème  \n",
    "$$\n",
    "\\hat{\\beta}_\\lambda(y) = \\arg\\min_{\\beta}\\frac{1}{2n}\\|y-X\\beta\\|_2^2+\\lambda\\|\\beta\\|_1\n",
    "$$\n",
    "devient nulle.\n",
    "\n",
    "Calcul du gradient au point $\\beta=0$ :\n",
    "\n",
    "(ici la sous-différentielle car pas possible d'avoir le gradient partout)\n",
    "$$\n",
    "-\\frac{1}{n} X^\\top y + \\lambda \\cdot \\partial \\|\\beta\\|_1\n",
    "$$\n",
    "\n",
    "Pour que $\\hat{\\beta}_\\lambda(y) = 0$ soit minimum, il faut que :\n",
    "\n",
    "(Quand la fonction n’est pas différentiable partout, on remplace la condition\n",
    "“le gradient est nul” par la condition “0 est dans la sous-différentielle”)\n",
    "$$\n",
    "0 \\in -\\frac{1}{n} X^\\top y + \\lambda \\cdot \\partial \\|\\beta\\|_1 \\quad \\text{en} \\quad \\beta=0\n",
    "$$\n",
    "Soit\n",
    "$$\n",
    "\\frac{1}{n}X^\\top y \\in \\lambda \\cdot \\partial \\|\\beta\\|_1\n",
    "$$\n",
    "Or,\n",
    "$$\\|\\beta\\|_1=\\sum_{j=1}^p|\\beta_j|$$\n",
    "et\n",
    "$$\\partial|x|= \\text{Cste} \\in [-1,1] \\quad \\text{avec la sous-différentiel au point} \\quad \\beta=0$$\n",
    "Donc :\n",
    "$$\\partial\\|\\beta\\|_1=\\{v\\in\\mathbb{R}^p:v_j\\in[-1,1]\\text{ pour tout }j\\} \\quad \\text{au point} \\quad \\beta = 0$$\n",
    "\n",
    "Ainsi :\n",
    "$$\n",
    "\\frac{1}{n}X^\\top y\\in\\lambda\\cdot\\partial\\|\\beta\\|_1 \\quad\\Longleftrightarrow\\quad  \\text{chaque composante de }\\frac{1}{n}X^\\top y\\text{ est dans }[-\\lambda,\\lambda] \n",
    "$$\n",
    "$$\\quad\\Longleftrightarrow\\quad \\left\\|\\frac{1}{n}X^\\top y\\right\\|_\\infty\\leq\\lambda$$\n",
    "    \n",
    "Donc :\n",
    "$$\n",
    "\\lambda_0(y) = \\left\\| \\frac{1}{n} X^\\top y \\right\\|_\\infty\n",
    "$$\n",
    "\n",
    "[C'est quoi la sous-différentiel ?](https://fr.wikipedia.org/wiki/Sous-diff%C3%A9rentiel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc8fdd6",
   "metadata": {},
   "source": [
    "### Le \"Quantile Universal Threshold\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b5510a",
   "metadata": {},
   "source": [
    ">**Définition 4.**\n",
    ">\n",
    ">Toutes les pénalités ne permettent pas d’appliquer la méthode du QUT. \n",
    ">\n",
    ">On dit qu’une pénalité est **QUT-compatible** si la fonction $\\lambda_0(Y)$ (ou $\\lambda_0^{\\text{local}}(Y)$) n’est pas constante. (ni toujours nulle ou infinie)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a70fa89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    ">**Définition 5.** \n",
    ">\n",
    ">Supposons que $Y \\sim f_{\\xi_0}$ et soit $\\hat{\\xi}_\\lambda(Y)$ un estimateur avec une fonction zero-thresholding associée $\\lambda_0(\\cdot)$. \n",
    ">\n",
    ">Posons $\\Lambda := \\lambda_0(\\mathbf{Y}_0)$, où $\\mathbf{Y}_0 \\sim f_{\\xi_0}$ sous le modèle nul avec le paramètre $\\xi_0 = 0$. \n",
    ">\n",
    ">Le quantile universal threshold $\\lambda^{\\mathrm{QUT}}$ est le quantile supérieur $(1-\\alpha)$ de la statistique de thresholding $\\Lambda$, à savoir\n",
    ">\n",
    ">$$\n",
    ">\\lambda^{\\mathrm{QUT}} := F^{-1}_\\Lambda(1-\\alpha).\n",
    ">$$\n",
    "\n",
    ">Dans le cas non convexe, on utilise la fonction $\\lambda_0^{\\text{local}}(Y)$ à la place, ce qui donne :\n",
    ">$$\n",
    ">\\lambda^{\\mathrm{QUT}} := F^{-1}_{\\Lambda^{\\text{local}}}(1 - \\alpha)\n",
    ">$$\n",
    ">avec $\\Lambda^{\\text{local}} = \\lambda_0^{\\text{local}}(Y_0)$.\n",
    "\n",
    "\n",
    "On recommande de choisir $\\alpha=O(1/\\sqrt{\\log P})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a53acd4",
   "metadata": {},
   "source": [
    "### Statistique pivotal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7422a4",
   "metadata": {},
   "source": [
    ">**Définition 6.**\n",
    ">\n",
    ">Une statistique $\\Lambda(Y)$ est dite pivotal si sa loi ne dépend d’aucun paramètre inconnu du modèle. (comme $\\sigma^2$ ou $\\beta$ par exemple)\n",
    "\n",
    "Le QUT repose sur le fait qu’on peut simuler la loi de $\\lambda_0(Y)$ sous le modèle nul $Y \\sim f_{\\xi_0}$ avec $\\xi_0 = 0$, pour en extraire un quantile seuil.\n",
    "\n",
    "Mais pour que cette simulation soit possible, il faut que la distribution de $\\Lambda$ soit connue ou du moins indépendante des paramètres inconnus.\n",
    "\n",
    "Sinon on se retrouve obligé d'estimer ces paramètres inconnus et on perd le caractère vraiment universel de la méthode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3154f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15510a09",
   "metadata": {},
   "source": [
    "Pour savoir si notre $\\lambda_{\\text{QUT}}$ est bien estimé. On peut s'interesser aux taux de sélection sous le modèle nul.\n",
    "\n",
    "Autrement dit, on estime :\n",
    "$$\n",
    "\\hat{\\alpha} = \\mathbb{P}_{H_0} \\left( \\hat{\\xi}_{\\lambda_{\\mathrm{QUT}}}(Y) \\neq 0 \\right)\n",
    "$$\n",
    "Ce taux doit être proche de $\\alpha$ en théorie !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09ffec7",
   "metadata": {},
   "source": [
    "## Algorithmes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de02a9",
   "metadata": {},
   "source": [
    "### Cas du LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da2f52",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boxed{\n",
    "\\begin{array}{ll}\n",
    "\\textbf{QUT Simulation Algorithm (LASSO, oracle)} \\\\\n",
    "\\\\\n",
    "\\textbf{Step 0.} & \\text{Fixer } M \\in \\mathbb{N} \\text{ (nombre de simulations), } \\alpha \\in (0, 1), \\text{ et connaître } \\sigma^2. \\\\\n",
    "& \\text{Initialiser un vecteur } \\Lambda \\leftarrow \\emptyset. \\\\\n",
    "\\\\\n",
    "\\textbf{Step m.} & \\text{Pour } m = 1 \\text{ à } M : \\\\\n",
    "& \\quad \\text{Simuler } Y_{\\text{Sim}}^{(m)} \\sim \\mathcal{N}(0, \\sigma^2 I_n). \\\\\n",
    "& \\quad \\text{Calculer } \\lambda_0^{(m)} \\leftarrow \\left\\| \\frac{1}{n} X^\\top Y_{\\text{Sim}}^{(m)} \\right\\|_\\infty. \\\\\n",
    "& \\quad \\text{Ajouter } \\lambda_0^{(m)} \\text{ à } \\Lambda. \\\\\n",
    "\\\\\n",
    "\\textbf{Final Step.} & \\text{Définir le seuil } \\lambda^{\\text{QUT}} \\leftarrow \\text{quantile}_{1 - \\alpha}(\\Lambda).\n",
    "\\end{array}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a39bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_tools import qut_lasso_oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee28da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/30] alpha_hat = 0.0390\n",
      "[2/30] alpha_hat = 0.0570\n",
      "[3/30] alpha_hat = 0.0560\n",
      "[4/30] alpha_hat = 0.0690\n",
      "[5/30] alpha_hat = 0.0380\n",
      "[6/30] alpha_hat = 0.0500\n",
      "[7/30] alpha_hat = 0.0390\n",
      "[8/30] alpha_hat = 0.0450\n",
      "[9/30] alpha_hat = 0.0690\n",
      "[10/30] alpha_hat = 0.0480\n",
      "[11/30] alpha_hat = 0.0550\n",
      "[12/30] alpha_hat = 0.0390\n",
      "[13/30] alpha_hat = 0.0470\n",
      "[14/30] alpha_hat = 0.0460\n",
      "[15/30] alpha_hat = 0.0480\n",
      "[16/30] alpha_hat = 0.0550\n",
      "[17/30] alpha_hat = 0.0480\n",
      "[18/30] alpha_hat = 0.0420\n",
      "[19/30] alpha_hat = 0.0480\n",
      "[20/30] alpha_hat = 0.0710\n",
      "[21/30] alpha_hat = 0.0470\n",
      "[22/30] alpha_hat = 0.0690\n",
      "[23/30] alpha_hat = 0.0610\n",
      "[24/30] alpha_hat = 0.0480\n",
      "[25/30] alpha_hat = 0.0510\n",
      "[26/30] alpha_hat = 0.0480\n",
      "[27/30] alpha_hat = 0.0550\n",
      "[28/30] alpha_hat = 0.0490\n",
      "[29/30] alpha_hat = 0.0560\n",
      "[30/30] alpha_hat = 0.0440\n",
      "\n",
      "→ Moyenne de alpha_hat : 0.0512 ± 0.0091 (attendu ≈ 0.05)\n",
      "Temps total : 2.7 s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from linear_tools import ista  # adapte si nécessaire\n",
    "from time import time\n",
    "\n",
    "# --- Paramètres globaux ---\n",
    "M = 1000               # simulations par matrice\n",
    "N_matrices = 30       # nombre de matrices X\n",
    "n, p = 100, 200        # dimensions\n",
    "alpha = 0.05           # niveau cible\n",
    "sigma = 0.2            # bruit\n",
    "alpha_hats = []        # stockage des alpha estimés\n",
    "\n",
    "# --- ISTA config ---\n",
    "def grad_f(beta, y, X):\n",
    "    return -2 * X.T @ (y - X @ beta)\n",
    "\n",
    "def prox_g(beta, lmbda, L):\n",
    "    return np.sign(beta) * np.maximum(np.abs(beta) - lmbda / L, 0.0)\n",
    "\n",
    "# --- Boucle principale ---\n",
    "start = time()\n",
    "for i in range(N_matrices):\n",
    "    # Génération de X standardisé\n",
    "    X = np.random.randn(n, p)\n",
    "    X = X / np.linalg.norm(X, axis=0)\n",
    "    \n",
    "    # Estimation du seuil QUT\n",
    "    lambda_qut = qut_lasso_oracle(X, sigma=sigma, alpha=alpha, M=M, seed=i)\n",
    "    \n",
    "    # Constante de Lipschitz\n",
    "    L = np.linalg.norm(X, ord=2) ** 2\n",
    "\n",
    "    # Test du seuil sous le modèle nul\n",
    "    non_zero_count = 0\n",
    "    rng = np.random.default_rng(seed=42 + i)\n",
    "    for _ in range(M):\n",
    "        Y_sim = rng.normal(0, sigma, size=n)\n",
    "        beta0 = np.zeros(p)\n",
    "        beta_hat = ista(\n",
    "            grad_f=grad_f,\n",
    "            prox_g=prox_g,\n",
    "            x0=beta0,\n",
    "            L=L,\n",
    "            grad_f_args=(Y_sim, X),\n",
    "            prox_g_args=(lambda_qut, L),\n",
    "            max_iter=10000,\n",
    "            tol=1e-9\n",
    "        )\n",
    "        if np.any(beta_hat != 0):\n",
    "            non_zero_count += 1\n",
    "\n",
    "    alpha_hat = non_zero_count / M\n",
    "    alpha_hats.append(alpha_hat)\n",
    "    print(f\"[{i+1}/{N_matrices}] alpha_hat = {alpha_hat:.4f}\")\n",
    "\n",
    "# --- Résumé ---\n",
    "mean_alpha_hat = np.mean(alpha_hats)\n",
    "std_alpha_hat = np.std(alpha_hats)\n",
    "print(f\"\\n→ Moyenne de alpha_hat : {mean_alpha_hat:.4f} ± {std_alpha_hat:.4f} (attendu ≈ {alpha})\")\n",
    "print(f\"Temps total : {time() - start:.1f} s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

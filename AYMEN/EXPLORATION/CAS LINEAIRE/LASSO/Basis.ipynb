{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1970ec81",
   "metadata": {},
   "source": [
    "# Régularisation LASSO ($\\ell_1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fb72d",
   "metadata": {},
   "source": [
    "## Fondamentaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2110aa8",
   "metadata": {},
   "source": [
    "La régression LASSO (Least Absolute Shrinkage and Selection Operator) est une technique de régularisation qui ajoute une pénalité L1 aux coefficients du modèle. \n",
    "\n",
    "Cette méthode présente deux avantages majeurs : elle permet une sélection automatique des variables en réduisant certains coefficients exactement à zéro, tout en limitant le sur-apprentissage grâce à la pénalisation des coefficients élevés.\n",
    "\n",
    "Contrairement à la régularisation Ridge (L2) qui ne fait que réduire l'amplitude des coefficients, le LASSO peut éliminer des variables du modèle. C'est pourqoui on l'utilise pour nos modèle de régression linéaire parcimonieux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfcf62",
   "metadata": {},
   "source": [
    "Pour ce faire, on résout ce problème de minimisation :\n",
    "\n",
    "$$ \\boxed{\\min_{\\beta\\in\\mathbb{R}^p}\\frac{1}{2n}\\|y-X\\beta\\|_2^2+\\lambda\\|\\beta\\|_1}$$\n",
    "\n",
    "avec une pénalité $\\|\\beta\\|_1=\\sum_{j=1}^p|\\beta_j|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eec321",
   "metadata": {},
   "source": [
    "## Choix du $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54494b32",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f3406",
   "metadata": {},
   "source": [
    "Pour résoudre ce problème de minisation on peut utiliser dans notre cas 2 méthodes ISTA et CD (Coordinate descent) qui sont expliqué en profondeur dans les notebooks présent dans le dossier OUTILS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cf1d9",
   "metadata": {},
   "source": [
    "### ISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b770122",
   "metadata": {},
   "source": [
    "On se ramène à un problème à résoudre de cette forme :  \n",
    "$$\\min_{\\beta\\in\\mathbb{R}^p} F(\\beta) := f(\\beta) + g(\\beta)$$\n",
    "\n",
    "avec\n",
    "\n",
    "- $f(\\beta) = \\dfrac{1}{2n} \\|y - X\\beta\\|_2^2$ :  \n",
    "    Il s'agit du terme d'erreur quadratique (moindre carrés) (MSE).  \n",
    "    $f$ est une fonction convexe, différentiable, à gradient lipschitzien.\n",
    "\n",
    "- $g(\\beta) = \\lambda \\|\\beta\\|_1$ :  \n",
    "    Il s'agit du terme de pénalisation $\\ell_1$. \n",
    "    $g$ est convexe, non différentiable, mais proximable.\n",
    "\n",
    "On peut ainsi très bien utiliser ISTA pour résoudre ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391137f",
   "metadata": {},
   "source": [
    "Le gradient de la fonction $f$ se calcule comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla f(\\beta) = -\\frac{1}{n} X^\\top (y - X\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf0526",
   "metadata": {},
   "source": [
    "#### Constante L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3dfa4f",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "\n",
    "Par définition,  $L$ est tel que :\n",
    "$$\\|\\nabla f(a) - \\nabla f(b)\\|_2 \\leq L \\|a - b\\|_2, \\quad \\forall a, b \\in \\mathbb{R}^p$$\n",
    "\n",
    "Or, \n",
    "$$\\nabla f(\\beta) = -\\frac{1}{n} X^\\top (y - X\\beta)$$\n",
    "\n",
    "Donc \n",
    "$$\\|\\nabla f(a) - \\nabla f(b)\\|_2 = \\left\\| \\frac{1}{n} X^\\top X (a - b) \\right\\|_2 \\leq \\left\\| \\frac{1}{n} X^\\top X \\right\\|_2 \\|a - b\\|_2$$\n",
    "\n",
    "Ainsi, \n",
    "$$L = \\left\\| \\frac{1}{n} X^\\top X \\right\\|_2 = \\frac{1}{n} \\|X\\|_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8393b9c",
   "metadata": {},
   "source": [
    "#### Proximal de g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb014ebf",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "Dans notre cas, le proximal du terme de pénalisation $\\ell_1$ avec un pas $\\gamma = 1/L$ s'écrit :\n",
    "\n",
    "$$\n",
    "\\text{prox}_{\\frac{\\lambda}{L} \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ \\frac{L}{2} \\|x - v\\|_2^2 + \\lambda \\|x\\|_1 \\right\\}\n",
    "$$\n",
    "\n",
    "Ce proximal correspond toujours à l'opérateur de seuillage doux :\n",
    "\n",
    "$$\n",
    "\\left[\\text{prox}_{\\frac{\\lambda}{L} \\|\\cdot\\|_1}(v)\\right]_j = \\text{sign}(v_j) \\cdot \\max(|v_j| - \\frac{\\lambda}{L},\\, 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79065294",
   "metadata": {},
   "source": [
    "### Coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a48c5",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "Comme pour ISTA, on se ramène à un problème de la forme suivante :\n",
    "\n",
    "$$\\min_\\beta h(\\beta) := f(\\beta) + \\lambda \\Omega(\\beta),$$\n",
    "\n",
    "où dans notre cas :\n",
    "\n",
    "- $f : \\mathbb{R}^p \\to \\mathbb{R} = \\dfrac{1}{2n} \\|y - X\\beta\\|_2^2$\n",
    "- $\\Omega : \\mathbb{R}^p \\to \\mathbb{R}$ est la fonction de pénalisation, non lisse mais séparable :\n",
    "    - $\\Omega(\\beta) = \\sum_{j=1}^p \\Omega_j(\\beta_j)$ avec $\\Omega_j : \\mathbb{R} \\to \\mathbb{R} = |\\beta_j|$.\n",
    "- $\\lambda > 0$ est le paramètre de régularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ebc552",
   "metadata": {},
   "source": [
    "#### Constante L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736dfbb0",
   "metadata": {},
   "source": [
    "C'est la même que pour ista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1a154",
   "metadata": {},
   "source": [
    "#### Proximal de g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def9afc6",
   "metadata": {},
   "source": [
    "Dans notre cas, le proximal du terme de pénalisation $\\ell_1$ avec un pas $\\gamma = 1/L$ s'écrit :\n",
    "\n",
    "$$\n",
    "\\text{prox}_{\\frac{\\lambda}{L} \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ \\frac{L}{2} \\|x - v\\|_2^2 + \\lambda \\|x\\|_1 \\right\\}\n",
    "$$\n",
    "\n",
    "Ce proximal correspond toujours à l'opérateur de seuillage doux :\n",
    "\n",
    "$$\n",
    "\\left[\\text{prox}_{\\frac{\\lambda}{L} \\|\\cdot\\|_1}(v)\\right]_j = \\text{sign}(v_j) \\cdot \\max(|v_j| - \\frac{\\lambda}{L},\\, 0)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1970ec81",
   "metadata": {},
   "source": [
    "# Régularisation LASSO ($\\ell_1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30fb72d",
   "metadata": {},
   "source": [
    "## Fondamentaux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2110aa8",
   "metadata": {},
   "source": [
    "La régression LASSO (Least Absolute Shrinkage and Selection Operator) est une technique de régularisation qui ajoute une pénalité L1 aux coefficients du modèle. \n",
    "\n",
    "Cette méthode présente deux avantages majeurs : elle permet une sélection automatique des variables en réduisant certains coefficients exactement à zéro, tout en limitant le sur-apprentissage grâce à la pénalisation des coefficients élevés.\n",
    "\n",
    "Contrairement à la régularisation Ridge (L2) qui ne fait que réduire l'amplitude des coefficients, le LASSO peut éliminer des variables du modèle. C'est pourqoui on l'utilise pour nos modèle de régression linéaire parcimonieux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfcf62",
   "metadata": {},
   "source": [
    "Pour ce faire, on résout ce problème de minimisation :\n",
    "\n",
    "$$ \\boxed{\\min_{\\beta\\in\\mathbb{R}^p}\\frac{1}{2n}\\|y-X\\beta\\|_2^2+\\lambda\\|\\beta\\|_1}$$\n",
    "\n",
    "avec une pénalité $\\|\\beta\\|_1=\\sum_{j=1}^p|\\beta_j|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eec321",
   "metadata": {},
   "source": [
    "## Choix du $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54494b32",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f3406",
   "metadata": {},
   "source": [
    "Pour résoudre ce problème de minisation on peut utiliser dans notre cas 2 méthodes ISTA et CD (Coordinate descent) qui sont expliqué en profondeur dans les notebooks présent dans le dossier OUTILS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cf1d9",
   "metadata": {},
   "source": [
    "### ISTA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b770122",
   "metadata": {},
   "source": [
    "On se ramène à un problème à résoudre de cette forme :  \n",
    "$$\\min_{\\beta\\in\\mathbb{R}^p} F(\\beta) := f(\\beta) + g(\\beta)$$\n",
    "\n",
    "avec\n",
    "\n",
    "- $f(\\beta) = \\dfrac{1}{2n} \\|y - X\\beta\\|_2^2$ :  \n",
    "    Il s'agit du terme d'erreur quadratique (moindre carrés) (MSE).  \n",
    "    $f$ est une fonction convexe, différentiable, à gradient lipschitzien.\n",
    "\n",
    "- $g(\\beta) = \\lambda \\|\\beta\\|_1$ :  \n",
    "    Il s'agit du terme de pénalisation $\\ell_1$. \n",
    "    $g$ est convexe, non différentiable, mais proximable.\n",
    "\n",
    "On peut ainsi très bien utiliser ISTA pour résoudre ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391137f",
   "metadata": {},
   "source": [
    "Le gradient de la fonction $f$ se calcule comme suit :\n",
    "\n",
    "$$\n",
    "\\nabla f(\\beta) = -\\frac{1}{n} X^\\top (y - X\\beta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f2e4e",
   "metadata": {},
   "source": [
    "# Calucl de L\n",
    "\n",
    "# Forme du prox dans notre cas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79065294",
   "metadata": {},
   "source": [
    "### Coordinate descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe9cf9",
   "metadata": {},
   "source": [
    "# Pareil que ista"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

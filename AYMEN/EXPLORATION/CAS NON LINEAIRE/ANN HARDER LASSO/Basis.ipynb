{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed55ed1",
   "metadata": {},
   "source": [
    "# R√©seaux de neurones artificielle avec regularisation HARDER LASSO (ANN HARDER LASSO)\n",
    "**Aymen.B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021e2ef",
   "metadata": {},
   "source": [
    "## Th√©orie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc81bc",
   "metadata": {},
   "source": [
    "## D√©velopement \n",
    "\n",
    "(Je pars de la base pour arriver au mod√®le finale explicit√© dans la partie th√©orie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f5d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d2ee0",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation Adam manuelle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7d056",
   "metadata": {},
   "source": [
    "#### Ici pas de p√©nalisation la fonction de cout est simplement MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee9df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layers.append(torch.nn.Linear(in_f, out_f))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def train(self, X, Y, epochs=1000, lr=0.001, loss_fn=None, batch_size=32, shuffle=True, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        loss_function = loss_fn or torch.nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        m = [torch.zeros_like(p) for p in self.parameters()]\n",
    "        v = [torch.zeros_like(p) for p in self.parameters()]\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                t += 1\n",
    "\n",
    "                outputs = self.forward(batch_X)\n",
    "                loss = loss_function(outputs, batch_Y)\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, param in enumerate(self.parameters()):\n",
    "                        if param.grad is None:\n",
    "                            continue\n",
    "                        g = param.grad\n",
    "\n",
    "                        m[i] = beta1 * m[i] + (1 - beta1) * g\n",
    "                        v[i] = beta2 * v[i] + (1 - beta2) * (g * g)\n",
    "\n",
    "                        m_hat = m[i] / (1 - beta1 ** t)\n",
    "                        v_hat = v[i] / (1 - beta2 ** t)\n",
    "\n",
    "                        param -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c246d6e",
   "metadata": {},
   "source": [
    "#### On change de fonction co√ªt on lui rajoute une p√©nalit√© multipli√© par un coefficient lambda arbitrairement choisi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff67bd",
   "metadata": {},
   "source": [
    "En th√©orie il devrait y avoir un souci sur le choix de l'optimiseur mais ce n'est pas un soucis car pytorch reponds √† ce probl√®me avec le sous gradient de la norme $\\ell_1$ non diff√©rentiable en 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3f62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Lasso_Adam(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layers.append(torch.nn.Linear(in_f, out_f))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def train(self,\n",
    "        X,\n",
    "        Y,\n",
    "        epochs=1000,\n",
    "        lr=0.001,\n",
    "        loss_fn=None,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        lmbda = 0.1):\n",
    "        \n",
    "        loss_function = loss_fn or torch.nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        m = [torch.zeros_like(p) for p in self.parameters()]\n",
    "        v = [torch.zeros_like(p) for p in self.parameters()]\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                t += 1\n",
    "\n",
    "                outputs = self.forward(batch_X)\n",
    "                loss = loss_function(outputs, batch_Y)\n",
    "\n",
    "                if lmbda > 0:\n",
    "                    l1_penalty = 0.0\n",
    "                    for name, param in self.named_parameters():\n",
    "                        if 'weight' in name:  # On √©vite les biais\n",
    "                            l1_penalty += torch.norm(param, p=1)\n",
    "                    loss += lmbda * l1_penalty\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, param in enumerate(self.parameters()):\n",
    "                        if param.grad is None:\n",
    "                            continue\n",
    "                        g = param.grad\n",
    "\n",
    "                        m[i] = beta1 * m[i] + (1 - beta1) * g\n",
    "                        v[i] = beta2 * v[i] + (1 - beta2) * (g * g)\n",
    "\n",
    "                        m_hat = m[i] / (1 - beta1 ** t)\n",
    "                        v_hat = v[i] / (1 - beta2 ** t)\n",
    "\n",
    "                        param -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2031c",
   "metadata": {},
   "source": [
    "Test de notre code pour voir l'effet de la p√©nalisation sur les poids !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1825fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Sans r√©gularisation L1\n",
      "Poids premi√®re couche : tensor([[ 0.3630, -0.4430, -0.0320,  0.1170, -0.7460,  0.0170, -0.1030, -0.0630,\n",
      "          0.1580, -0.0410],\n",
      "        [-0.5310, -0.0160, -0.1300,  0.1940,  0.3900, -0.0140, -0.1990, -0.0500,\n",
      "          0.2540, -0.0920],\n",
      "        [ 0.0690, -1.2470,  0.0600, -0.0090, -0.0250,  0.0060,  0.0220, -0.0340,\n",
      "          0.0230,  0.0150]])\n",
      "Nombre de poids proches de z√©ro (< 1e-2) : 2\n",
      "\n",
      "üîπ Avec r√©gularisation L1\n",
      "Poids premi√®re couche : tensor([[-0.0010,  0.5830, -0.0140, -0.0030,  0.0210, -0.0000, -0.0040,  0.0020,\n",
      "         -0.0010, -0.0000],\n",
      "        [-0.4350,  0.0040, -0.0120, -0.0040,  0.6680, -0.0160, -0.0060,  0.0060,\n",
      "          0.0030, -0.0030],\n",
      "        [ 0.0010, -0.6610,  0.0070,  0.0020,  0.0020, -0.0020,  0.0010,  0.0000,\n",
      "          0.0010,  0.0020]])\n",
      "Nombre de poids proches de z√©ro (< 1e-2) : 22\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_no_reg = ANN_Lasso_Adam(layer_sizes)\n",
    "model_l1 = ANN_Lasso_Adam(layer_sizes)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "model_no_reg.train(X_train, Y_train, epochs=500, lr=1e-3, lmbda=0.0)\n",
    "model_l1.train(X_train, Y_train, epochs=500, lr=1e-3, lmbda=1e-2)\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-2) :\", (first_layer_weights.abs() < 1e-2).sum().item())\n",
    "\n",
    "print(\"üîπ Sans r√©gularisation L1\")\n",
    "print_weights(model_no_reg)\n",
    "\n",
    "print(\"\\nüîπ Avec r√©gularisation L1\")\n",
    "print_weights(model_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7f1dd",
   "metadata": {},
   "source": [
    "IL y a un belle effet de notre p√©nalisation ! une p√©nalisation qui √† lieu sur tous les poids !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba1f7e",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation lasso avec p√©nalisation de tous les poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf457c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Lasso_ISTA(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layers.append(torch.nn.Linear(in_f, out_f))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def soft_threshold(self, z, gamma):\n",
    "        return torch.sign(z) * torch.maximum(torch.abs(z) - gamma, torch.zeros_like(z))\n",
    "\n",
    "    def compute_loss(self, outputs, targets, lmbda):\n",
    "        mse = torch.nn.functional.mse_loss(outputs, targets)\n",
    "        l1_penalty = sum(torch.norm(p, 1) for name, p in self.named_parameters() if 'weight' in name)\n",
    "        return mse + lmbda * l1_penalty\n",
    "\n",
    "    def quadratic_upper_bound(self, y, x, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for y_i, x_i, g_i in zip(y, x, grad):\n",
    "            quad += torch.sum(g_i * (y_i - x_i)) + (L / 2) * torch.norm(y_i - x_i) ** 2 + lmbda * torch.norm(y_i, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "        found = False\n",
    "        while not found:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            for (name, x, g) in zip([n for n, _ in self.named_parameters()], x_old, grad):\n",
    "                if 'weight' in name:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(self.soft_threshold(step, gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            outputs = self.forward(batch_X)\n",
    "            F_val = self.compute_loss(outputs, batch_Y, lmbda)\n",
    "            Q_val = self.quadratic_upper_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                found = True\n",
    "            else:\n",
    "                L *= eta\n",
    "        return L, F_val\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, L0=1.0, batch_size=32, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        L_k = L0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                outputs = self.forward(batch_X)\n",
    "                fx = torch.nn.functional.mse_loss(outputs, batch_Y)\n",
    "\n",
    "                all_params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in all_params]\n",
    "                grad = torch.autograd.grad(fx, all_params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=L_k, eta=eta)\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d5d88",
   "metadata": {},
   "source": [
    "Comparaison avec un r√©seau qui utilise Adam. Bien s√ªr pas grands chose n'est comparable : Adam p√©nalise m√™me les biais, Ista est plus pr√©cis avec un prox explicite. Mais les r√©sultats √† attendre sont que ISTA trouvent une loss proche de celle d'adam avec beaucoup plus de param√®tres nulles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0974ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 0.309999, L_k: 6.5536\n",
      "Epoch [200/1000], Loss: 0.295137, L_k: 6.5536\n",
      "Epoch [300/1000], Loss: 0.293031, L_k: 6.5536\n",
      "Epoch [400/1000], Loss: 0.292413, L_k: 6.5536\n",
      "Epoch [500/1000], Loss: 0.294899, L_k: 6.5536\n",
      "Epoch [600/1000], Loss: 0.291777, L_k: 13.1072\n",
      "Epoch [700/1000], Loss: 0.291582, L_k: 13.1072\n",
      "Epoch [800/1000], Loss: 0.291432, L_k: 13.1072\n",
      "Epoch [900/1000], Loss: 0.291332, L_k: 13.1072\n",
      "Epoch [1000/1000], Loss: 0.291247, L_k: 13.1072\n",
      "üîπ Entra√Ænement avec ADAM\n",
      "Epoch [100/1000], Loss: 1.682929\n",
      "Epoch [200/1000], Loss: 1.647589\n",
      "Epoch [300/1000], Loss: 1.614174\n",
      "Epoch [400/1000], Loss: 1.583389\n",
      "Epoch [500/1000], Loss: 1.553096\n",
      "Epoch [600/1000], Loss: 1.523164\n",
      "Epoch [700/1000], Loss: 1.492478\n",
      "Epoch [800/1000], Loss: 1.461333\n",
      "Epoch [900/1000], Loss: 1.428978\n",
      "Epoch [1000/1000], Loss: 1.396872\n",
      "üîπ Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.4130, -0.0030, -0.0110, -0.0100,  0.6350, -0.0220, -0.0150,  0.0070,\n",
      "         -0.0010, -0.0120],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 20\n",
      "üîπ Optimisation ADAM\n",
      "Poids premi√®re couche : tensor([[-0.2100,  0.0440,  0.0490,  0.0660, -0.1490,  0.0020,  0.0000,  0.2120,\n",
      "          0.1350, -0.1770],\n",
      "        [-0.2790, -0.1010,  0.0040, -0.1690,  0.2540, -0.0480, -0.0650,  0.0010,\n",
      "          0.0020,  0.1310],\n",
      "        [ 0.1930, -0.4090,  0.1380, -0.0330,  0.2590, -0.0390, -0.0240,  0.0000,\n",
      "          0.0660,  0.0960]])\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_ista = ANN_Lasso_ISTA(layer_sizes, verbose=True)\n",
    "model_adam = ANN_Lasso_Adam(layer_sizes, verbose=True)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model_ista.train(X_train, Y_train, epochs=1000, L0=1e-4, lmbda=1e-2, shuffle=True, batch_size=1000)\n",
    "print(\"üîπ Entra√Ænement avec ADAM\")\n",
    "model_adam.train(X_train, Y_train, epochs=1000, lr=1e-4, lmbda=1e-2, shuffle=True, batch_size=1000)\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-3) :\", (first_layer_weights.abs() < 1e-3).sum().item())\n",
    "\n",
    "print(\"üîπ Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "print(\"üîπ Optimisation ADAM\")\n",
    "print_weights(model_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a6544",
   "metadata": {},
   "source": [
    "Rassurant comme r√©sultat les loss diminue et ista obtiens bien plus de poids nulle que Adam. 1√®re remarque ISTA est sensible √† un batch size petit et c'est du √† la condition de backtracking qui est trop dur √† satisfaire avec une aussi grande variance quand le batch est petit. 2eme remarques ISTA convergent tr√®s lentement !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bd9fe",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation lasso avec p√©nalisation $\\ell_1$ sur la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af829de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Lasso(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layer = torch.nn.Linear(in_f, out_f)\n",
    "            layers.append(layer)\n",
    "            if i == 0:\n",
    "                self.first_linear = layer  # On garde la premi√®re couche pour la p√©nalisation\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def soft_threshold(self, z, gamma):\n",
    "        return torch.sign(z) * torch.maximum(torch.abs(z) - gamma, torch.zeros_like(z))\n",
    "\n",
    "    def compute_loss(self, outputs, targets, lmbda):\n",
    "        mse = torch.nn.functional.mse_loss(outputs, targets)\n",
    "        l1_penalty = torch.norm(self.first_linear.weight, p=1)\n",
    "        return mse + lmbda * l1_penalty\n",
    "\n",
    "    def quadratic_upper_bound(self, y, x, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y_i, x_i, g_i) in enumerate(zip(y, x, grad)):\n",
    "            quad += torch.sum(g_i * (y_i - x_i)) + (L / 2) * torch.norm(y_i - x_i) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y_i, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "        found = False\n",
    "        while not found:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            \n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0: # Premi√®re couche\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(self.soft_threshold(step, gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            outputs = self.forward(batch_X)\n",
    "            F_val = self.compute_loss(outputs, batch_Y, lmbda)\n",
    "            Q_val = self.quadratic_upper_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                found = True\n",
    "            else:\n",
    "                L *= eta\n",
    "        return L, F_val\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, L0=1.0, batch_size=32, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        L_k = L0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                outputs = self.forward(batch_X)\n",
    "                fx = torch.nn.functional.mse_loss(outputs, batch_Y)\n",
    "\n",
    "                all_params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in all_params]\n",
    "                grad = torch.autograd.grad(fx, all_params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=L0, eta=eta)\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e7281",
   "metadata": {},
   "source": [
    "Ok je test mon mod√®le pour m'assurer qu'il converge et fais apparaitre des zeros parmis la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2b3fe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.1710, -0.2630, -0.1900, -0.1850,  0.1670, -0.0110, -0.2900,  0.3100,\n",
      "          0.0820,  0.2910],\n",
      "        [ 0.0390,  0.0450,  0.2280, -0.2150,  0.2990, -0.2030,  0.0290, -0.2060,\n",
      "         -0.2100,  0.2430],\n",
      "        [-0.0840, -0.0590,  0.2470,  0.3100, -0.2850, -0.0060,  0.0900,  0.3040,\n",
      "         -0.1260, -0.1030]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 0.275648, L_k: 0.8192\n",
      "Epoch [200/1000], Loss: 0.272402, L_k: 1.6384\n",
      "Epoch [300/1000], Loss: 0.271973, L_k: 13.1072\n",
      "Epoch [400/1000], Loss: 0.271575, L_k: 26.2144\n",
      "Epoch [500/1000], Loss: 0.271209, L_k: 1.6384\n",
      "Epoch [600/1000], Loss: 0.270905, L_k: 26.2144\n",
      "Epoch [700/1000], Loss: 0.270677, L_k: 3.2768\n",
      "Epoch [800/1000], Loss: 0.270510, L_k: 3.2768\n",
      "Epoch [900/1000], Loss: 0.270368, L_k: 6.5536\n",
      "Epoch [1000/1000], Loss: 0.270246, L_k: 6.5536\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.2550, -0.0010, -0.0070, -0.0070,  0.3920, -0.0140, -0.0090,  0.0060,\n",
      "         -0.0010, -0.0080],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 21\n",
      "üîπ Features activ√©es (<1e-4) :\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_ista = ANN_Lasso(layer_sizes, verbose=True)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) :\", (first_layer_weights.abs() < 1e-3).sum().item())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model_ista.train(X_train, Y_train, epochs=1000, L0=1e-4, lmbda=0.01, shuffle=True, batch_size=1000)\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "weights = model_ista.first_linear.weight.detach()\n",
    "active_features = (weights.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\\n\", active_features.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803cef2",
   "metadata": {},
   "source": [
    "Top tout √ßa j'ai eu du mal √† l'impl√©menter surtout pour le calcul de Q qui √©tait mal fais. J'obtiens 3 parmis les 5 features utile c'est plutot bon !\n",
    "Les L_k ne diverge pas !\n",
    "La loss diminue donc il apprends il est vivant !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaf257",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation Square-root lasso avec p√©nalisation $\\ell_1$ sur la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2deb1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_SR_Lasso(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layer = torch.nn.Linear(in_f, out_f)\n",
    "            layers.append(layer)\n",
    "            if i == 0:\n",
    "                self.first_linear = layer\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compute_loss(self, outputs, targets, lmbda):\n",
    "        sqrt_loss = torch.norm(outputs - targets, p=2)\n",
    "        l1_penalty = torch.norm(self.first_linear.weight, p=1)\n",
    "        return sqrt_loss + lmbda * l1_penalty\n",
    "\n",
    "    def quadratic_upper_bound(self, y_new, x_old, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y_i, x_i, g_i) in enumerate(zip(y_new, x_old, grad)):\n",
    "            quad += torch.sum(g_i * (y_i - x_i)) + (L / 2) * torch.norm(y_i - x_i) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y_i, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "\n",
    "        found = False\n",
    "        while not found:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(torch.nn.functional.softshrink(step, lambd=gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            outputs = self.forward(batch_X)\n",
    "\n",
    "            F_val = self.compute_loss(outputs, batch_Y, lmbda)\n",
    "            Q_val = self.quadratic_upper_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            #print(f\"F_val: {F_val.item()}, Q_val: {Q_val.item()}, L: {L}\")\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                found = True\n",
    "            else:\n",
    "                L *= eta\n",
    "        return L, F_val\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000, L0=1.0, batch_size=32, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                L_k = L0\n",
    "                outputs = self.forward(batch_X)\n",
    "                fx = torch.norm(outputs - batch_Y, p=2)  # ‚àöLasso loss (without L1)\n",
    "\n",
    "                all_params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in all_params]\n",
    "                grad = torch.autograd.grad(fx, all_params, create_graph=False)\n",
    "\n",
    "                #print(\"New ! Ista step\")\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=L_k, eta=eta)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "149c28b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.1710, -0.2630, -0.1900, -0.1850,  0.1670, -0.0110, -0.2900,  0.3100,\n",
      "          0.0820,  0.2910],\n",
      "        [ 0.0390,  0.0450,  0.2280, -0.2150,  0.2990, -0.2030,  0.0290, -0.2060,\n",
      "         -0.2100,  0.2430],\n",
      "        [-0.0840, -0.0590,  0.2470,  0.3100, -0.2850, -0.0060,  0.0900,  0.3040,\n",
      "         -0.1260, -0.1030]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/10000], Loss: 14.908038, L_k: 1024.0000\n",
      "Epoch [200/10000], Loss: 14.853792, L_k: 64.0000\n",
      "Epoch [300/10000], Loss: 14.822049, L_k: 256.0000\n",
      "Epoch [400/10000], Loss: 14.801093, L_k: 256.0000\n",
      "Epoch [500/10000], Loss: 14.785966, L_k: 128.0000\n",
      "Epoch [600/10000], Loss: 14.776273, L_k: 512.0000\n",
      "Epoch [700/10000], Loss: 14.767251, L_k: 256.0000\n",
      "Epoch [800/10000], Loss: 14.753124, L_k: 512.0000\n",
      "Epoch [900/10000], Loss: 14.743493, L_k: 256.0000\n",
      "Epoch [1000/10000], Loss: 14.728110, L_k: 64.0000\n",
      "Epoch [1100/10000], Loss: 14.721274, L_k: 512.0000\n",
      "Epoch [1200/10000], Loss: 14.715225, L_k: 512.0000\n",
      "Epoch [1300/10000], Loss: 14.709088, L_k: 32.0000\n",
      "Epoch [1400/10000], Loss: 14.703785, L_k: 128.0000\n",
      "Epoch [1500/10000], Loss: 14.703539, L_k: 8388608.0000\n",
      "Epoch [1600/10000], Loss: 14.703499, L_k: 8192.0000\n",
      "Epoch [1700/10000], Loss: 14.703470, L_k: 8192.0000\n",
      "Epoch [1800/10000], Loss: 14.703441, L_k: 131072.0000\n",
      "Epoch [1900/10000], Loss: 14.703419, L_k: 524288.0000\n",
      "Epoch [2000/10000], Loss: 14.703400, L_k: 8388608.0000\n",
      "Epoch [2100/10000], Loss: 14.703378, L_k: 8192.0000\n",
      "Epoch [2200/10000], Loss: 14.703356, L_k: 32768.0000\n",
      "Epoch [2300/10000], Loss: 14.703340, L_k: 131072.0000\n",
      "Epoch [2400/10000], Loss: 14.703320, L_k: 131072.0000\n",
      "Epoch [2500/10000], Loss: 14.703302, L_k: 65536.0000\n",
      "Epoch [2600/10000], Loss: 14.703285, L_k: 65536.0000\n",
      "Epoch [2700/10000], Loss: 14.703272, L_k: 16384.0000\n",
      "Epoch [2800/10000], Loss: 14.703251, L_k: 16777216.0000\n",
      "Epoch [2900/10000], Loss: 14.703238, L_k: 32768.0000\n",
      "Epoch [3000/10000], Loss: 14.703223, L_k: 65536.0000\n",
      "Epoch [3100/10000], Loss: 14.703208, L_k: 8192.0000\n",
      "Epoch [3200/10000], Loss: 14.703192, L_k: 8192.0000\n",
      "Epoch [3300/10000], Loss: 14.703175, L_k: 16384.0000\n",
      "Epoch [3400/10000], Loss: 14.703156, L_k: 8192.0000\n",
      "Epoch [3500/10000], Loss: 14.703139, L_k: 65536.0000\n",
      "Epoch [3600/10000], Loss: 14.703125, L_k: 65536.0000\n",
      "Epoch [3700/10000], Loss: 14.703111, L_k: 65536.0000\n",
      "Epoch [3800/10000], Loss: 14.703096, L_k: 65536.0000\n",
      "Epoch [3900/10000], Loss: 14.703082, L_k: 262144.0000\n",
      "Epoch [4000/10000], Loss: 14.703069, L_k: 65536.0000\n",
      "Epoch [4100/10000], Loss: 14.703056, L_k: 262144.0000\n",
      "Epoch [4200/10000], Loss: 14.703043, L_k: 32768.0000\n",
      "Epoch [4300/10000], Loss: 14.703032, L_k: 16384.0000\n",
      "Epoch [4400/10000], Loss: 14.703015, L_k: 32768.0000\n",
      "Epoch [4500/10000], Loss: 14.703002, L_k: 16384.0000\n",
      "Epoch [4600/10000], Loss: 14.702991, L_k: 16384.0000\n",
      "Epoch [4700/10000], Loss: 14.702978, L_k: 32768.0000\n",
      "Epoch [4800/10000], Loss: 14.702967, L_k: 65536.0000\n",
      "Epoch [4900/10000], Loss: 14.702956, L_k: 32768.0000\n",
      "Epoch [5000/10000], Loss: 14.702940, L_k: 4194304.0000\n",
      "Epoch [5100/10000], Loss: 14.702928, L_k: 4194304.0000\n",
      "Epoch [5200/10000], Loss: 14.702918, L_k: 32768.0000\n",
      "Epoch [5300/10000], Loss: 14.702905, L_k: 32768.0000\n",
      "Epoch [5400/10000], Loss: 14.702894, L_k: 262144.0000\n",
      "Epoch [5500/10000], Loss: 14.702884, L_k: 16384.0000\n",
      "Epoch [5600/10000], Loss: 14.702869, L_k: 32768.0000\n",
      "Epoch [5700/10000], Loss: 14.702857, L_k: 32768.0000\n",
      "Epoch [5800/10000], Loss: 14.702847, L_k: 262144.0000\n",
      "Epoch [5900/10000], Loss: 14.702837, L_k: 65536.0000\n",
      "Epoch [6000/10000], Loss: 14.702826, L_k: 16384.0000\n",
      "Epoch [6100/10000], Loss: 14.702813, L_k: 32768.0000\n",
      "Epoch [6200/10000], Loss: 14.702805, L_k: 131072.0000\n",
      "Epoch [6300/10000], Loss: 14.702794, L_k: 16384.0000\n",
      "Epoch [6400/10000], Loss: 14.702781, L_k: 32768.0000\n",
      "Epoch [6500/10000], Loss: 14.702768, L_k: 131072.0000\n",
      "Epoch [6600/10000], Loss: 14.702761, L_k: 32768.0000\n",
      "Epoch [6700/10000], Loss: 14.702749, L_k: 65536.0000\n",
      "Epoch [6800/10000], Loss: 14.702738, L_k: 65536.0000\n",
      "Epoch [6900/10000], Loss: 14.702729, L_k: 262144.0000\n",
      "Epoch [7000/10000], Loss: 14.702719, L_k: 16384.0000\n",
      "Epoch [7100/10000], Loss: 14.702709, L_k: 65536.0000\n",
      "Epoch [7200/10000], Loss: 14.702697, L_k: 131072.0000\n",
      "Epoch [7300/10000], Loss: 14.702683, L_k: 32768.0000\n",
      "Epoch [7400/10000], Loss: 14.702671, L_k: 65536.0000\n",
      "Epoch [7500/10000], Loss: 14.702661, L_k: 16777216.0000\n",
      "Epoch [7600/10000], Loss: 14.702649, L_k: 262144.0000\n",
      "Epoch [7700/10000], Loss: 14.702641, L_k: 32768.0000\n",
      "Epoch [7800/10000], Loss: 14.702631, L_k: 131072.0000\n",
      "Epoch [7900/10000], Loss: 14.702621, L_k: 1048576.0000\n",
      "Epoch [8000/10000], Loss: 14.702613, L_k: 65536.0000\n",
      "Epoch [8100/10000], Loss: 14.702604, L_k: 2097152.0000\n",
      "Epoch [8200/10000], Loss: 14.702595, L_k: 65536.0000\n",
      "Epoch [8300/10000], Loss: 14.702584, L_k: 134217728.0000\n",
      "Epoch [8400/10000], Loss: 14.702573, L_k: 262144.0000\n",
      "Epoch [8500/10000], Loss: 14.702564, L_k: 32768.0000\n",
      "Epoch [8600/10000], Loss: 14.702556, L_k: 32768.0000\n",
      "Epoch [8700/10000], Loss: 14.702544, L_k: 131072.0000\n",
      "Epoch [8800/10000], Loss: 14.702533, L_k: 65536.0000\n",
      "Epoch [8900/10000], Loss: 14.702525, L_k: 32768.0000\n",
      "Epoch [9000/10000], Loss: 14.702514, L_k: 32768.0000\n",
      "Epoch [9100/10000], Loss: 14.702500, L_k: 524288.0000\n",
      "Epoch [9200/10000], Loss: 14.702491, L_k: 65536.0000\n",
      "Epoch [9300/10000], Loss: 14.702478, L_k: 65536.0000\n",
      "Epoch [9400/10000], Loss: 14.702468, L_k: 32768.0000\n",
      "Epoch [9500/10000], Loss: 14.702457, L_k: 131072.0000\n",
      "Epoch [9600/10000], Loss: 14.702450, L_k: 32768.0000\n",
      "Epoch [9700/10000], Loss: 14.702441, L_k: 32768.0000\n",
      "Epoch [9800/10000], Loss: 14.702429, L_k: 32768.0000\n",
      "Epoch [9900/10000], Loss: 14.702419, L_k: 67108864.0000\n",
      "Epoch [10000/10000], Loss: 14.702412, L_k: 16384.0000\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.2340, -0.0010, -0.0070, -0.0060,  0.3590, -0.0130, -0.0080,  0.0050,\n",
      "         -0.0010, -0.0070],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 21\n",
      "üîπ Features activ√©es (<1e-4) :\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_ista = ANN_SR_Lasso(layer_sizes, verbose=True)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) :\", (first_layer_weights.abs() < 1e-3).sum().item())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model_ista.fit(X_train, Y_train, epochs=10000, L0=1, lmbda=0.3, shuffle=True, batch_size=1000)\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "weights = model_ista.first_linear.weight.detach()\n",
    "active_features = (weights.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\\n\", active_features.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d0b71",
   "metadata": {},
   "source": [
    "Ok alors l√† des difficult√©s √† comprendre ce qui n'allait pas pour comprendre que tout vas bien c'est simplement que je comparer avec avant sauf que pour le lasso c'est MSE qui est utilis√© donc Rien √† voir avec la square root. Mais r√©sultant me semble satisfaisant il y a bien une s√©lection de variable pour un lambda suffisament grand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65a7a8",
   "metadata": {},
   "source": [
    "### D√©veloppement propre d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation Square-root lasso avec p√©nalisation $\\ell_1$ sur la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class AnnSRLasso(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i + 1])\n",
    "            layers.append(layer)\n",
    "            \n",
    "            if i == 0:\n",
    "                self.first_layer = layer\n",
    "            \n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation:\n",
    "                layers.append(last_activation())\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compute_loss(self, out, targets, lmbda):\n",
    "        mse = torch.norm(out - targets, p=2)\n",
    "        l1_reg = torch.norm(self.first_layer.weight, p=1)\n",
    "        return mse + lmbda * l1_reg\n",
    "\n",
    "    def quad_bound(self, y_new, x_old, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y, x, g) in enumerate(zip(y_new, x_old, grad)):\n",
    "            quad += torch.sum(g * (y - x)) + (L / 2) * torch.norm(y - x) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_x, batch_y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "        \n",
    "        while True:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            \n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(F.softshrink(step, lambd=gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            out = self.forward(batch_x)\n",
    "            F_val = self.compute_loss(out, batch_y, lmbda)\n",
    "            Q_val = self.quad_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                return L, F_val\n",
    "            \n",
    "            L *= eta\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000, L0=1.0, batch_size=None, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in loader:\n",
    "                L_k = L0\n",
    "                out = self.forward(batch_x)\n",
    "                fx = torch.norm(out - batch_y, p=2)\n",
    "\n",
    "                params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in params]\n",
    "                grad = torch.autograd.grad(fx, params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_x, batch_y, lmbda, L_start=L_k, eta=eta)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59d337ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : [[ 0.171 -0.263 -0.19  -0.185  0.167 -0.011 -0.29   0.31   0.082  0.291]\n",
      " [ 0.039  0.045  0.228 -0.215  0.299 -0.203  0.029 -0.206 -0.21   0.243]\n",
      " [-0.084 -0.059  0.247  0.31  -0.285 -0.006  0.09   0.304 -0.126 -0.103]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 3.255974, L_k: 64.0000\n",
      "Epoch [200/1000], Loss: 3.228405, L_k: 64.0000\n",
      "Epoch [300/1000], Loss: 2.418235, L_k: 64.0000\n",
      "Epoch [400/1000], Loss: 3.098960, L_k: 64.0000\n",
      "Epoch [500/1000], Loss: 3.436137, L_k: 64.0000\n",
      "Epoch [600/1000], Loss: 2.829231, L_k: 64.0000\n",
      "Epoch [700/1000], Loss: 2.999206, L_k: 128.0000\n",
      "Epoch [800/1000], Loss: 3.303372, L_k: 128.0000\n",
      "Epoch [900/1000], Loss: 2.728492, L_k: 64.0000\n",
      "Epoch [1000/1000], Loss: 3.102011, L_k: 64.0000\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Poids premi√®re couche : [[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.292 -0.046 -0.001 -0.031  0.656 -0.058  0.013  0.027  0.02   0.027]\n",
      " [-0.    -0.     0.     0.     0.    -0.     0.     0.    -0.     0.   ]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 20\n",
      "üîπ Features activ√©es (<1e-4) : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n, p = 1000, 10\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "model = AnnSRLasso([p, 3, 1], verbose=True)\n",
    "\n",
    "def print_weights(model):\n",
    "    w = model.first_layer.weight.detach().cpu().numpy()\n",
    "    print(\"Poids premi√®re couche :\", np.round(w, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-3) :\", (np.abs(w) < 1e-3).sum())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model.fit(X_train, Y_train, epochs=1000, L0=1.0, lmbda=0.3)\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print_weights(model)\n",
    "\n",
    "w = model.first_layer.weight.detach()\n",
    "active = (w.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\", active.nonzero(as_tuple=True)[0].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ada97c",
   "metadata": {},
   "source": [
    "### D√©veloppement Mod√®le LASSO ANN de l'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd357d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class LASSOANN(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.ELU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i + 1])\n",
    "            layers.append(layer)\n",
    "\n",
    "            if i == 0:\n",
    "                self.first_layer = layer\n",
    "\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation:\n",
    "                layers.append(last_activation())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def compute_loss(self, out, targets, lmbda):\n",
    "        residual = torch.norm(out - targets, p=2)\n",
    "        l1_reg = torch.norm(self.first_layer.weight, p=1)\n",
    "        return residual + lmbda * l1_reg\n",
    "\n",
    "    def lambda_qut(self, X, M=1000, alpha=0.05, seed=None, sigma_deriv_0=1.0, pi_l=1.0):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        Lambda = []\n",
    "        for _ in range(M):\n",
    "            Y_sim = torch.randn(n, device=X.device)\n",
    "            denom = torch.norm(Y_sim, p=2)\n",
    "            if denom == 0:\n",
    "                continue\n",
    "            lam0 = torch.norm(X.T @ Y_sim / denom, p=float('inf'))\n",
    "            Lambda.append(lam0.item())\n",
    "\n",
    "        lam_qut = pi_l * sigma_deriv_0 * torch.tensor(Lambda).quantile(1 - alpha).item()\n",
    "        return lam_qut\n",
    "\n",
    "    def quad_bound(self, y_new, x_old, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y, x, g) in enumerate(zip(y_new, x_old, grad)):\n",
    "            quad += torch.sum(g * (y - x)) + (L / 2) * torch.norm(y - x) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_x, batch_y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "\n",
    "        while True:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "\n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(F.softshrink(step, lambd=gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            out = self.forward(batch_x)\n",
    "            F_val = self.compute_loss(out, batch_y, lmbda)\n",
    "            Q_val = self.quad_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                return L, F_val\n",
    "\n",
    "            L *= eta\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000, L0=1.0, batch_size=None, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        L_k = L0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in loader:\n",
    "                \n",
    "                out = self.forward(batch_x)\n",
    "                fx = torch.norm(out - batch_y, p=2)\n",
    "\n",
    "                params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in params]\n",
    "                grad = torch.autograd.grad(fx, params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_x, batch_y, lmbda, L_start=L_k, eta=eta)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "420fe228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : [[ 0.171 -0.263 -0.19  -0.185  0.167 -0.011 -0.29   0.31   0.082  0.291]\n",
      " [ 0.039  0.045  0.228 -0.215  0.299 -0.203  0.029 -0.206 -0.21   0.243]\n",
      " [-0.084 -0.059  0.247  0.31  -0.285 -0.006  0.09   0.304 -0.126 -0.103]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 18.897556, L_k: 256.0000\n",
      "Epoch [200/1000], Loss: 18.460762, L_k: 256.0000\n",
      "Epoch [300/1000], Loss: 18.278563, L_k: 512.0000\n",
      "Epoch [400/1000], Loss: 18.195658, L_k: 512.0000\n",
      "Epoch [500/1000], Loss: 18.127386, L_k: 512.0000\n",
      "Epoch [600/1000], Loss: 18.069847, L_k: 512.0000\n",
      "Epoch [700/1000], Loss: 18.020458, L_k: 512.0000\n",
      "Epoch [800/1000], Loss: 17.977440, L_k: 512.0000\n",
      "Epoch [900/1000], Loss: 17.939453, L_k: 512.0000\n",
      "Epoch [1000/1000], Loss: 17.905497, L_k: 512.0000\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Lambda QUT : 2.7963171005249023\n",
      "Poids premi√®re couche : [[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.185  0.     0.    -0.002  0.285 -0.003 -0.003 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.     0.    -0.    -0.    -0.    -0.    -0.   ]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 25\n",
      "üîπ Features activ√©es (<1e-4) : [0, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n, p = 1000, 10\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "model = LASSOANN([p, 3, 1], verbose=True)\n",
    "\n",
    "def print_weights(model):\n",
    "    w = model.first_layer.weight.detach().cpu().numpy()\n",
    "    print(\"Poids premi√®re couche :\", np.round(w, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-3) :\", (np.abs(w) < 1e-3).sum())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "lambda_qut = model.lambda_qut(X, M=1000, alpha=0.05)\n",
    "model.fit(X, Y, lmbda=lambda_qut, epochs=1000, L0=1.0, shuffle=True, batch_size=None)\n",
    "\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print(\"Lambda QUT :\", lambda_qut)\n",
    "print_weights(model)\n",
    "\n",
    "w = model.first_layer.weight.detach()\n",
    "active = (w.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\", active.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37878b",
   "metadata": {},
   "source": [
    "#### Test de l'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5476b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data_nonlinear(n=500, p=50, h=5, noise_std=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    G√©n√®re des donn√©es simul√©es pour le test non lin√©aire de l‚Äôarticle.\n",
    "    \n",
    "    - Œº(x) = ‚àë_{i=1}^h 10 * |x_{2i} - x_{2i - 1}|\n",
    "    - Support S* = {0, 1, ..., 2h - 1}\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(0, 1, size=(n, p))\n",
    "    \n",
    "    y = np.zeros(n)\n",
    "    for i in range(1, h + 1):\n",
    "        j1, j2 = 2 * i - 2, 2 * i - 1  # indices des variables utilis√©es\n",
    "        y += 10 * np.abs(X[:, j2] - X[:, j1])\n",
    "    \n",
    "    y += rng.normal(0, noise_std, size=n)\n",
    "    \n",
    "    S_true = set(range(2 * h))  # Les colonnes 0 √† 2h - 1\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).reshape(-1, 1), S_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843b30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_variables(W1: torch.Tensor, eps: float = 1e-5) -> set:\n",
    "    \"\"\"\n",
    "    Retourne l‚Äôensemble des indices j tels que la colonne j de W1 est active.\n",
    "    \"\"\"\n",
    "    return {j for j in range(W1.shape[1]) if torch.norm(W1[:, j]) > eps}\n",
    "\n",
    "def pesr(W1: torch.Tensor, S_true: set, eps: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcule la PESR : 1 si S_hat == S_true, sinon 0.\n",
    "    \n",
    "    Args:\n",
    "        S_hat (set): ensemble des variables s√©lectionn√©es\n",
    "        S_true (set): ensemble des vraies variables pertinentes\n",
    "\n",
    "    Returns:\n",
    "        float: 1.0 si exact recovery, 0.0 sinon\n",
    "    \"\"\"\n",
    "    S_hat = get_active_variables(W1, eps)\n",
    "    return float(S_hat == S_true)\n",
    "\n",
    "def f1_score(S_hat: set, S_true: set) -> float:\n",
    "    TP = np.sum([1 for i in S_true if i in S_hat])\n",
    "    FP = np.sum([1 for i in S_hat if i not in S_true])\n",
    "    FN = np.sum([1 for i in S_true if i not in S_hat])\n",
    "    denom = 2 * TP + FP + FN\n",
    "    return 2 * TP / denom if denom > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979bdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent.parent.parent)+\"\\AYMEN\\EXPLORATION\\CAS NON LINEAIRE\\OUTILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b96a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X| h = 0\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|XXX| Simu = 10\n",
      "|XXX| Simu = 11\n",
      "|XXX| Simu = 12\n",
      "|XXX| Simu = 13\n",
      "|XXX| Simu = 14\n",
      "|XXX| Simu = 15\n",
      "|XXX| Simu = 16\n",
      "|XXX| Simu = 17\n",
      "|XXX| Simu = 18\n",
      "|XXX| Simu = 19\n",
      "|XXX| Simu = 20\n",
      "|XXX| Simu = 21\n",
      "|XXX| Simu = 22\n",
      "|XXX| Simu = 23\n",
      "|XXX| Simu = 24\n",
      "|XXX| Simu = 25\n",
      "|XXX| Simu = 26\n",
      "|XXX| Simu = 27\n",
      "|XXX| Simu = 28\n",
      "|XXX| Simu = 29\n",
      "|XXX| Simu = 30\n",
      "|XXX| Simu = 31\n",
      "|XXX| Simu = 32\n",
      "|XXX| Simu = 33\n",
      "|XXX| Simu = 34\n",
      "|XXX| Simu = 35\n",
      "|XXX| Simu = 36\n",
      "|XXX| Simu = 37\n",
      "|XXX| Simu = 38\n",
      "|XXX| Simu = 39\n",
      "|XXX| Simu = 40\n",
      "|XXX| Simu = 41\n",
      "|XXX| Simu = 42\n",
      "|XXX| Simu = 43\n",
      "|XXX| Simu = 44\n",
      "|XXX| Simu = 45\n",
      "|XXX| Simu = 46\n",
      "|XXX| Simu = 47\n",
      "|XXX| Simu = 48\n",
      "|XXX| Simu = 49\n",
      "|XXX| Simu = 50\n",
      "|XXX| Simu = 51\n",
      "|XXX| Simu = 52\n",
      "|XXX| Simu = 53\n",
      "|XXX| Simu = 54\n",
      "|XXX| Simu = 55\n",
      "|XXX| Simu = 56\n",
      "|XXX| Simu = 57\n",
      "|XXX| Simu = 58\n",
      "|XXX| Simu = 59\n",
      "|XXX| Simu = 60\n",
      "|XXX| Simu = 61\n",
      "|XXX| Simu = 62\n",
      "|XXX| Simu = 63\n",
      "|XXX| Simu = 64\n",
      "|XXX| Simu = 65\n",
      "|XXX| Simu = 66\n",
      "|XXX| Simu = 67\n",
      "|XXX| Simu = 68\n",
      "|XXX| Simu = 69\n",
      "|XXX| Simu = 70\n",
      "|XXX| Simu = 71\n",
      "|XXX| Simu = 72\n",
      "|XXX| Simu = 73\n",
      "|XXX| Simu = 74\n",
      "|XXX| Simu = 75\n",
      "|XXX| Simu = 76\n",
      "|XXX| Simu = 77\n",
      "|XXX| Simu = 78\n",
      "|XXX| Simu = 79\n",
      "|XXX| Simu = 80\n",
      "|XXX| Simu = 81\n",
      "|XXX| Simu = 82\n",
      "|XXX| Simu = 83\n",
      "|XXX| Simu = 84\n",
      "|XXX| Simu = 85\n",
      "|XXX| Simu = 86\n",
      "|XXX| Simu = 87\n",
      "|XXX| Simu = 88\n",
      "|XXX| Simu = 89\n",
      "|XXX| Simu = 90\n",
      "|XXX| Simu = 91\n",
      "|XXX| Simu = 92\n",
      "|XXX| Simu = 93\n",
      "|XXX| Simu = 94\n",
      "|XXX| Simu = 95\n",
      "|XXX| Simu = 96\n",
      "|XXX| Simu = 97\n",
      "|XXX| Simu = 98\n",
      "|XXX| Simu = 99\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from non_linear_tools import plot_scores\n",
    "scores = {'pesr': [], 'f1': []}\n",
    "s_range = []\n",
    "\n",
    "simu_iter = 100\n",
    "seed = 42\n",
    "\n",
    "for h in [0]:\n",
    "    n, p = 500, 50\n",
    "    s = 2 * h\n",
    "    pesr_list = []\n",
    "    f1_list = []\n",
    "    print(f\"|X| h = {h}\")\n",
    "    for i in range(simu_iter):\n",
    "        print(f\"|XXX| Simu = {i}\")\n",
    "        X, y, S_true = generate_data_nonlinear(n, p, h=h, seed=seed * i)\n",
    "        model = LASSOANN(sizes=[50, 20, 1], verbose=False)\n",
    "        lmbda = model.lambda_qut(X, M=1000, alpha=0.05)\n",
    "        model.fit(X, y, lmbda=lmbda, epochs=50, L0=0.5)\n",
    "        W1 = model.first_layer.weight.detach()\n",
    "        S_hat = get_active_variables(W1)\n",
    "        pesr_list.append(pesr(W1, S_true))\n",
    "        f1_list.append(f1_score(S_hat, S_true))\n",
    "    scores['pesr'].append(np.mean(pesr_list))\n",
    "    scores['f1'].append(np.mean(f1_list))\n",
    "    s_range.append(s)\n",
    "\n",
    "plot_scores(scores, s_range, \"PESR et F1 Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630076f7",
   "metadata": {},
   "source": [
    "Ok je m'arrete l√† niveau r√©seau j'ai pas exacctement les memes r√©sultat mais c'est bon. Une erreur dans mon F1 score dailleurs qui fais que monter pfff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

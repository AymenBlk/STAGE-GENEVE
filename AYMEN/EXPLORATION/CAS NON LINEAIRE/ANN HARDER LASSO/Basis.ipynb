{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed55ed1",
   "metadata": {},
   "source": [
    "# R√©seaux de neurones artificielle avec regularisation HARDER LASSO (ANN HARDER LASSO)\n",
    "**Aymen.B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021e2ef",
   "metadata": {},
   "source": [
    "## Th√©orie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc81bc",
   "metadata": {},
   "source": [
    "## D√©velopement \n",
    "\n",
    "(Je pars de la base pour arriver au mod√®le finale explicit√© dans la partie th√©orie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f5d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d2ee0",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation Adam manuelle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b7d056",
   "metadata": {},
   "source": [
    "#### Ici pas de p√©nalisation la fonction de cout est simplement MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee9df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layers.append(torch.nn.Linear(in_f, out_f))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def train(self, X, Y, epochs=1000, lr=0.001, loss_fn=None, batch_size=32, shuffle=True, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        loss_function = loss_fn or torch.nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        m = [torch.zeros_like(p) for p in self.parameters()]\n",
    "        v = [torch.zeros_like(p) for p in self.parameters()]\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                t += 1\n",
    "\n",
    "                outputs = self.forward(batch_X)\n",
    "                loss = loss_function(outputs, batch_Y)\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, param in enumerate(self.parameters()):\n",
    "                        if param.grad is None:\n",
    "                            continue\n",
    "                        g = param.grad\n",
    "\n",
    "                        m[i] = beta1 * m[i] + (1 - beta1) * g\n",
    "                        v[i] = beta2 * v[i] + (1 - beta2) * (g * g)\n",
    "\n",
    "                        m_hat = m[i] / (1 - beta1 ** t)\n",
    "                        v_hat = v[i] / (1 - beta2 ** t)\n",
    "\n",
    "                        param -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c246d6e",
   "metadata": {},
   "source": [
    "#### On change de fonction co√ªt on lui rajoute une p√©nalit√© multipli√© par un coefficient lambda arbitrairement choisi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff67bd",
   "metadata": {},
   "source": [
    "En th√©orie il devrait y avoir un souci sur le choix de l'optimiseur mais ce n'est pas un soucis car pytorch reponds √† ce probl√®me avec le sous gradient de la norme $\\ell_1$ non diff√©rentiable en 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3f62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Lasso_Adam(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layers.append(torch.nn.Linear(in_f, out_f))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def train(self,\n",
    "        X,\n",
    "        Y,\n",
    "        epochs=1000,\n",
    "        lr=0.001,\n",
    "        loss_fn=None,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        epsilon=1e-8,\n",
    "        lmbda = 0.1):\n",
    "        \n",
    "        loss_function = loss_fn or torch.nn.MSELoss()\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        m = [torch.zeros_like(p) for p in self.parameters()]\n",
    "        v = [torch.zeros_like(p) for p in self.parameters()]\n",
    "\n",
    "        t = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                t += 1\n",
    "\n",
    "                outputs = self.forward(batch_X)\n",
    "                loss = loss_function(outputs, batch_Y)\n",
    "\n",
    "                if lmbda > 0:\n",
    "                    l1_penalty = 0.0\n",
    "                    for name, param in self.named_parameters():\n",
    "                        if 'weight' in name:  # On √©vite les biais\n",
    "                            l1_penalty += torch.norm(param, p=1)\n",
    "                    loss += lmbda * l1_penalty\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, param in enumerate(self.parameters()):\n",
    "                        if param.grad is None:\n",
    "                            continue\n",
    "                        g = param.grad\n",
    "\n",
    "                        m[i] = beta1 * m[i] + (1 - beta1) * g\n",
    "                        v[i] = beta2 * v[i] + (1 - beta2) * (g * g)\n",
    "\n",
    "                        m_hat = m[i] / (1 - beta1 ** t)\n",
    "                        v_hat = v[i] / (1 - beta2 ** t)\n",
    "\n",
    "                        param -= lr * m_hat / (v_hat.sqrt() + epsilon)\n",
    "\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2031c",
   "metadata": {},
   "source": [
    "Test de notre code pour voir l'effet de la p√©nalisation sur les poids !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1825fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Sans r√©gularisation L1\n",
      "Poids premi√®re couche : tensor([[ 0.3630, -0.4430, -0.0320,  0.1170, -0.7460,  0.0170, -0.1030, -0.0630,\n",
      "          0.1580, -0.0410],\n",
      "        [-0.5310, -0.0160, -0.1300,  0.1940,  0.3900, -0.0140, -0.1990, -0.0500,\n",
      "          0.2540, -0.0920],\n",
      "        [ 0.0690, -1.2470,  0.0600, -0.0090, -0.0250,  0.0060,  0.0220, -0.0340,\n",
      "          0.0230,  0.0150]])\n",
      "Nombre de poids proches de z√©ro (< 1e-2) : 2\n",
      "\n",
      "üîπ Avec r√©gularisation L1\n",
      "Poids premi√®re couche : tensor([[-0.0010,  0.5830, -0.0140, -0.0030,  0.0210, -0.0000, -0.0040,  0.0020,\n",
      "         -0.0010, -0.0000],\n",
      "        [-0.4350,  0.0040, -0.0120, -0.0040,  0.6680, -0.0160, -0.0060,  0.0060,\n",
      "          0.0030, -0.0030],\n",
      "        [ 0.0010, -0.6610,  0.0070,  0.0020,  0.0020, -0.0020,  0.0010,  0.0000,\n",
      "          0.0010,  0.0020]])\n",
      "Nombre de poids proches de z√©ro (< 1e-2) : 22\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_no_reg = ANN_Lasso_Adam(layer_sizes)\n",
    "model_l1 = ANN_Lasso_Adam(layer_sizes)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "model_no_reg.train(X_train, Y_train, epochs=500, lr=1e-3, lmbda=0.0)\n",
    "model_l1.train(X_train, Y_train, epochs=500, lr=1e-3, lmbda=1e-2)\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-2) :\", (first_layer_weights.abs() < 1e-2).sum().item())\n",
    "\n",
    "print(\"üîπ Sans r√©gularisation L1\")\n",
    "print_weights(model_no_reg)\n",
    "\n",
    "print(\"\\nüîπ Avec r√©gularisation L1\")\n",
    "print_weights(model_l1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a7f1dd",
   "metadata": {},
   "source": [
    "IL y a un belle effet de notre p√©nalisation ! une p√©nalisation qui √† lieu sur tous les poids !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba1f7e",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation lasso avec p√©nalisation de tous les poids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf457c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Lasso_ISTA(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layers.append(torch.nn.Linear(in_f, out_f))\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def soft_threshold(self, z, gamma):\n",
    "        return torch.sign(z) * torch.maximum(torch.abs(z) - gamma, torch.zeros_like(z))\n",
    "\n",
    "    def compute_loss(self, outputs, targets, lmbda):\n",
    "        mse = torch.nn.functional.mse_loss(outputs, targets)\n",
    "        l1_penalty = sum(torch.norm(p, 1) for name, p in self.named_parameters() if 'weight' in name)\n",
    "        return mse + lmbda * l1_penalty\n",
    "\n",
    "    def quadratic_upper_bound(self, y, x, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for y_i, x_i, g_i in zip(y, x, grad):\n",
    "            quad += torch.sum(g_i * (y_i - x_i)) + (L / 2) * torch.norm(y_i - x_i) ** 2 + lmbda * torch.norm(y_i, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "        found = False\n",
    "        while not found:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            for (name, x, g) in zip([n for n, _ in self.named_parameters()], x_old, grad):\n",
    "                if 'weight' in name:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(self.soft_threshold(step, gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            outputs = self.forward(batch_X)\n",
    "            F_val = self.compute_loss(outputs, batch_Y, lmbda)\n",
    "            Q_val = self.quadratic_upper_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                found = True\n",
    "            else:\n",
    "                L *= eta\n",
    "        return L, F_val\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, L0=1.0, batch_size=32, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        L_k = L0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                outputs = self.forward(batch_X)\n",
    "                fx = torch.nn.functional.mse_loss(outputs, batch_Y)\n",
    "\n",
    "                all_params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in all_params]\n",
    "                grad = torch.autograd.grad(fx, all_params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=L_k, eta=eta)\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d5d88",
   "metadata": {},
   "source": [
    "Comparaison avec un r√©seau qui utilise Adam. Bien s√ªr pas grands chose n'est comparable : Adam p√©nalise m√™me les biais, Ista est plus pr√©cis avec un prox explicite. Mais les r√©sultats √† attendre sont que ISTA trouvent une loss proche de celle d'adam avec beaucoup plus de param√®tres nulles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0974ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 0.309999, L_k: 6.5536\n",
      "Epoch [200/1000], Loss: 0.295137, L_k: 6.5536\n",
      "Epoch [300/1000], Loss: 0.293031, L_k: 6.5536\n",
      "Epoch [400/1000], Loss: 0.292413, L_k: 6.5536\n",
      "Epoch [500/1000], Loss: 0.294899, L_k: 6.5536\n",
      "Epoch [600/1000], Loss: 0.291777, L_k: 13.1072\n",
      "Epoch [700/1000], Loss: 0.291582, L_k: 13.1072\n",
      "Epoch [800/1000], Loss: 0.291432, L_k: 13.1072\n",
      "Epoch [900/1000], Loss: 0.291332, L_k: 13.1072\n",
      "Epoch [1000/1000], Loss: 0.291247, L_k: 13.1072\n",
      "üîπ Entra√Ænement avec ADAM\n",
      "Epoch [100/1000], Loss: 1.682929\n",
      "Epoch [200/1000], Loss: 1.647589\n",
      "Epoch [300/1000], Loss: 1.614174\n",
      "Epoch [400/1000], Loss: 1.583389\n",
      "Epoch [500/1000], Loss: 1.553096\n",
      "Epoch [600/1000], Loss: 1.523164\n",
      "Epoch [700/1000], Loss: 1.492478\n",
      "Epoch [800/1000], Loss: 1.461333\n",
      "Epoch [900/1000], Loss: 1.428978\n",
      "Epoch [1000/1000], Loss: 1.396872\n",
      "üîπ Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.4130, -0.0030, -0.0110, -0.0100,  0.6350, -0.0220, -0.0150,  0.0070,\n",
      "         -0.0010, -0.0120],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 20\n",
      "üîπ Optimisation ADAM\n",
      "Poids premi√®re couche : tensor([[-0.2100,  0.0440,  0.0490,  0.0660, -0.1490,  0.0020,  0.0000,  0.2120,\n",
      "          0.1350, -0.1770],\n",
      "        [-0.2790, -0.1010,  0.0040, -0.1690,  0.2540, -0.0480, -0.0650,  0.0010,\n",
      "          0.0020,  0.1310],\n",
      "        [ 0.1930, -0.4090,  0.1380, -0.0330,  0.2590, -0.0390, -0.0240,  0.0000,\n",
      "          0.0660,  0.0960]])\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_ista = ANN_Lasso_ISTA(layer_sizes, verbose=True)\n",
    "model_adam = ANN_Lasso_Adam(layer_sizes, verbose=True)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model_ista.train(X_train, Y_train, epochs=1000, L0=1e-4, lmbda=1e-2, shuffle=True, batch_size=1000)\n",
    "print(\"üîπ Entra√Ænement avec ADAM\")\n",
    "model_adam.train(X_train, Y_train, epochs=1000, lr=1e-4, lmbda=1e-2, shuffle=True, batch_size=1000)\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-3) :\", (first_layer_weights.abs() < 1e-3).sum().item())\n",
    "\n",
    "print(\"üîπ Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "print(\"üîπ Optimisation ADAM\")\n",
    "print_weights(model_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a6544",
   "metadata": {},
   "source": [
    "Rassurant comme r√©sultat les loss diminue et ista obtiens bien plus de poids nulle que Adam. 1√®re remarque ISTA est sensible √† un batch size petit et c'est du √† la condition de backtracking qui est trop dur √† satisfaire avec une aussi grande variance quand le batch est petit. 2eme remarques ISTA convergent tr√®s lentement !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bd9fe",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation lasso avec p√©nalisation $\\ell_1$ sur la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af829de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_Lasso(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layer = torch.nn.Linear(in_f, out_f)\n",
    "            layers.append(layer)\n",
    "            if i == 0:\n",
    "                self.first_linear = layer  # On garde la premi√®re couche pour la p√©nalisation\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def soft_threshold(self, z, gamma):\n",
    "        return torch.sign(z) * torch.maximum(torch.abs(z) - gamma, torch.zeros_like(z))\n",
    "\n",
    "    def compute_loss(self, outputs, targets, lmbda):\n",
    "        mse = torch.nn.functional.mse_loss(outputs, targets)\n",
    "        l1_penalty = torch.norm(self.first_linear.weight, p=1)\n",
    "        return mse + lmbda * l1_penalty\n",
    "\n",
    "    def quadratic_upper_bound(self, y, x, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y_i, x_i, g_i) in enumerate(zip(y, x, grad)):\n",
    "            quad += torch.sum(g_i * (y_i - x_i)) + (L / 2) * torch.norm(y_i - x_i) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y_i, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "        found = False\n",
    "        while not found:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            \n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0: # Premi√®re couche\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(self.soft_threshold(step, gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            outputs = self.forward(batch_X)\n",
    "            F_val = self.compute_loss(outputs, batch_Y, lmbda)\n",
    "            Q_val = self.quadratic_upper_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                found = True\n",
    "            else:\n",
    "                L *= eta\n",
    "        return L, F_val\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, L0=1.0, batch_size=32, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        L_k = L0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                outputs = self.forward(batch_X)\n",
    "                fx = torch.nn.functional.mse_loss(outputs, batch_Y)\n",
    "\n",
    "                all_params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in all_params]\n",
    "                grad = torch.autograd.grad(fx, all_params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=L0, eta=eta)\n",
    "                self.zero_grad()\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e7281",
   "metadata": {},
   "source": [
    "Ok je test mon mod√®le pour m'assurer qu'il converge et fais apparaitre des zeros parmis la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2b3fe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.1710, -0.2630, -0.1900, -0.1850,  0.1670, -0.0110, -0.2900,  0.3100,\n",
      "          0.0820,  0.2910],\n",
      "        [ 0.0390,  0.0450,  0.2280, -0.2150,  0.2990, -0.2030,  0.0290, -0.2060,\n",
      "         -0.2100,  0.2430],\n",
      "        [-0.0840, -0.0590,  0.2470,  0.3100, -0.2850, -0.0060,  0.0900,  0.3040,\n",
      "         -0.1260, -0.1030]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 0.275648, L_k: 0.8192\n",
      "Epoch [200/1000], Loss: 0.272402, L_k: 1.6384\n",
      "Epoch [300/1000], Loss: 0.271973, L_k: 13.1072\n",
      "Epoch [400/1000], Loss: 0.271575, L_k: 26.2144\n",
      "Epoch [500/1000], Loss: 0.271209, L_k: 1.6384\n",
      "Epoch [600/1000], Loss: 0.270905, L_k: 26.2144\n",
      "Epoch [700/1000], Loss: 0.270677, L_k: 3.2768\n",
      "Epoch [800/1000], Loss: 0.270510, L_k: 3.2768\n",
      "Epoch [900/1000], Loss: 0.270368, L_k: 6.5536\n",
      "Epoch [1000/1000], Loss: 0.270246, L_k: 6.5536\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.2550, -0.0010, -0.0070, -0.0070,  0.3920, -0.0140, -0.0090,  0.0060,\n",
      "         -0.0010, -0.0080],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 21\n",
      "üîπ Features activ√©es (<1e-4) :\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_ista = ANN_Lasso(layer_sizes, verbose=True)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) :\", (first_layer_weights.abs() < 1e-3).sum().item())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model_ista.train(X_train, Y_train, epochs=1000, L0=1e-4, lmbda=0.01, shuffle=True, batch_size=1000)\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "weights = model_ista.first_linear.weight.detach()\n",
    "active_features = (weights.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\\n\", active_features.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803cef2",
   "metadata": {},
   "source": [
    "Top tout √ßa j'ai eu du mal √† l'impl√©menter surtout pour le calcul de Q qui √©tait mal fais. J'obtiens 3 parmis les 5 features utile c'est plutot bon !\n",
    "Les L_k ne diverge pas !\n",
    "La loss diminue donc il apprends il est vivant !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaf257",
   "metadata": {},
   "source": [
    "### D√©veloppement d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation Square-root lasso avec p√©nalisation $\\ell_1$ sur la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2deb1621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN_SR_Lasso(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes, activation=torch.nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_f, out_f = layer_sizes[i], layer_sizes[i + 1]\n",
    "            layer = torch.nn.Linear(in_f, out_f)\n",
    "            layers.append(layer)\n",
    "            if i == 0:\n",
    "                self.first_linear = layer\n",
    "            if i < len(layer_sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation is not None:\n",
    "                layers.append(last_activation())\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compute_loss(self, outputs, targets, lmbda):\n",
    "        sqrt_loss = torch.norm(outputs - targets, p=2)\n",
    "        l1_penalty = torch.norm(self.first_linear.weight, p=1)\n",
    "        return sqrt_loss + lmbda * l1_penalty\n",
    "\n",
    "    def quadratic_upper_bound(self, y_new, x_old, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y_i, x_i, g_i) in enumerate(zip(y_new, x_old, grad)):\n",
    "            quad += torch.sum(g_i * (y_i - x_i)) + (L / 2) * torch.norm(y_i - x_i) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y_i, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "\n",
    "        found = False\n",
    "        while not found:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(torch.nn.functional.softshrink(step, lambd=gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            outputs = self.forward(batch_X)\n",
    "\n",
    "            F_val = self.compute_loss(outputs, batch_Y, lmbda)\n",
    "            Q_val = self.quadratic_upper_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            #print(f\"F_val: {F_val.item()}, Q_val: {Q_val.item()}, L: {L}\")\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                found = True\n",
    "            else:\n",
    "                L *= eta\n",
    "        return L, F_val\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000, L0=1.0, batch_size=32, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_X, batch_Y in dataloader:\n",
    "                L_k = L0\n",
    "                outputs = self.forward(batch_X)\n",
    "                fx = torch.norm(outputs - batch_Y, p=2)  # ‚àöLasso loss (without L1)\n",
    "\n",
    "                all_params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in all_params]\n",
    "                grad = torch.autograd.grad(fx, all_params, create_graph=False)\n",
    "\n",
    "                #print(\"New ! Ista step\")\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_X, batch_Y, lmbda, L_start=L_k, eta=eta)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "149c28b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.1710, -0.2630, -0.1900, -0.1850,  0.1670, -0.0110, -0.2900,  0.3100,\n",
      "          0.0820,  0.2910],\n",
      "        [ 0.0390,  0.0450,  0.2280, -0.2150,  0.2990, -0.2030,  0.0290, -0.2060,\n",
      "         -0.2100,  0.2430],\n",
      "        [-0.0840, -0.0590,  0.2470,  0.3100, -0.2850, -0.0060,  0.0900,  0.3040,\n",
      "         -0.1260, -0.1030]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/10000], Loss: 14.908038, L_k: 1024.0000\n",
      "Epoch [200/10000], Loss: 14.853792, L_k: 64.0000\n",
      "Epoch [300/10000], Loss: 14.822049, L_k: 256.0000\n",
      "Epoch [400/10000], Loss: 14.801093, L_k: 256.0000\n",
      "Epoch [500/10000], Loss: 14.785966, L_k: 128.0000\n",
      "Epoch [600/10000], Loss: 14.776273, L_k: 512.0000\n",
      "Epoch [700/10000], Loss: 14.767251, L_k: 256.0000\n",
      "Epoch [800/10000], Loss: 14.753124, L_k: 512.0000\n",
      "Epoch [900/10000], Loss: 14.743493, L_k: 256.0000\n",
      "Epoch [1000/10000], Loss: 14.728110, L_k: 64.0000\n",
      "Epoch [1100/10000], Loss: 14.721274, L_k: 512.0000\n",
      "Epoch [1200/10000], Loss: 14.715225, L_k: 512.0000\n",
      "Epoch [1300/10000], Loss: 14.709088, L_k: 32.0000\n",
      "Epoch [1400/10000], Loss: 14.703785, L_k: 128.0000\n",
      "Epoch [1500/10000], Loss: 14.703539, L_k: 8388608.0000\n",
      "Epoch [1600/10000], Loss: 14.703499, L_k: 8192.0000\n",
      "Epoch [1700/10000], Loss: 14.703470, L_k: 8192.0000\n",
      "Epoch [1800/10000], Loss: 14.703441, L_k: 131072.0000\n",
      "Epoch [1900/10000], Loss: 14.703419, L_k: 524288.0000\n",
      "Epoch [2000/10000], Loss: 14.703400, L_k: 8388608.0000\n",
      "Epoch [2100/10000], Loss: 14.703378, L_k: 8192.0000\n",
      "Epoch [2200/10000], Loss: 14.703356, L_k: 32768.0000\n",
      "Epoch [2300/10000], Loss: 14.703340, L_k: 131072.0000\n",
      "Epoch [2400/10000], Loss: 14.703320, L_k: 131072.0000\n",
      "Epoch [2500/10000], Loss: 14.703302, L_k: 65536.0000\n",
      "Epoch [2600/10000], Loss: 14.703285, L_k: 65536.0000\n",
      "Epoch [2700/10000], Loss: 14.703272, L_k: 16384.0000\n",
      "Epoch [2800/10000], Loss: 14.703251, L_k: 16777216.0000\n",
      "Epoch [2900/10000], Loss: 14.703238, L_k: 32768.0000\n",
      "Epoch [3000/10000], Loss: 14.703223, L_k: 65536.0000\n",
      "Epoch [3100/10000], Loss: 14.703208, L_k: 8192.0000\n",
      "Epoch [3200/10000], Loss: 14.703192, L_k: 8192.0000\n",
      "Epoch [3300/10000], Loss: 14.703175, L_k: 16384.0000\n",
      "Epoch [3400/10000], Loss: 14.703156, L_k: 8192.0000\n",
      "Epoch [3500/10000], Loss: 14.703139, L_k: 65536.0000\n",
      "Epoch [3600/10000], Loss: 14.703125, L_k: 65536.0000\n",
      "Epoch [3700/10000], Loss: 14.703111, L_k: 65536.0000\n",
      "Epoch [3800/10000], Loss: 14.703096, L_k: 65536.0000\n",
      "Epoch [3900/10000], Loss: 14.703082, L_k: 262144.0000\n",
      "Epoch [4000/10000], Loss: 14.703069, L_k: 65536.0000\n",
      "Epoch [4100/10000], Loss: 14.703056, L_k: 262144.0000\n",
      "Epoch [4200/10000], Loss: 14.703043, L_k: 32768.0000\n",
      "Epoch [4300/10000], Loss: 14.703032, L_k: 16384.0000\n",
      "Epoch [4400/10000], Loss: 14.703015, L_k: 32768.0000\n",
      "Epoch [4500/10000], Loss: 14.703002, L_k: 16384.0000\n",
      "Epoch [4600/10000], Loss: 14.702991, L_k: 16384.0000\n",
      "Epoch [4700/10000], Loss: 14.702978, L_k: 32768.0000\n",
      "Epoch [4800/10000], Loss: 14.702967, L_k: 65536.0000\n",
      "Epoch [4900/10000], Loss: 14.702956, L_k: 32768.0000\n",
      "Epoch [5000/10000], Loss: 14.702940, L_k: 4194304.0000\n",
      "Epoch [5100/10000], Loss: 14.702928, L_k: 4194304.0000\n",
      "Epoch [5200/10000], Loss: 14.702918, L_k: 32768.0000\n",
      "Epoch [5300/10000], Loss: 14.702905, L_k: 32768.0000\n",
      "Epoch [5400/10000], Loss: 14.702894, L_k: 262144.0000\n",
      "Epoch [5500/10000], Loss: 14.702884, L_k: 16384.0000\n",
      "Epoch [5600/10000], Loss: 14.702869, L_k: 32768.0000\n",
      "Epoch [5700/10000], Loss: 14.702857, L_k: 32768.0000\n",
      "Epoch [5800/10000], Loss: 14.702847, L_k: 262144.0000\n",
      "Epoch [5900/10000], Loss: 14.702837, L_k: 65536.0000\n",
      "Epoch [6000/10000], Loss: 14.702826, L_k: 16384.0000\n",
      "Epoch [6100/10000], Loss: 14.702813, L_k: 32768.0000\n",
      "Epoch [6200/10000], Loss: 14.702805, L_k: 131072.0000\n",
      "Epoch [6300/10000], Loss: 14.702794, L_k: 16384.0000\n",
      "Epoch [6400/10000], Loss: 14.702781, L_k: 32768.0000\n",
      "Epoch [6500/10000], Loss: 14.702768, L_k: 131072.0000\n",
      "Epoch [6600/10000], Loss: 14.702761, L_k: 32768.0000\n",
      "Epoch [6700/10000], Loss: 14.702749, L_k: 65536.0000\n",
      "Epoch [6800/10000], Loss: 14.702738, L_k: 65536.0000\n",
      "Epoch [6900/10000], Loss: 14.702729, L_k: 262144.0000\n",
      "Epoch [7000/10000], Loss: 14.702719, L_k: 16384.0000\n",
      "Epoch [7100/10000], Loss: 14.702709, L_k: 65536.0000\n",
      "Epoch [7200/10000], Loss: 14.702697, L_k: 131072.0000\n",
      "Epoch [7300/10000], Loss: 14.702683, L_k: 32768.0000\n",
      "Epoch [7400/10000], Loss: 14.702671, L_k: 65536.0000\n",
      "Epoch [7500/10000], Loss: 14.702661, L_k: 16777216.0000\n",
      "Epoch [7600/10000], Loss: 14.702649, L_k: 262144.0000\n",
      "Epoch [7700/10000], Loss: 14.702641, L_k: 32768.0000\n",
      "Epoch [7800/10000], Loss: 14.702631, L_k: 131072.0000\n",
      "Epoch [7900/10000], Loss: 14.702621, L_k: 1048576.0000\n",
      "Epoch [8000/10000], Loss: 14.702613, L_k: 65536.0000\n",
      "Epoch [8100/10000], Loss: 14.702604, L_k: 2097152.0000\n",
      "Epoch [8200/10000], Loss: 14.702595, L_k: 65536.0000\n",
      "Epoch [8300/10000], Loss: 14.702584, L_k: 134217728.0000\n",
      "Epoch [8400/10000], Loss: 14.702573, L_k: 262144.0000\n",
      "Epoch [8500/10000], Loss: 14.702564, L_k: 32768.0000\n",
      "Epoch [8600/10000], Loss: 14.702556, L_k: 32768.0000\n",
      "Epoch [8700/10000], Loss: 14.702544, L_k: 131072.0000\n",
      "Epoch [8800/10000], Loss: 14.702533, L_k: 65536.0000\n",
      "Epoch [8900/10000], Loss: 14.702525, L_k: 32768.0000\n",
      "Epoch [9000/10000], Loss: 14.702514, L_k: 32768.0000\n",
      "Epoch [9100/10000], Loss: 14.702500, L_k: 524288.0000\n",
      "Epoch [9200/10000], Loss: 14.702491, L_k: 65536.0000\n",
      "Epoch [9300/10000], Loss: 14.702478, L_k: 65536.0000\n",
      "Epoch [9400/10000], Loss: 14.702468, L_k: 32768.0000\n",
      "Epoch [9500/10000], Loss: 14.702457, L_k: 131072.0000\n",
      "Epoch [9600/10000], Loss: 14.702450, L_k: 32768.0000\n",
      "Epoch [9700/10000], Loss: 14.702441, L_k: 32768.0000\n",
      "Epoch [9800/10000], Loss: 14.702429, L_k: 32768.0000\n",
      "Epoch [9900/10000], Loss: 14.702419, L_k: 67108864.0000\n",
      "Epoch [10000/10000], Loss: 14.702412, L_k: 16384.0000\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Poids premi√®re couche : tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [-0.2340, -0.0010, -0.0070, -0.0060,  0.3590, -0.0130, -0.0080,  0.0050,\n",
      "         -0.0010, -0.0070],\n",
      "        [-0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "          0.0000, -0.0000]])\n",
      "Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) : 21\n",
      "üîπ Features activ√©es (<1e-4) :\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "p = 10\n",
    "torch.manual_seed(42)\n",
    "layer_sizes = [10, 3, 1]\n",
    "\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "# Seulement les 5 premi√®res dimensions sont utiles s = 5\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "model_ista = ANN_SR_Lasso(layer_sizes, verbose=True)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "def print_weights(model):\n",
    "    first_layer_weights = None\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            first_layer_weights = param.detach().clone()\n",
    "            break\n",
    "    print(\"Poids premi√®re couche :\", np.round(first_layer_weights, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro dans la premi√®re couche (< 1e-3) :\", (first_layer_weights.abs() < 1e-3).sum().item())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model_ista.fit(X_train, Y_train, epochs=10000, L0=1, lmbda=0.3, shuffle=True, batch_size=1000)\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print_weights(model_ista)\n",
    "\n",
    "weights = model_ista.first_linear.weight.detach()\n",
    "active_features = (weights.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\\n\", active_features.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d0b71",
   "metadata": {},
   "source": [
    "Ok alors l√† des difficult√©s √† comprendre ce qui n'allait pas pour comprendre que tout vas bien c'est simplement que je comparer avec avant sauf que pour le lasso c'est MSE qui est utilis√© donc Rien √† voir avec la square root. Mais r√©sultant me semble satisfaisant il y a bien une s√©lection de variable pour un lambda suffisament grand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65a7a8",
   "metadata": {},
   "source": [
    "### D√©veloppement propre d'un mod√®le de r√©seaux de neurones avec optimisation ISTA et regularisation Square-root lasso avec p√©nalisation $\\ell_1$ sur la premi√®re couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4600d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class AnnSRLasso(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.ReLU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i + 1])\n",
    "            layers.append(layer)\n",
    "            \n",
    "            if i == 0:\n",
    "                self.first_layer = layer\n",
    "            \n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation:\n",
    "                layers.append(last_activation())\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def compute_loss(self, out, targets, lmbda):\n",
    "        mse = torch.norm(out - targets, p=2)\n",
    "        l1_reg = torch.norm(self.first_layer.weight, p=1)\n",
    "        return mse + lmbda * l1_reg\n",
    "\n",
    "    def quad_bound(self, y_new, x_old, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y, x, g) in enumerate(zip(y_new, x_old, grad)):\n",
    "            quad += torch.sum(g * (y - x)) + (L / 2) * torch.norm(y - x) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_x, batch_y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "        \n",
    "        while True:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "            \n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(F.softshrink(step, lambd=gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            out = self.forward(batch_x)\n",
    "            F_val = self.compute_loss(out, batch_y, lmbda)\n",
    "            Q_val = self.quad_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                return L, F_val\n",
    "            \n",
    "            L *= eta\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000, L0=1.0, batch_size=None, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in loader:\n",
    "                L_k = L0\n",
    "                out = self.forward(batch_x)\n",
    "                fx = torch.norm(out - batch_y, p=2)\n",
    "\n",
    "                params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in params]\n",
    "                grad = torch.autograd.grad(fx, params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_x, batch_y, lmbda, L_start=L_k, eta=eta)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59d337ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "Poids premi√®re couche : [[ 0.171 -0.263 -0.19  -0.185  0.167 -0.011 -0.29   0.31   0.082  0.291]\n",
      " [ 0.039  0.045  0.228 -0.215  0.299 -0.203  0.029 -0.206 -0.21   0.243]\n",
      " [-0.084 -0.059  0.247  0.31  -0.285 -0.006  0.09   0.304 -0.126 -0.103]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 3.255974, L_k: 64.0000\n",
      "Epoch [200/1000], Loss: 3.228405, L_k: 64.0000\n",
      "Epoch [300/1000], Loss: 2.418235, L_k: 64.0000\n",
      "Epoch [400/1000], Loss: 3.098960, L_k: 64.0000\n",
      "Epoch [500/1000], Loss: 3.436137, L_k: 64.0000\n",
      "Epoch [600/1000], Loss: 2.829231, L_k: 64.0000\n",
      "Epoch [700/1000], Loss: 2.999206, L_k: 128.0000\n",
      "Epoch [800/1000], Loss: 3.303372, L_k: 128.0000\n",
      "Epoch [900/1000], Loss: 2.728492, L_k: 64.0000\n",
      "Epoch [1000/1000], Loss: 3.102011, L_k: 64.0000\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Poids premi√®re couche : [[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.292 -0.046 -0.001 -0.031  0.656 -0.058  0.013  0.027  0.02   0.027]\n",
      " [-0.    -0.     0.     0.     0.    -0.     0.     0.    -0.     0.   ]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 20\n",
      "üîπ Features activ√©es (<1e-4) : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n, p = 1000, 10\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "model = AnnSRLasso([p, 3, 1], verbose=True)\n",
    "\n",
    "def print_weights(model):\n",
    "    w = model.first_layer.weight.detach().cpu().numpy()\n",
    "    print(\"Poids premi√®re couche :\", np.round(w, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-3) :\", (np.abs(w) < 1e-3).sum())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "model.fit(X_train, Y_train, epochs=1000, L0=1.0, lmbda=0.3)\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print_weights(model)\n",
    "\n",
    "w = model.first_layer.weight.detach()\n",
    "active = (w.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\", active.nonzero(as_tuple=True)[0].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ada97c",
   "metadata": {},
   "source": [
    "### D√©veloppement Mod√®le LASSO ANN de l'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd357d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "class LASSOANN(nn.Module):\n",
    "    def __init__(self, sizes, activation=nn.ELU, last_activation=None, verbose=False):\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i + 1])\n",
    "            layers.append(layer)\n",
    "\n",
    "            if i == 0:\n",
    "                self.first_layer = layer\n",
    "\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(activation())\n",
    "            elif last_activation:\n",
    "                layers.append(last_activation())\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def compute_loss(self, out, targets, lmbda):\n",
    "        residual = torch.norm(out - targets, p=2)\n",
    "        l1_reg = torch.norm(self.first_layer.weight, p=1)\n",
    "        return residual + lmbda * l1_reg\n",
    "\n",
    "    def lambda_qut(self, X, M=1000, alpha=0.05, seed=None, sigma_deriv_0=1.0, pi_l=1.0):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        Lambda = []\n",
    "        for _ in range(M):\n",
    "            Y_sim = torch.randn(n, device=X.device)\n",
    "            denom = torch.norm(Y_sim, p=2)\n",
    "            if denom == 0:\n",
    "                continue\n",
    "            lam0 = torch.norm(X.T @ Y_sim / denom, p=float('inf'))\n",
    "            Lambda.append(lam0.item())\n",
    "\n",
    "        lam_qut = pi_l * sigma_deriv_0 * torch.tensor(Lambda).quantile(1 - alpha).item()\n",
    "        return lam_qut\n",
    "\n",
    "    def quad_bound(self, y_new, x_old, grad, L, fx, lmbda):\n",
    "        quad = 0.0\n",
    "        for i, (y, x, g) in enumerate(zip(y_new, x_old, grad)):\n",
    "            quad += torch.sum(g * (y - x)) + (L / 2) * torch.norm(y - x) ** 2\n",
    "            if i == 0:\n",
    "                quad += lmbda * torch.norm(y, 1)\n",
    "        return fx + quad\n",
    "\n",
    "    def ista_step(self, x_old, grad, fx, batch_x, batch_y, lmbda, L_start=1.0, eta=2.0):\n",
    "        L = L_start\n",
    "\n",
    "        while True:\n",
    "            gamma = lmbda / L\n",
    "            y_new = []\n",
    "\n",
    "            for i, (x, g) in enumerate(zip(x_old, grad)):\n",
    "                if i == 0:\n",
    "                    step = x - (1 / L) * g\n",
    "                    y_new.append(F.softshrink(step, lambd=gamma))\n",
    "                else:\n",
    "                    y_new.append(x - (1 / L) * g)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p, y in zip(self.parameters(), y_new):\n",
    "                    p.copy_(y)\n",
    "\n",
    "            out = self.forward(batch_x)\n",
    "            F_val = self.compute_loss(out, batch_y, lmbda)\n",
    "            Q_val = self.quad_bound(y_new, x_old, grad, L, fx, lmbda)\n",
    "\n",
    "            if F_val <= Q_val:\n",
    "                return L, F_val\n",
    "\n",
    "            L *= eta\n",
    "\n",
    "    def fit(self, X, Y, epochs=1000, L0=1.0, batch_size=None, shuffle=True, lmbda=0.1, eta=2.0):\n",
    "        if batch_size is None:\n",
    "            batch_size = len(X)\n",
    "        dataset = TensorDataset(X, Y)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_x, batch_y in loader:\n",
    "                L_k = L0\n",
    "                out = self.forward(batch_x)\n",
    "                fx = torch.norm(out - batch_y, p=2)\n",
    "\n",
    "                params = list(self.parameters())\n",
    "                x_old = [p.clone().detach() for p in params]\n",
    "                grad = torch.autograd.grad(fx, params, create_graph=False)\n",
    "\n",
    "                L_k, F_val = self.ista_step(x_old, grad, fx, batch_x, batch_y, lmbda, L_start=L_k, eta=eta)\n",
    "\n",
    "            if self.verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {F_val.item():.6f}, L_k: {L_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420fe228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Avant Optimisation ISTA\n",
      "pesr : 0\n",
      "tp : 10\n",
      "fp : 0.0\n",
      "f1 : 0.024691358024691357\n",
      "Poids premi√®re couche : [[ 0.171 -0.263 -0.19  -0.185  0.167 -0.011 -0.29   0.31   0.082  0.291]\n",
      " [ 0.039  0.045  0.228 -0.215  0.299 -0.203  0.029 -0.206 -0.21   0.243]\n",
      " [-0.084 -0.059  0.247  0.31  -0.285 -0.006  0.09   0.304 -0.126 -0.103]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 0\n",
      "üîπ Entra√Ænement avec ISTA\n",
      "Epoch [100/1000], Loss: 18.404345, L_k: 512.0000\n",
      "Epoch [200/1000], Loss: 18.097887, L_k: 128.0000\n",
      "Epoch [300/1000], Loss: 18.009583, L_k: 256.0000\n",
      "Epoch [400/1000], Loss: 17.932562, L_k: 1024.0000\n",
      "Epoch [500/1000], Loss: 17.848225, L_k: 512.0000\n",
      "Epoch [600/1000], Loss: 17.795483, L_k: 128.0000\n",
      "Epoch [700/1000], Loss: 17.750362, L_k: 1024.0000\n",
      "Epoch [800/1000], Loss: 17.705883, L_k: 2048.0000\n",
      "Epoch [900/1000], Loss: 17.670773, L_k: 1024.0000\n",
      "Epoch [1000/1000], Loss: 17.648170, L_k: 256.0000\n",
      "üîπ Apr√®s Optimisation ISTA\n",
      "Lambda QUT : 2.7963171005249023\n",
      "pesr : 0\n",
      "tp : 5\n",
      "fp : 0.0\n",
      "f1 : 0.012422360248447204\n",
      "Poids premi√®re couche : [[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.151  0.     0.    -0.002  0.234 -0.004 -0.004 -0.    -0.    -0.   ]\n",
      " [-0.    -0.    -0.    -0.     0.    -0.    -0.    -0.    -0.    -0.   ]]\n",
      "Nombre de poids proches de z√©ro (< 1e-3) : 25\n",
      "üîπ Features activ√©es (<1e-4) : [0, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "n, p = 1000, 10\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(n, p)\n",
    "\n",
    "Y = torch.sin(X[:, 0]) + torch.cos(X[:, 1])**2 + 0.3 * X[:, 2] * X[:, 3] - X[:, 4]\n",
    "Y = Y.unsqueeze(1)\n",
    "\n",
    "X_train, Y_train = X[:800], Y[:800]\n",
    "X_test, Y_test = X[800:], Y[800:]\n",
    "\n",
    "model = LASSOANN([p, 3, 1], verbose=True)\n",
    "\n",
    "def print_weights(model):\n",
    "    w = model.first_layer.weight.detach().cpu().numpy()\n",
    "    print(\"Poids premi√®re couche :\", np.round(w, 3))\n",
    "    print(\"Nombre de poids proches de z√©ro (< 1e-3) :\", (np.abs(w) < 1e-3).sum())\n",
    "\n",
    "print(\"üîπ Avant Optimisation ISTA\")\n",
    "print_weights(model)\n",
    "\n",
    "print(\"üîπ Entra√Ænement avec ISTA\")\n",
    "lambda_qut = model.lambda_qut(X, M=1000, alpha=0.05)\n",
    "model.fit(X, Y, lmbda=lambda_qut, epochs=1000, L0=1.0, shuffle=True, batch_size=None)\n",
    "\n",
    "\n",
    "print(\"üîπ Apr√®s Optimisation ISTA\")\n",
    "print(\"Lambda QUT :\", lambda_qut)\n",
    "print_weights(model)\n",
    "\n",
    "w = model.first_layer.weight.detach()\n",
    "active = (w.abs() > 1e-4).any(dim=0)\n",
    "print(\"üîπ Features activ√©es (<1e-4) :\", active.nonzero(as_tuple=True)[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f37878b",
   "metadata": {},
   "source": [
    "#### Test de l'article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5476b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data_nonlinear(n=500, p=50, h=5, noise_std=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    G√©n√®re des donn√©es simul√©es pour le test non lin√©aire de l‚Äôarticle.\n",
    "    \n",
    "    - Œº(x) = ‚àë_{i=1}^h 10 * |x_{2i} - x_{2i - 1}|\n",
    "    - Support S* = {0, 1, ..., 2h - 1}\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(0, 1, size=(n, p))\n",
    "    \n",
    "    y = np.zeros(n)\n",
    "    for i in range(1, h + 1):\n",
    "        j1, j2 = 2 * i - 2, 2 * i - 1  # indices des variables utilis√©es\n",
    "        y += 10 * np.abs(X[:, j2] - X[:, j1])\n",
    "    \n",
    "    y += rng.normal(0, noise_std, size=n)\n",
    "    \n",
    "    S_true = set(range(2 * h))  # Les colonnes 0 √† 2h - 1\n",
    "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).reshape(-1, 1), S_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "843b30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_variables(W1: torch.Tensor, eps: float = 1e-5) -> set:\n",
    "    \"\"\"\n",
    "    Retourne l‚Äôensemble des indices j tels que la colonne j de W1 est active.\n",
    "    \"\"\"\n",
    "    return {j for j in range(W1.shape[1]) if torch.norm(W1[:, j]) > eps}\n",
    "\n",
    "def pesr(W1: torch.Tensor, S_true: set, eps: float = 0.0) -> float:\n",
    "    \"\"\"\n",
    "    Calcule la PESR : 1 si S_hat == S_true, sinon 0.\n",
    "    \n",
    "    Args:\n",
    "        S_hat (set): ensemble des variables s√©lectionn√©es\n",
    "        S_true (set): ensemble des vraies variables pertinentes\n",
    "\n",
    "    Returns:\n",
    "        float: 1.0 si exact recovery, 0.0 sinon\n",
    "    \"\"\"\n",
    "    S_hat = get_active_variables(W1, eps)\n",
    "    return float(S_hat == S_true)\n",
    "\n",
    "def f1_score(S_hat: set, S_true: set) -> float:\n",
    "    TP = np.sum([1 for i in S_true if i in S_hat])\n",
    "    FP = np.sum([1 for i in S_hat if i not in S_true])\n",
    "    FN = np.sum([1 for i in S_true if i not in S_hat])\n",
    "    denom = 2 * TP + FP + FN\n",
    "    return 2 * TP / denom if denom > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979bdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent.parent.parent.parent)+\"\\AYMEN\\EXPLORATION\\CAS NON LINEAIRE\\OUTILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7b96a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X| h = 0\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 1\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 2\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 3\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 4\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 5\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 6\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 7\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 8\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 9\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n",
      "|X| h = 10\n",
      "|XXX| Simu = 0\n",
      "|XXX| Simu = 1\n",
      "|XXX| Simu = 2\n",
      "|XXX| Simu = 3\n",
      "|XXX| Simu = 4\n",
      "|XXX| Simu = 5\n",
      "|XXX| Simu = 6\n",
      "|XXX| Simu = 7\n",
      "|XXX| Simu = 8\n",
      "|XXX| Simu = 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from non_linear_tools import plot_scores\n",
    "scores = {'pesr': [], 'f1': []}\n",
    "s_range = []\n",
    "\n",
    "simu_iter = 10\n",
    "seed = 42\n",
    "\n",
    "for h in range(0, 11):\n",
    "    n, p = 500, 50\n",
    "    s = 2 * h\n",
    "    pesr_list = []\n",
    "    f1_list = []\n",
    "    print(f\"|X| h = {h}\")\n",
    "    for i in range(simu_iter):\n",
    "        print(f\"|XXX| Simu = {i}\")\n",
    "        X, y, S_true = generate_data_nonlinear(n, p, h=h, seed=seed * i)\n",
    "        model = LASSOANN(sizes=[50, 20, 1], verbose=False)\n",
    "        lmbda = model.lambda_qut(X, M=1000, alpha=0.05)\n",
    "        model.fit(X, y, lmbda=lmbda, epochs=10, L0=0.5)\n",
    "        W1 = model.first_layer.weight.detach()\n",
    "        S_hat = get_active_variables(W1)\n",
    "        pesr_list.append(pesr(W1, S_true))\n",
    "        f1_list.append(f1_score(S_hat, S_true))\n",
    "    scores['pesr'].append(np.mean(pesr_list))\n",
    "    scores['f1'].append(np.mean(f1_list))\n",
    "    s_range.append(s)\n",
    "\n",
    "plot_scores(scores, s_range, \"PESR et F1 Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630076f7",
   "metadata": {},
   "source": [
    "Ok je m'arrete l√† niveau r√©seau j'ai pas exacctement les memes r√©sultat mais c'est bon. Une erreur dans mon F1 score dailleurs qui fais que monter pfff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

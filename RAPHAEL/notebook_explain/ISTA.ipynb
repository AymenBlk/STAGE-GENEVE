{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f58abe19",
   "metadata": {},
   "source": [
    "$\\textbf{GOAL}$ : We want to get a correct ISTA algorithm converging until tolerance given (or not) L value, prox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e064553",
   "metadata": {},
   "source": [
    "*The L-lipschitz constant is known for LASSO but not $\\sqrt{LASSO}$ and surely for other $f$. So we will need **backtrack** : choose a \"large\" L and decreases (dividing by 2) over iterations and see if the loss decreases (accept so L/2).*   \n",
    "\n",
    "*The prox sera toujours donné dans ce projet car c'est un hypothèse que g soit proximable*   \n",
    "\n",
    "TODO : S'assurer de l'hyptothese + def d'être proximable (ça veut surement dire que le prox, selon g, est explicite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cb7a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Printf, Statistics, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae38996",
   "metadata": {},
   "source": [
    "# ISTA\n",
    "\n",
    "$\\textbf{Definition}$\n",
    "Let $\\gamma>0$ and convex function $g : \\mathbb{R}^n \\to \\mathbb{R}$, for $n>0$, we call proximal operator of $g$ with $\\gamma$, \n",
    "$$\n",
    "\\emph{prox}_{\\gamma, g} : v\\in \\mathbb{R}^n \\mapsto \\arg\\min_{x\\in\\mathbb R^{n}}\\;\\Bigl\\{\\,g(x)+\\tfrac1{2\\gamma}\\|x-v\\|_2^2\\Bigr\\}\n",
    "$$\n",
    "\n",
    "\n",
    "Iterative Soft-Thresholding Algorithm (ISTA) is a proximal gradient method, each iteration $(w_k)$ performs a gradient descent step on the smooth loss followed by a soft-thresholding. \n",
    "\n",
    "$\\textbf{Definition}$\n",
    "To minimize the problem $f+g$ with $f$ convex, where its differential is $L$-Lipschitz and g convex. We approximate the minimizer $w^*$ by $(w_k)_{k\\geq0}$ defined as, $w_0$ an initial point and with the update rule, for $\\gamma>0$,\n",
    "    $$\n",
    "    w^{(k+1)} \\;=\\; \\mathrm{prox}_{\\gamma, g}\\!\\Big(w_k - \\gamma\\, \\partial f(w_k) \\Big)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29555d",
   "metadata": {},
   "source": [
    "# Basic LASSO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fc5b0",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "\\min_{b \\in \\mathbb{R}^{p}}\n",
    "\\frac12 \\|y - X b\\|_{2}^{2}\n",
    "+ \\lambda \\|b\\|_{1}\n",
    "\\tag{LASSO}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e1963",
   "metadata": {},
   "source": [
    "So in this first case, we have a optimization problem under the form $f+g$ with $f=MSE$ and $g = \\ell_1$-norm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446cedb",
   "metadata": {},
   "source": [
    "$L = X^TX$ because \n",
    "$$\n",
    "\\|\\nabla f(b) - \\nabla f(a)\\|\n",
    "= \\|-X^{T}(y-X b) + X^{T}(y-X a)\\|\n",
    "= \\|X^{T}X\\,(b - a)\\|\n",
    "\\le \\|X^{T}X\\|\\,\\|b - a\\|$$\n",
    "\n",
    "and then we use $\\ell_2$-norm since it allows to the proximal to be explicit.  \n",
    "So we have the square of largest eigenvalue for $X^TX$ ,\n",
    "\n",
    "$$||X^TX||_2 = ||X||_2^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31954b",
   "metadata": {},
   "source": [
    "**Proximal**\n",
    "$$f : w \\mapsto \\tfrac12\\|Aw-b\\|_2^2 \\quad \\text{and} \\quad g : w \\mapsto \\lambda\\|w\\|_1$$\n",
    "For $k\\geq0$, let $u_k = w_k-\\gamma A^\\top(Aw_k-b)$. \n",
    "Then, $\\forall i\\in \\llbracket 1,n \\rrbracket$,\n",
    "$$\n",
    "w_{k+1}^{(i)}\n",
    "=\\emph{sign}\\bigl(u_k^{(i)}\\bigr)\\,\\max\\bigl(|u_k^{(i)}|-\\gamma\\lambda,0\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfba1e",
   "metadata": {},
   "source": [
    "\n",
    "*Proof*  \n",
    "\n",
    "Since $g$ is separable we have coordinate-wise on $\\mathrm{prox}$, hence, for $i \\in \\llbracket 1,n \\rrbracket$,\n",
    "\\begin{equation}\n",
    "w_{k+1}^{(i)}\n",
    "=\\mathrm{prox}_{\\gamma,\\lambda|\\cdot|}\\bigl(u_k^{(i)}\\bigr) = \n",
    "\\arg\\min_{x\\in\\mathbb{R}} \\Bigl\\{ \\lambda|x|+\\tfrac{1}{2\\gamma}\\Bigl(x-u_k^{(i)}\\Bigr)^2\\Bigr\\} \\tag{R1}\n",
    "\\end{equation}\n",
    "\n",
    "From KKT Theorem, we denote $x^*$ the minimizer of (R1) verifying, $$0 \\in \\partial \\Bigl( x\\mapsto  \\lambda|x|+\\tfrac{1}{2\\gamma}\\Bigl(x-u_k^{(i)}\\Bigr)^ 2\\Bigr)(x^*) = \\lambda \\partial|\\cdot|_1(x^*) + \\frac{1}{\\gamma}(x^*-u_k^{(i)})$$\n",
    "\n",
    "+  $x^{*}>0 \\implies 0 = \\lambda\\cdot1 + \\tfrac{1}{\\gamma}(x^{*}-u_k^{(i)})\n",
    "   \\;\\Longleftrightarrow\\;\n",
    "   x^{*}=u_k^{(i)}-\\gamma\\lambda$ et $u_k^{(i)}>\\gamma\\lambda$\n",
    "   \n",
    "+ $x^{*}<0 \\implies 0 = \\lambda\\cdot1 + \\tfrac{1}{\\gamma}(x^{*}-u_k^{(i)})\n",
    "   \\;\\Longleftrightarrow\\;\n",
    "   x^{*}=u_k^{(i)}-\\gamma\\lambda$ et $u_k^{(i)}<-\\gamma\\lambda$\n",
    "\n",
    "+ $x^{*}=0 \\implies \n",
    "   0\\in \\lambda[-1,1] + \\tfrac{1}{\\gamma}(0-u_k^{(i)})\n",
    "   \\;\\Longrightarrow\\;\n",
    "   u_k^{(i)}\\in[-\\gamma\\lambda,\\;\\gamma\\lambda]\n",
    "   $\n",
    "\n",
    "Combining these three cases gives,\n",
    "\n",
    "$$\n",
    "w_{k+1}^{(i)}\n",
    "=x^{*}\n",
    "=\\begin{cases}\n",
    "u_k^{(i)}-\\gamma\\lambda, & u_k^{(i)}>\\gamma\\lambda\\\\\n",
    "0, & |u_k^{(i)}|\\le\\gamma\\lambda\\\\\n",
    "u_k^{(i)}+\\gamma\\lambda, & u_k^{(i)}<-\\gamma\\lambda\n",
    "\\end{cases}\n",
    "=\\mathrm{sign}\\bigl(u_k^{(i)}\\bigr)\\,\\max\\bigl\\{|u_k^{(i)}|-\\gamma\\lambda,\\;0\\bigr\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e818d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prox (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prox(x, τ) = sign(x) * max(abs(x) - τ, zero(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075fa6eb",
   "metadata": {},
   "source": [
    "### Simple (primal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b73cf",
   "metadata": {},
   "source": [
    "We run the algroithm while the difference value of the cost function (f+g) at two consecutive iteration is below than $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f7140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ista_basic_lasso (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ista_basic_lasso(X, y, λ, prox;\n",
    "        max_iter = 100_000, tol = 1e-9,print_freq = 1000, verbose = true\n",
    "    )\n",
    "    m, p = size(X)\n",
    "    L = opnorm(X)^2 # Lipschitz constant of ∇½‖y-Xβ‖²\n",
    "    step = 1/L #in ]0,2/L[ so linearly convergent\n",
    "    β = zeros(eltype(X), p)\n",
    "    β_next = similar(β)\n",
    "    r = copy(y) # residual\n",
    "    # Initial cost value\n",
    "    cost_prev = Inf\n",
    "    \n",
    "    for k in 1:max_iter\n",
    "        # Gradient step\n",
    "        grad = -(X' * r) # ∇½‖y-Xβ‖²\n",
    "        @. β_next = prox(β - step * grad, λ*step)\n",
    "        mul!(r, X, β_next)\n",
    "        @. r = y - r\n",
    "        cost_current = 0.5*dot(r,r) + λ*sum(abs, β_next)\n",
    "    \n",
    "        if abs(cost_current - cost_prev) < tol\n",
    "            β .= β_next\n",
    "            if verbose\n",
    "                @printf(\"[ISTA] END iter %5d  cost=%.3e  diff=%.3e\\n\", k, cost_current, abs(cost_current - cost_prev))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "        \n",
    "        if verbose && (k == 1 || k % print_freq == 0)\n",
    "            @printf(\"[ISTA]  iter %5d  cost=%.3e  diff=%.3e\\n\", k, cost_current, abs(cost_current - cost_prev))\n",
    "        end\n",
    "        cost_prev = cost_current\n",
    "        β .= β_next\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return β\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e329af",
   "metadata": {},
   "source": [
    "### GAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299ba8c",
   "metadata": {},
   "source": [
    "For my project (screen cleaning), I use ISTA until the solution was $\\epsilon$-optimal (the solution under the primal, there : $f+g$) and this is possible according to the gap function after definind the dual\n",
    "$$\n",
    "  (\\mathcal{D}_\\lambda) : \n",
    "  \\max_{u\\in\\mathbb{R}^m}\\; -\\tfrac12\\|u\\|_{2}^{2} + \\langle u,y\\rangle  \n",
    "  \\quad\\text{s.t.}\\quad \\|A^Tu\\|_\\infty\\le\\lambda$$\n",
    "\n",
    "  and we stop the algorithm when $\\mathcal{G}(x_k,u_k) := P(x_k)-D(y-Ax_k) \\leq \\epsilon$ with $u_k:=y-Ax_k$ the corresponding point in the dual from the primal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f212f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ista_basic_lasso_gap (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ista_basic_lasso_gap(X, y, λ, prox;\n",
    "        max_iter = 100_000,tol = 1e-9,print_freq = 1000, verbose = true\n",
    "    )\n",
    "    m, p  = size(X)\n",
    "    L = opnorm(X)^2 # Lipschitz constant of ∇½‖y-Xβ‖²\n",
    "    step = 1/L #in ]0,2/L[ so linearly convergent\n",
    "    β = zeros(eltype(X), p)\n",
    "    r = copy(y) # residual\n",
    "\n",
    "    for k in 1:max_iter\n",
    "        grad  = -(X' * r) # ∇½‖y-Xβ‖²\n",
    "        @. β  = prox(β - step * grad, λ*step)\n",
    "        mul!(r, X, β) # r = Xβ\n",
    "        @. r = y - r\n",
    "        primal = 0.5*dot(r,r) + λ*sum(abs,β)\n",
    "        θ = r .* min(one(eltype(X)), λ / maximum(abs.(X' * r)))  # dual projection\n",
    "        dual = 0.5*dot(y,y) - 0.5*dot(y .- θ, y .- θ)\n",
    "        gap = max(primal - dual, 0.0) # >= 0\n",
    "\n",
    "        if verbose && (k == 1 || k % print_freq == 0)\n",
    "            @printf(\"[ISTA]  iter %5d  gap=%.3e\\n\", k, gap)\n",
    "        end\n",
    "        \n",
    "        if verbose && gap < tol\n",
    "            @printf(\"[ISTA] END iter %5d cost=%.3e gap=%.3e\\n\", k, primal, gap)\n",
    "            break # or return\n",
    "        end\n",
    "    end\n",
    "    return β\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26aec3",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ae2f9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, p = 100, 50\n",
    "Random.seed!(42)\n",
    "X = randn(n, p)\n",
    "sigma = 0.1\n",
    "y = X * randn(p) + sigma * randn(n)\n",
    "λ = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bfe0890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ISTA]  iter     1  cost=7.895e+02  diff=Inf\n",
      "[ISTA] END iter   259  cost=4.280e+00  diff=9.353e-10\n",
      "[ISTA]  iter     1  gap=7.876e+02\n",
      "[ISTA] END iter   672 cost=4.280e+00 gap=9.652e-10\n"
     ]
    }
   ],
   "source": [
    "beta_1 = ista_basic_lasso(X, y, λ, prox, tol=1e-9)\n",
    "beta_2 = ista_basic_lasso_gap(X, y, λ, prox, tol=1e-9)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab966b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4914334963922471e-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maximum(abs.(beta_1 - beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bce1178f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100×50 Matrix{Float64}:\n",
       " -0.363357      1.70818    -0.601298    …  -0.963881  -0.263208   -0.608203\n",
       "  0.251737     -1.03105     0.0701926      -0.866058   1.76491    -1.15529\n",
       " -0.314988     -0.144414   -1.78484         0.415071  -0.401666   -0.306233\n",
       " -0.311252     -1.62692    -0.166235       -0.830509   0.395066    0.217733\n",
       "  0.816307     -0.0621749  -1.74545        -0.61771    0.0689813   0.989967\n",
       "  0.476738      0.214392   -0.980311    …  -1.98395   -0.560384    0.141943\n",
       " -0.859555      1.52944     1.16886         0.84472    0.756063    0.100051\n",
       " -1.46929       0.274373    0.920525        1.83407   -1.22233     0.0582548\n",
       " -2.11433       1.72908     0.460454        0.966417  -0.664859    0.0794762\n",
       "  0.0437817    -0.878513    0.253296        0.913405   1.16119    -0.0985575\n",
       "  ⋮                                     ⋱                         \n",
       " -0.000939762   0.566839   -0.675231       -0.714065   3.09126     0.924723\n",
       " -1.50078       0.717872    1.15522        -0.12098    0.0609324  -2.19035\n",
       "  0.262925     -1.11375    -0.0930658      -0.113415  -1.28752    -1.5434\n",
       " -1.06112      -0.807109    0.509414        0.481431  -1.59372     0.855101\n",
       "  1.03642      -0.80858     0.882482    …  -0.66046   -0.512621   -0.848564\n",
       "  0.723465      1.79954     0.4973         -0.678485   0.428996   -0.21825\n",
       "  0.85416      -0.844068   -2.29264         1.15421   -0.950393    0.0570421\n",
       "  0.368207     -1.52932     1.55087         1.295      1.50624     0.823258\n",
       " -0.0465609    -0.195099    0.00921224      0.885279   0.170773    0.112328"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e35b5c",
   "metadata": {},
   "source": [
    "# General function *(f, g, L, prox given)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb71509",
   "metadata": {},
   "source": [
    "Precedents functions use primal, dual which depend on f and g, so we give, inspired from the first algorithm, ISTA function with prox given and the L Lispitchz constant given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9b96f12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ista_L (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function ista_L(x0, f, g, ∇f, L, prox;\n",
    "        max_iter = 100_000, tol = 1e-9,print_freq = 1000, verbose = true\n",
    "    )\n",
    "    x = copy(x0)\n",
    "    x_next = similar(x)\n",
    "    \n",
    "    step = 1/L\n",
    "    cost_prev = f(x) + g(x)\n",
    "    for k in 1:max_iter\n",
    "        grad = ∇f(x)\n",
    "        @. x_next = prox(x-step*grad, step)\n",
    "        \n",
    "        cost_current = f(x_next) + g(x_next)\n",
    "        if abs(cost_current - cost_prev) <tol\n",
    "            x .= x_next\n",
    "            if verbose\n",
    "                @printf(\"[ISTA] END iter %5d  cost=%.3e  diff=%.3e\\n\", k, cost_current, abs(cost_current - cost_prev))\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "        if verbose && (k == 1 || k % print_freq == 0)\n",
    "            @printf(\"[ISTA] iter %5d  cost=%.3e  diff=%.3e\\n\", k, cost_current, abs(cost_current - cost_prev))\n",
    "        end\n",
    "    \n",
    "        cost_prev = cost_current\n",
    "        x .= x_next\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d20b2f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f344bd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, p = 100, 50\n",
    "Random.seed!(42)\n",
    "X = randn(n, p)\n",
    "sigma = 0.1\n",
    "y = X * randn(p) + sigma * randn(n)\n",
    "λ = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29015814",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cb19d",
   "metadata": {},
   "source": [
    "On est censé tombé sur les memes résultats que précédemment. C'est le cas, la différence est nulle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fbed715e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ISTA] iter     1  cost=7.895e+02  diff=1.733e+03\n",
      "[ISTA] END iter   259  cost=4.280e+00  diff=9.353e-10\n",
      "Différence entre beta_L et beta_1 : 0.0\n"
     ]
    }
   ],
   "source": [
    "f(b) = 0.5*norm(y-X*b,2)^2\n",
    "∇f(b) = -X'*(y - X*b)\n",
    "g(x) = λ*sum(abs, x) # L1 norm\n",
    "prox_L(x, step) = sign(x) * max(abs(x) - λ*step, zero(x))\n",
    "\n",
    "beta0 = zeros(p)# initial conditions\n",
    "\n",
    "beta_L = ista_L(beta0, f, g, ∇f, opnorm(X)^2, prox_L, tol=1e-9)\n",
    "\n",
    "println(\"Différence entre beta_L et beta_1 : \", maximum(abs.(beta_L.-beta_1))) # beta_1 was defined earlier and has to be the same as beta_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2186e55",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0aaf55",
   "metadata": {},
   "source": [
    "TODO : find interesting problem with confirmed (known / analytic) result to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a80c7",
   "metadata": {},
   "source": [
    "# General function BackTrack *(f, g, prox given)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10dc43",
   "metadata": {},
   "source": [
    "We have to guess $L$ Lispichtz constant by consider a first \"\"large\"\" (??) value.  \n",
    "\n",
    "Then at each iteration, TODO : read FISTA pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f40c697",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

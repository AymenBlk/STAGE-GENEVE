{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a30c8a7",
   "metadata": {},
   "source": [
    "$\\textbf{GOAL}$ : We want to get a correct CD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df187eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Printf, Statistics, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503c887",
   "metadata": {},
   "source": [
    "# Coordinate Descent (CD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49918b0",
   "metadata": {},
   "source": [
    "\n",
    "Let $f:\\mathbb R^{n}\\to\\mathbb R$ $C^1$, convex and with gradient of f $L_{max}$-Lispschitz. Then a penalty function separable $g$ such as we want minimize $F:=f+\\lambda g$, for a hyperparameter $\\lambda$, \n",
    "\n",
    "Typical choices of separable penalty $\\Omega_i(t)=|t|$ ($\\ell\\_1$‐norm), $\\Omega_i=\\iota_{[a_i,b_i]}$ (indicator of a coordinate box)\n",
    "\n",
    "For each \\$i\\in{1,\\dots,n}\\$ define the smallest \\$L\\_i>0\\$ such that, $\\forall x\\in\\mathbb R^{n},t\\in\\mathbb R$, \n",
    "$\n",
    "\\bigl|[\\partial f(x+te_i)]_i-[\\partial f(x)]_i\\bigr|\n",
    "\\leq L_i|t|\n",
    "$\n",
    "and we set  $L_{\\max}:=\\max_{1\\le i\\le n}L_i$\n",
    "\n",
    "\n",
    "**Property** (Pourquoi celle là ? jsp mais je la trouve intéressante donc je la note) [**Strong** convexity]\n",
    "If there exists, $\\forall x,y$,  $\\sigma>0$ with\n",
    "$$ f(y)\\ge f(x)+\\partial f(x)^{\\top}(y-x)+\\tfrac{\\sigma}{2}\\|y-x\\|_2^{2}$$\n",
    "then $f$ is $ \\sigma $-strongly convex.\n",
    "\n",
    "*We prefer a large $\\sigma$ to converge faster (strong curvature) than low $sigma$. Moreover, we are only convex if $\\sigma=0$.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a1f79",
   "metadata": {},
   "source": [
    "The main idea of the method is to optimize one dimension at the time. So, we fix $i \\in \\llbracket 1,n\\rrbracket$ and we optimise from $x\\in \\mathbb{R}^n$ to $y:=x+(\\chi-x_i)e_i$ where $\\chi \\in \\mathbb{R}$. We have to find $\\chi$.\n",
    "\n",
    "By Lipschitzity of the gradient of L using Taylor expansion,   \n",
    "$$\n",
    "f\\bigl(y)\\le\\ f(x)+ [\\nabla f(x)]_i(\\chi-x_i) + \\frac{L_i}{2}(\\chi-x_i)^2\n",
    "$$\n",
    " \n",
    "So,\n",
    "$[F(y)-F(x)]_i \\leq  [\\partial f(x)]_i(\\chi-x_i)+\\frac{L_i}{2}(\\chi-x_i)^2 +\\lambda\\bigl[\\Omega_i(\\chi)-\\Omega_i(x_i)\\bigr] $.\n",
    "Then, we want the smallest upper bound, that's to say for $\\alpha\\le 1/L_i$,\n",
    "$$\\chi^* := \\argmin_\\chi [\\partial f(x)]_i(\\chi-x_i)+\\frac{1}{2\\alpha}(\\chi-x_i)^2+\\lambda\\Omega_i(\\chi) $$\n",
    "So we have for this $\\chi^*$ (optimal) descending iteration $F(y)-F(x) \\leq 0 \\Longleftrightarrow F(x+(\\chi^*-x_i)e_i) \\leq F(x)$\n",
    "\n",
    "We can remark that, $$\\chi^* = \\operatorname{prox}_{\\alpha,\\lambda\\Omega_i}\\!\\bigl(x_i-\\alpha[\\nabla f(x)]_i\\bigr)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902081b3",
   "metadata": {},
   "source": [
    "In the following we use the cyclic algorithm 2 of\n",
    "> Coordinate Descent Algorithms, Stephen J. Wright *(https://arxiv.org/pdf/1502.04759)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3611df6",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914975dd",
   "metadata": {},
   "source": [
    "To resolve LASSO, we need to find these $L_i$ and the explicit form of the proximal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fcc5f",
   "metadata": {},
   "source": [
    "Let $X \\in \\mathbb{R}^{n \\times p}$ and $y \\in \\mathbb{R}^{n}$\n",
    "\n",
    "$$\n",
    "\\min_{b \\in \\mathbb{R}^{p}}\n",
    "\\frac12 \\|y - X b\\|_{2}^{2}\n",
    "+ \\lambda \\|b\\|_{1}\n",
    "\\tag{LASSO}\n",
    "$$\n",
    "\n",
    "So in this first case, we have a optimization problem under the form $f+g$ with $f=MSE$ and $g = \\ell_1$-norm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5b2e3",
   "metadata": {},
   "source": [
    "We have for each coordinate $i$, $\\partial f(x)_i=X_i^{\\top}(Xb-y)$ *($X_i$ represent the i-th column of X)*\n",
    "\n",
    "$$[\\partial f(x+te_i)]_i-[\\partial f(x)]_i=X_i^{\\top}\\!\\bigl(X(b+te_i)-y\\bigr)-X_i^{\\top}(Xb-y)=tX_i^{\\top}X_i=t\\|X_i\\|_2^{2}\n",
    "$$\n",
    "So $\\forall x,t$ and setting $L_i:=\\|X_i\\|_2^{2}$ \n",
    "\n",
    "$$|[\\partial f(x+te_i)]_i-[\\partial f(x)]_i|\\le L_i|t|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98455136",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5d237",
   "metadata": {},
   "source": [
    "\n",
    "$\\textbf{Proposition} $\n",
    "Let $x\\in\\mathbb R^{n}$, an index i and a step $0<\\alpha\\le \\frac{1}{L_i}$ with $u:=x_i-\\alpha[\\partial f(x)]_i$  \n",
    "So, $$\\chi^* =\\operatorname{sign}(u)\\max\\{|u|-\\alpha\\lambda,0\\}$$\n",
    "\n",
    "*Proof.*  \n",
    "$$\n",
    "   [\\partial f(x)]_i(\\chi-x_i) + \\frac{1}{2\\alpha}(\\chi-x_i)^2= \n",
    "   \\frac{1}{2\\alpha}\\bigl(\\chi - (x_i-\\alpha[\\partial f(x)]_i)\\bigr)^2- \\frac{\\alpha}{2}[\\partial f(x)]_i^{2}\n",
    "   = \\frac{1}{2\\alpha}(\\chi-u)^2 - \\frac{\\alpha}{2}[\\partial f(x)]_i^{2}\n",
    "$$\n",
    " \n",
    "The minimisation reduces to (last term is constant according to $\\chi$)  \n",
    "$$\n",
    "     \\chi^\\star = \\argmin_{\\chi}\\Bigl\\{ \\tfrac12(\\chi-u)^2 + \\alpha\\lambda|\\chi| \\Bigr\\} = \\operatorname{prox}_{\\alpha,\\lambda|\\cdot|}(u)\n",
    "$$\n",
    "\n",
    "And we know develop the expression of this proximal : See ISTA.ipynb (in RAPHAEL) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26c9cb2",
   "metadata": {},
   "source": [
    "# General function (f, g, L, prox given)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f6093",
   "metadata": {},
   "source": [
    "Can be improve because we could pass an array of function for the gradient of f for instance instead of computing the gradient of f on all coordinate and take only the i-th coordinate.\n",
    "\n",
    "In the project we don't use massively, only to assure ISTA algorithm converges so we don't need performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d81b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cd_L (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cd_L(x0, f, g, ∇f, L, prox;\n",
    "        max_iter = 100_000, tol = 1e-9,print_freq = 1000, verbose = true\n",
    "    )\n",
    "    x = copy(x0)\n",
    "    n = length(x)\n",
    "    Li = (L isa AbstractVector) ? L : fill(L, n) # is an array. Each entry for each coordinate\n",
    "    cost_prev = f(x) +g(x)\n",
    "    epoch = 0\n",
    "\n",
    "    for k in 1:max_iter\n",
    "        i = (mod(k-1, n) + 1) # cyclic\n",
    "\n",
    "        grad_i = ∇f(x)[i] \n",
    "        step = 1 / Li[i]\n",
    "        z_i = prox(x[i] - step * grad_i, step)\n",
    "        x[i] = z_i \n",
    "        if i == n # finished a full sweep (one epoch)\n",
    "            epoch += 1\n",
    "\n",
    "            cost_current = f(x) + g(x)\n",
    "            diff         = abs(cost_current - cost_prev)\n",
    "\n",
    "            if verbose && (epoch == 1 || epoch % print_freq == 0)\n",
    "                @printf(\"[CD]  epoch %6d  cost = %.3e   diff = %.3e\\n\",epoch, cost_current, diff)\n",
    "            end\n",
    "\n",
    "            if diff < tol\n",
    "                if verbose\n",
    "                    @printf(\"[CD]  END   epoch %6d  cost = %.3e   diff = %.3e\\n\",epoch, cost_current, diff)\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "            cost_prev = cost_current\n",
    "        end\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7f5f4",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "387cfae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, p = 100, 50\n",
    "Random.seed!(42)\n",
    "X = randn(n, p)\n",
    "sigma = 0.1\n",
    "y = X * randn(p) + sigma * randn(n)\n",
    "λ = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70b981",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87744625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CD]  epoch      1  cost = 3.806e+02   diff = 2.142e+03\n",
      "[CD]  END   epoch     46  cost = 4.280e+00   diff = 8.367e-10\n",
      "[CD]  epoch      1  cost = 3.480e+02   diff = 2.175e+03\n",
      "[CD]  END   epoch     81  cost = 4.280e+00   diff = 9.842e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50-element Vector{Float64}:\n",
       " -0.22167751066748717\n",
       " -0.8107640338613908\n",
       "  1.2197351546784327\n",
       "  2.1296541867440673\n",
       "  0.8582180832818391\n",
       " -0.31706472256486956\n",
       "  0.6494017717295051\n",
       " -0.2791655374119629\n",
       "  1.1051331043116277\n",
       " -0.1107449431558722\n",
       "  ⋮\n",
       "  0.26028211088965736\n",
       "  2.000041131330912\n",
       " -0.27439979683330834\n",
       " -1.1161814851235266\n",
       " -1.231318360750697\n",
       "  0.11211728011527106\n",
       " -0.9711042311174327\n",
       " -0.32961665412223606\n",
       " -0.15896085918977879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f(b) = 0.5*norm(y-X*b,2)^2\n",
    "∇f(b) = -X'*(y - X*b)\n",
    "g(x) = λ*sum(abs, x) # L1 norm\n",
    "prox_L(x, step) = sign.(x) .* max.(abs.(x) .- λ*step, zero(x))\n",
    "l_L = [norm(X[:, j],2)^2 for j in 1:p]\n",
    "Lmax = maximum([norm(X[:, j])^2 for j in 1:p])\n",
    "\n",
    "\n",
    "beta0 = zeros(p)# initial conditions\n",
    "beta_L_l = cd_L(beta0, f, g, ∇f, l_L, prox_L, tol=1e-9)\n",
    "beta_L_max = cd_L(beta0, f, g, ∇f, Lmax, prox_L, tol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd44b0a",
   "metadata": {},
   "source": [
    "It's good ! We have same results than ISTA and using the adapated L constants instead of Lmax is accelarating well convergence of the algorithm !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dab756",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9d73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../../comp_translate/SaveLoadTxt.jl\") # Assuming you have a SaveLoadTxt.jl file with the necessary functions\n",
    "using .SaveLoadTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d16cc1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17296b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 100, 50\n",
    "sigma = 0.1\n",
    "X, y = load_txt(\"../../comp_translate/data/ISTA.txt\")\n",
    "λ = 0.1\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440cf49",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883f64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CD]  epoch      1  cost = 3.806e+02   diff = 2.142e+03\n",
      "[CD]  END   epoch     46  cost = 4.280e+00   diff = 8.367e-10\n",
      "[CD]  epoch      1  cost = 3.480e+02   diff = 2.175e+03\n",
      "[CD]  END   epoch     81  cost = 4.280e+00   diff = 9.842e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50-element Vector{Float64}:\n",
       " -0.22167751066748717\n",
       " -0.8107640338613908\n",
       "  1.2197351546784327\n",
       "  2.1296541867440673\n",
       "  0.8582180832818391\n",
       " -0.31706472256486956\n",
       "  0.6494017717295051\n",
       " -0.2791655374119629\n",
       "  1.1051331043116277\n",
       " -0.1107449431558722\n",
       "  ⋮\n",
       "  0.26028211088965736\n",
       "  2.000041131330912\n",
       " -0.27439979683330834\n",
       " -1.1161814851235266\n",
       " -1.231318360750697\n",
       "  0.11211728011527106\n",
       " -0.9711042311174327\n",
       " -0.32961665412223606\n",
       " -0.15896085918977879"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f(b) = 0.5*norm(y-X*b,2)^2\n",
    "∇f(b) = -X'*(y - X*b)\n",
    "g(x) = λ*sum(abs, x) # L1 norm\n",
    "prox_L(x, step) = sign.(x) .* max.(abs.(x) .- λ*step, zero(x))\n",
    "l_L = [norm(X[:, j])^2 for j in 1:p]\n",
    "Lmax = maximum([norm(X[:, j])^2 for j in 1:p])\n",
    "\n",
    "\n",
    "beta0 = zeros(p)# initial conditions\n",
    "beta_L_l = cd_L(beta0, f, g, ∇f, l_L, prox_L, tol=1e-9)\n",
    "beta_L_max = cd_L(beta0, f, g, ∇f, Lmax, prox_L, tol=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e429d",
   "metadata": {},
   "source": [
    "# General function BackTrack *(f, g, $L_0$, prox given)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc47efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

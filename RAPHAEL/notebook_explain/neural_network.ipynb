{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d50c1f",
   "metadata": {},
   "source": [
    "$\\textbf{GOAL}$ Create from scratch a fully connected neural network (ANN) and get exact result than library already integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fbf1a",
   "metadata": {},
   "source": [
    "**This notebook is nota at all the main subject of the intership, more and better explaination exists other than here. Its more a compilation of ideas on neural network and code test than a explicit course.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e232cc",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>TODO :</b></span> Je pense un probleme sur la matrice de poids du from scratch. \n",
    "\n",
    "On doit tout reprendre proprement avec des tests solides.\n",
    "\n",
    "Pas assez convergent sur le cas linéaire. \n",
    "Peut-être rester sur du Lux et après voir pourquoi le from scratch ne fonctionne pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c5b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Random, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6ec3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../functions/ISTA.jl\")\n",
    "\n",
    "import .ISTA: ista_L, ista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007cd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Dense_scratch\n",
    "    W::Matrix{Float64}\n",
    "    b::Vector{Float64}\n",
    "    ς::Function\n",
    "    ς′::Function\n",
    "end\n",
    "\n",
    "mutable struct MLP\n",
    "    layers::Vector{Dense_scratch}\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bfe4e",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd85ede",
   "metadata": {},
   "source": [
    "## Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5c783",
   "metadata": {},
   "source": [
    "We need ANN because it allows to modelize non linear function from ther activation function which are non linear.\n",
    "\n",
    "**Definition.**\n",
    "For an input $x\\in\\mathbb R^{p_1}$ and an integer $l\\ge 1$ (total layers), a *fully-connected ANN* is the composite map\n",
    "\n",
    "$$\n",
    "\\mu_\\theta(x) =S_l\\circ S_{l-1}\\circ\\cdots\\circ S_{1}(x)\n",
    "$$\n",
    "\n",
    "with parameters\n",
    "$$\n",
    "\\theta = \\bigl(W_1, (W_2,\\dots,W_l,b_1, \\dots, b_l)\\bigr) =(\\theta^{(1)},\\theta^{(2)})$$\n",
    "\n",
    "where, for $k=\\llbracket 1, l-1 \\rrbracket$\n",
    "$$\n",
    "S_k(u)=\\varsigma\\!\\bigl(b_k+W_k u\\bigr),\n",
    "\\qquad\n",
    "S_l(u)=b_l+W_l u,\n",
    "$$\n",
    "\n",
    "$W_k\\in\\mathbb R^{p_{k+1}\\times p_k}$, $b_k\\in\\mathbb R^{p_{k+1}}$, and $p_{l+1}=1$ for scalar regression ($b_l = c$ the intercept)\n",
    "\n",
    "From ``lambda_qut.ipynb`` we need assumptions.  \n",
    "<span style=\"color:red\"><b>TODO :</b></span>  Les hypotheses sont plausibles mais pas sûr de ce que j'avance.  \n",
    "To keep the zero-function reachable and allow QUT-compatible sparsity we impose \n",
    "$\\varsigma\\in C^2$, unbounded, $L$-Lipschitz, $\\varsigma(0)=0$ and $\\varsigma'(0)>0$ like shifted-ReLU ($ReLU_\\alpha$), $ELU_\\alpha$, Softplus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfa6fc",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03915fa",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e72ba",
   "metadata": {},
   "source": [
    "Given a loss function we need to minimize the error made by the learner $\\mu$, so we will optimize it by **BackPropagation**\n",
    "\n",
    "On theLASSO problem (from chatGPT), for any weight $W_k$,\n",
    "$$\n",
    "\\partial_{W_k} \\tfrac12\\|y-\\mu_\\theta(X)\\|_2^2\n",
    "=\n",
    "\\Bigl(J_{S_{k+1:l}}(z_k)^\\top\\bigl(\\mu_\\theta(X)-y\\bigr)\\Bigr)z_{k-1}^\\top,\n",
    "$$\n",
    "\n",
    "where $z_{k-1}=S_{k-1}\\circ\\cdots\\circ S_1(X)$ and $J_{S_{k+1:l}}$ is the Jacobian of the suffix $S_l\\circ\\cdots\\circ S_{k+1}$. This is the usual chain-rule implemented by back-prop.\n",
    "\n",
    "\n",
    "\n",
    "I've written this few years ago, it can be helpful.  \n",
    "Let $n\\in \\mathbb{N}^*$ be the sample size of data, $X_i \\in \\mathbb{R}^{N_0}$ be one input data and $y_i, \\hat{y}_i \\in \\mathbb{R}^{N_L}$ be, respectively, the true output associated with $X_i$ and the prediction inferred by the model (in this case, the perceptron) to the $i$-th data element. \n",
    "\n",
    "\n",
    "\n",
    "We have via the chain rule,\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial Z_i} \\cdot \\frac{\\partial Z_i}{\\partial w_{ij}}$$  \n",
    "\n",
    "\n",
    "We can rewrite it to get a general code later, for one parameter block the full chain rule by fixing a layer index $1\\le \\ell\\le L$.\n",
    "The loss is $\\mathcal L\\bigl(a^{(L)},y\\bigr)$ with $y$ the target.\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W^{(\\ell)}} \n",
    "= \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial a^{(L)}}\n",
    "\\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "\\cdot \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\n",
    "\\cdots\n",
    "\\frac{\\partial a^{(\\ell)}}{\\partial z^{(\\ell)}}\n",
    "\\cdot \\frac{\\partial z^{(\\ell)}}{\\partial W^{(\\ell)}}\n",
    "$$\n",
    "\n",
    "Define the error signal,\n",
    "$$\n",
    "\\delta^{(\\ell)} := \\frac{\\partial \\mathcal L}{\\partial z^{(\\ell)}} \\in \\mathbb{R}^{n_\\ell}.\n",
    "$$\n",
    "\n",
    "By factorising the long product above, we obtain the recursive form,\n",
    "$$\n",
    "\\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial a^{(L)}} \n",
    "\\odot \\sigma^{(L)\\prime}\\big(z^{(L)}\\big)\n",
    "$$\n",
    "\n",
    "$$\\delta^{(\\ell)} = \\bigl(W^{(\\ell+1)}\\bigr)^\\top \\delta^{(\\ell+1)}\n",
    "               \\odot \\sigma^{(\\ell)\\prime}\\bigl(z^{(\\ell)}\\bigr),\n",
    "               \\qquad \\ell = L-1,\\dots,1$$\n",
    "\n",
    "So, we got for all parameters,\n",
    "$$\n",
    "\n",
    "\\frac{\\partial\\mathcal L}{\\partial W^{(\\ell)}}= \\delta^{(\\ell)} \\, \\big(a^{(\\ell-1)}\\big)^\\top$$\n",
    "$$\\frac{\\partial\\mathcal L}{\\partial b^{(\\ell)}}\n",
    "  = \\delta^{(\\ell)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375e622",
   "metadata": {},
   "source": [
    "### Non convexity of ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8551df",
   "metadata": {},
   "source": [
    "The optimisation is not convexe in despite we have convex activation function.\n",
    "In some case we can expect convex problem but it's too restrictive\n",
    "> https://stats.stackexchange.com/questions/499120/are-there-any-convex-neural-networks  \n",
    "\n",
    "> Input Convex Neural Networks  \n",
    "Brandon Amos 1 Lei Xu 2 * J. Zico Kolter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5d8d7",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95afc7",
   "metadata": {},
   "source": [
    "### Basic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bfa6f",
   "metadata": {},
   "source": [
    "$$\n",
    "W_{t+1} = W_t - \\eta \\nabla W_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c32975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sgd_scratch_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To keep the same syntax as below but i agree is too much for nothing\n",
    "\n",
    "mutable struct SGD_scratch\n",
    "end\n",
    "\n",
    "function SGD_scratch(model::MLP)\n",
    "    SGD_scratch()\n",
    "end\n",
    "\n",
    "function sgd_scratch_update!(model::MLP, grads, opt::SGD_scratch, η)\n",
    "    @assert length(model.layers) == length(grads)\n",
    "    for (L, (dW, db)) in zip(model.layers, grads)\n",
    "        L.W .-= η .* dW          # every layer\n",
    "        L.b .-= η .* db\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976dbd5",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3fbfc",
   "metadata": {},
   "source": [
    "So we will use the optimizer Adam because it works very well, $W_t$ is the gradient of weights and biases at the $t$-th epochs  \n",
    "$\\eta$ is the learing rate\n",
    "\n",
    "$$m_0 = 0, \\quad v_0 = 0, \\quad 1 > \\beta_1, \\beta_2 \\geq 0, \\quad \\epsilon >0$$\n",
    "$$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\partial W_t\n",
    "$$\n",
    "$$\n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\partial W_t)^2\n",
    "$$\n",
    "$$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "   W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t + \\epsilon}} \\hat{m}_t\n",
    "$$\n",
    "\n",
    "> https://arxiv.org/pdf/1412.6980   \n",
    "\n",
    "> https://arxiv.org/pdf/1904.09237   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d443fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adam_scratch_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutable struct Adam_scratch\n",
    "    m::Vector{Matrix}; v::Vector{Matrix}\n",
    "    mb::Vector{Vector}; vb::Vector{Vector}\n",
    "    β1::Float64; β2::Float64; ε::Float64; t::Int\n",
    "end\n",
    "\n",
    "function Adam_scratch(model::MLP; β1=.9, β2=.999, ε=1e-8)\n",
    "    L = length(model.layers)\n",
    "    Adam_scratch([zeros(size(L.W)) for L in model.layers],\n",
    "         [zeros(size(L.W)) for L in model.layers],\n",
    "         [zeros(size(L.b)) for L in model.layers],\n",
    "         [zeros(size(L.b)) for L in model.layers],\n",
    "         β1, β2, ε, 0)\n",
    "end\n",
    "\n",
    "function adam_scratch_update!(model::MLP, grads, opt::Adam_scratch, η)\n",
    "    opt.t += 1\n",
    "    for (ℓ,(dW,db)) in enumerate(grads)\n",
    "        L = model.layers[ℓ]\n",
    "\n",
    "        # weights\n",
    "        opt.m[ℓ] .= opt.β1 .* opt.m[ℓ] .+ (1-opt.β1).*dW\n",
    "        opt.v[ℓ] .= opt.β2 .* opt.v[ℓ] .+ (1-opt.β2).* (dW.^2)\n",
    "        m̂ = opt.m[ℓ] ./ (1 - opt.β1^opt.t)\n",
    "        v̂ = opt.v[ℓ] ./ (1 - opt.β2^opt.t)\n",
    "        L.W .-= η .* m̂ ./ (sqrt.(v̂) .+ opt.ε)\n",
    "\n",
    "        # biases\n",
    "        opt.mb[ℓ] .= opt.β1 .* opt.mb[ℓ] .+ (1-opt.β1).*db\n",
    "        opt.vb[ℓ] .= opt.β2 .* opt.vb[ℓ] .+ (1-opt.β2).* (db.^2)\n",
    "        m̂b = opt.mb[ℓ] ./ (1 - opt.β1^opt.t)\n",
    "        v̂b = opt.vb[ℓ] ./ (1 - opt.β2^opt.t)\n",
    "        L.b .-= η .* m̂b ./ (sqrt.(v̂b) .+ opt.ε)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09450268",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d593284",
   "metadata": {},
   "source": [
    "There are good ways to initialize weight matrices and bias vectors depending on the activations functions.  \n",
    "<span style=\"color:red\"><b>TODO :</b></span> comment on initialise dans notre cas ? car c'est pas ReLU nous masi $ReLU_\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0dc2fd",
   "metadata": {},
   "source": [
    "# From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d047ab",
   "metadata": {},
   "source": [
    "I define MLP at the start of the notebook because it allows to get Adam in the good section and execute all cells from the button \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16a73e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dIdentity (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu(x)  = max.(0, x)\n",
    "drelu(x) = x .> 0\n",
    "\n",
    "relu_shift(x; α=1.0) = max(0.0,  x - α)\n",
    "drelu_shift(x; α=1.0) = x > α ? 1.0 : 0.0 \n",
    "\n",
    "softplus(x)  = log1p(exp(x))\n",
    "dsoftplus(x) = 1/(1+exp(-x))\n",
    "\n",
    "elu(x; α=1.0)  = x ≥ 0 ? x : α*(exp(x)-1)\n",
    "delu(x; α=1.0) = x ≥ 0 ? 1.0 : α*exp(x)\n",
    "\n",
    "Identity(α=1.0) = (x) -> x\n",
    "dIdentity(α=1.0) = (x) -> 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ae09c9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_mlp"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    build_mlp(sizes::Vector{Int}, activations::Vector{Pair})\n",
    "\n",
    "`activations[k] = σ=>σ′` for hidden layer k (1-based).  \n",
    "The output layer is linear (regression linear).\n",
    "\"\"\"\n",
    "function build_mlp(sizes::Vector{Int},activations::Vector{<:Pair{<:Function,<:Function}};rng=Random.default_rng())\n",
    "    Random.seed!(42)\n",
    "    L = length(sizes)-1\n",
    "    layers = Vector{Dense_scratch}(undef, L)\n",
    "    for ℓ in 1:L\n",
    "        n_in, n_out = sizes[ℓ], sizes[ℓ+1]\n",
    "        # on each layer we take the corresponding ς and ς' functions except for the last layer \n",
    "        ς,ς′ = ℓ < L ? (activations[ℓ].first, activations[ℓ].second) : (identity, _->1.0)\n",
    "        # scale = 1/sqrt(n_in) # TODO :  sqrt(2/n_in) mieux faire l'initialization (He/Kaiming)\n",
    "        # scale =0\n",
    "        scale = 1.\n",
    "        # layers[ℓ] = Dense_scratch(scale*randn(rng,n_out,n_in), zeros(n_out), ς, ς′)\n",
    "        layers[ℓ] = Dense_scratch(fill(1 / sqrt(n_in), n_out, n_in), zeros(n_out), ς, ς′)\n",
    "\n",
    "    end\n",
    "    return MLP(layers)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "92fd5fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward(model::MLP, x)\n",
    "    a = x\n",
    "    for L in model.layers\n",
    "        a = L.ς.(L.W*a .+ L.b)\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ea927f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Loss(loss_fn, delta_fn)\n",
    "\n",
    "f  : (ŷ, y) → real scalar   (for logging / early–stop)\n",
    "d_f : (ŷ, y) → δ⁽ᴸ⁾          (seed for back-prop)\n",
    "\"\"\"\n",
    "struct Loss{F,G}\n",
    "    f :: F\n",
    "    d_f :: G\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a34c7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    backprop(model, x, y, loss′)\n",
    "\n",
    "Input\n",
    "`x` is a column vector,\n",
    "`y` is the target output (column vector),\n",
    "`loss′` is the gradient of the loss function at the output layer (column vector).\n",
    "Returns `grads` : Vector of (dW, db) in layer-order.\n",
    "\"\"\"\n",
    "function backprop(model::MLP, x, y, loss::Loss)\n",
    "    a = x\n",
    "    A = [x]\n",
    "    Z = Vector{Vector}()    # pre-activations\n",
    "    for L in model.layers\n",
    "        z = L.W*a .+ L.b\n",
    "        push!(Z,z)\n",
    "\n",
    "        a = L.ς.(z)\n",
    "        push!(A,a)\n",
    "    end\n",
    "\n",
    "    δ = loss.d_f(A[end], y)\n",
    "    Ltot = length(model.layers)\n",
    "    grads = Vector{Tuple{Matrix,Vector}}(undef, Ltot)\n",
    "\n",
    "    for ℓ in Ltot:-1:1\n",
    "        L = model.layers[ℓ]\n",
    "\n",
    "        a_prev = A[ℓ]\n",
    "        δ = δ .* L.ς′.(Z[ℓ])\n",
    "        grads[ℓ] = (δ*a_prev', δ)\n",
    "        δ = L.W' * δ # propagation\n",
    "    end\n",
    "    return grads\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4b0ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(model, X, y, opt, opt_update!, loss::Loss; tol=1e-6, epochs=200, batch=32, η=1e-3, verbose=false, print_freq=100)\n",
    "    optimiser = opt(model)\n",
    "    N = size(X,2)\n",
    "    val_loss_prev = Inf\n",
    "    for epoch in 1:epochs\n",
    "        perm = randperm(N)\n",
    "        for k in 1:batch:N\n",
    "            idx = perm[k:min(k+batch-1, N)]\n",
    "            grads_sum = nothing\n",
    "\n",
    "            for j in idx\n",
    "                grads = backprop(model, X[:,j], y[j], loss)\n",
    "\n",
    "                if grads_sum === nothing\n",
    "                    grads_sum = grads\n",
    "                else\n",
    "                    for ℓ in eachindex(grads)\n",
    "                        grads_sum[ℓ][1] .+= grads[ℓ][1]\n",
    "                        grads_sum[ℓ][2] .+= grads[ℓ][2]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            # average\n",
    "            for ℓ in eachindex(grads_sum)\n",
    "                grads_sum[ℓ][1] ./= length(idx)\n",
    "                grads_sum[ℓ][2] ./= length(idx)\n",
    "            end\n",
    "            opt_update!(model, grads_sum, optimiser, η)\n",
    "        end\n",
    "\n",
    "        y_pred = forward(model, X)\n",
    "        val_loss = loss.f(y_pred, y, model)\n",
    "\n",
    "        if verbose && (epoch == 1 || epoch % print_freq == 0)\n",
    "            @printf(\"Epoch %d/%d, Loss: %.6e\\n\", epoch, epochs, val_loss)\n",
    "        end\n",
    "\n",
    "        if abs(val_loss - val_loss_prev) < tol\n",
    "            if verbose\n",
    "                @printf(\"Converged at epoch %d, Loss: %.6e\\n\", epoch, val_loss)\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "        val_loss_prev = val_loss\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a946a62",
   "metadata": {},
   "source": [
    "## Linear model (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea254617",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "n,p = 200,20\n",
    "X = randn(n,p)\n",
    "βtrue = randn(p) .* (rand(p) .< 0.3) # sparse\n",
    "y = X*βtrue + 0.05*randn(n)\n",
    "λ = 0. # lasso with lambda=0 is mse\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11f17a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.074235e+03\n",
      "Epoch 1/300, Loss: 1.108187e+03\n"
     ]
    }
   ],
   "source": [
    "sizes = [p,1] # zero hidden layer\n",
    "\n",
    "mse_loss = Loss(\n",
    "    (y_pred, y, model) -> 0.5 * sum((y_pred[1,:] .- y).^2), # flatten y, other option: vec(y_pred)\n",
    "    (y_pred, y) -> y_pred .- y\n",
    ")\n",
    "\n",
    "net_sgd  = build_mlp(sizes, Pair{Function,Function}[]) # no activations\n",
    "net_adam  = build_mlp(sizes,  Pair{Function,Function}[]) # no activations\n",
    "train!(net_sgd, X', y, SGD_scratch, sgd_scratch_update!, mse_loss ;tol=1e-12, epochs=300, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    "train!(net_adam, X', y, Adam_scratch, adam_scratch_update!, mse_loss ;tol=1e-12, epochs=300, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d489fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "βhat_nn_sgd = net_sgd.layers[1].W[:]\n",
    "βhat_nn_adam = net_adam.layers[1].W[:]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad4d2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ISTA] iter     1  cost=1.682e+02  diff=9.792e+02\n",
      "[ISTA] END iter    51  cost=2.183e-01  diff=8.733e-13\n"
     ]
    }
   ],
   "source": [
    "f(b) = 0.5*norm(y-X*b,2)^2\n",
    "∇f(b) = -X'*(y - X*b)\n",
    "g(x) = λ*sum(abs, x) # L1 norm\n",
    "prox_L(x, step) = sign.(x) .* max.(abs.(x) .- λ*step, zero(x))\n",
    "λ = 0\n",
    "beta0 = zeros(p)# initial conditions\n",
    "βhat_ista = ista_L(beta0, f, g, ∇f, opnorm(X)^2, prox_L, tol=1e-12)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb321258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||β̂_NN_sgd - β̂_ISTA||₂ = 1.670e-02\n",
      "\n",
      "||β̂_NN_adam - β̂_ISTA||₂ = 7.557e-04\n"
     ]
    }
   ],
   "source": [
    "@printf(\"\\n||β̂_NN_sgd - β̂_ISTA||₂ = %.3e\\n\", norm(βhat_nn_sgd-βhat_ista))\n",
    "@printf(\"\\n||β̂_NN_adam - β̂_ISTA||₂ = %.3e\\n\", norm(βhat_nn_adam-βhat_ista))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d561da5",
   "metadata": {},
   "source": [
    "It seems to be working but I can't success to get a better vector (1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944432a",
   "metadata": {},
   "source": [
    "## Others tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe7b4b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>TODO :</b></span> Mon code fonctionne mais c'est sur de petits détails qu'on remarque des différences donc j'ai pas d'idées pour voir si l'implémentation est bonne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e9056",
   "metadata": {},
   "source": [
    "### 3-layers, ReLU, same data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def7e4c",
   "metadata": {},
   "source": [
    "This test is only to see if the output is exactly the same than from Flux.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbd91dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "n,p = 20,2\n",
    "X = randn(n,p)\n",
    "βtrue = randn(p) .* (rand(p) .< 0.3) # sparse\n",
    "y = X*βtrue + 0.05*randn(n)\n",
    "λ = 0. # lasso with lambda=0 is mse\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b4380bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 2.266242e-02\n",
      "Epoch 1/30, Loss: 2.452424e-02\n"
     ]
    }
   ],
   "source": [
    "sizes = [p,1,16,1] # zero hidden layer\n",
    "\n",
    "mse_loss = Loss(\n",
    "    (y_pred, y, model) -> 0.5 * sum((y_pred[1,:] .- y).^2), # flatten y, other option: vec(y_pred)\n",
    "    (y_pred, y) -> y_pred .- y\n",
    ")\n",
    "\n",
    "net_sgd  = build_mlp(sizes, Pair{Function,Function}[\n",
    "    relu_shift => drelu_shift,\n",
    "    relu_shift => drelu_shift,\n",
    "    Identity() => dIdentity()\n",
    "])\n",
    "net_adam  = build_mlp(sizes, Pair{Function,Function}[\n",
    "    relu_shift => drelu_shift,\n",
    "    relu_shift => drelu_shift,\n",
    "    Identity() => dIdentity()\n",
    "])\n",
    "\n",
    "# ? J'ai baissé le nombre d'epochs pour que ça aille plsu vite mais mettez 30_000\n",
    "train!(net_sgd, X', y, SGD_scratch, sgd_scratch_update!, mse_loss ;tol=1e-12, epochs=30, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    "train!(net_adam, X', y, Adam_scratch, adam_scratch_update!, mse_loss ;tol=1e-12, epochs=30, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63a262ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×2 Matrix{Float64}:\n",
       " 0.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net_sgd.layers[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3c6531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "βhat_nn_sgd = net_sgd.layers[1].W[:]\n",
    "y_pred_sgd = net_sgd.layers[1].W * X' .+ net_sgd.layers[1].b\n",
    "\n",
    "βhat_nn_adam = net_adam.layers[1].W[:]\n",
    "y_pred_adam = net_adam.layers[1].W * X' .+ net_adam.layers[1].b\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ab0d5",
   "metadata": {},
   "source": [
    "# From Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7417966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux, Optimisers, MLUtils, Functors, Zygote, NNlib, Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0e235",
   "metadata": {},
   "source": [
    "## Linear model (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81ae74",
   "metadata": {},
   "source": [
    "I did only **ADAM** but **SGD** works also as precdent experiment !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad5fdb",
   "metadata": {},
   "source": [
    "### 3-layers, ReLU, same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "255a77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "n,p = 20,2\n",
    "X = randn(n,p)\n",
    "βtrue = randn(p) .* (rand(p) .< 0.3) # sparse\n",
    "y = X*βtrue + 0.05*randn(n)\n",
    "λ = 0. # lasso with lambda=0 is mse\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba0b4b",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a787a767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Number of observations less than batch-size, decreasing the batch-size to 20\n",
      "└ @ MLUtils C:\\Users\\Le R\\.julia\\packages\\MLUtils\\5jDrc\\src\\batchview.jl:104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element DataLoader(::Tuple{Matrix{Float32}, Matrix{Float32}}, shuffle=true, batchsize=32)\n",
       "  with first element:\n",
       "  (2×20 Matrix{Float32}, 1×20 Matrix{Float32},)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO : faire un warm start (initialiser la compilation, sinon trop long)\n",
    "# ---------------- synthetic data ----------------\n",
    "Random.seed!(42)\n",
    "# n, p = 200, 20 # valeurs que j'ai utilisé\n",
    "n, p = 20,2\n",
    "X     = randn(Float32, n, p)          # (obs, features)\n",
    "βtrue = randn(Float32, p) .* (rand(Float32, p) .< 0.3f0)\n",
    "y     = X * βtrue .+ 0.05f0 * randn(Float32, n)  # (obs,)\n",
    "λ     = 0f0                             # LASSO penalty (mse if λ = 0)\n",
    "\n",
    "X′  = permutedims(X)                    # Lux/Flux want (features, batch)\n",
    "y′  = reshape(y, 1, :)\n",
    "\n",
    "# ---------------- model ----------------\n",
    "relu_shift(x) = NNlib.relu.(x .+ 1f-8)\n",
    "\n",
    "# TODO : we have to use_bias !\n",
    "model = Chain(\n",
    "    Dense(p => 16, relu_shift;   use_bias = false),\n",
    "    Dense(16 => 16, relu_shift;  use_bias = false),\n",
    "    Dense(16 => 1,  identity;    use_bias = false)\n",
    ")     \n",
    "\n",
    "# ---------------- parameters & optimiser ----------------\n",
    "η          = 1f0 / opnorm(X)^2          # learning-rate heuristic\n",
    "rng        = Random.default_rng()\n",
    "ps, st     = Lux.setup(rng, model)\n",
    "\n",
    "opt        = Optimisers.Adam(η)\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "function l1(ps)                         # ∑|θ| over all leaves\n",
    "    s = 0f0\n",
    "    Functors.fmap(x -> x isa AbstractArray ? (s += sum(abs, x); x) : x, ps)\n",
    "    return s\n",
    "end\n",
    "\n",
    "function loss(model, ps, st, (x, y))\n",
    "    ŷ, st2 = model(x, ps, st)\n",
    "    mse    = 0.5f0 * sum((ŷ .- y).^2) / size(y,2)\n",
    "    return mse + λ*l1(ps), st2, (; mse)\n",
    "end\n",
    "\n",
    "# ---------------- data loader ----------------\n",
    "batchsize    = 32\n",
    "loader       = MLUtils.DataLoader((X′, y′); batchsize, shuffle = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "605627c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Starting training…\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y105sZmlsZQ==.jl:8\n",
      "┌ Info: Converged at epoch 26 | loss = 0.0006658\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y105sZmlsZQ==.jl:16\n"
     ]
    }
   ],
   "source": [
    "# ---------------- training loop ----------------\n",
    "backend      = Lux.AutoZygote()\n",
    "ts           = Lux.Training.TrainState(model, ps, st, opt) \n",
    "nepochs      = 30_000\n",
    "tol          = 1e-6\n",
    "prev_loss    = Inf\n",
    "\n",
    "@info \"Starting training…\"\n",
    "for epoch in 1:nepochs\n",
    "    curr = 0f0\n",
    "    for batch in loader\n",
    "        _, curr, _, ts = Lux.Training.single_train_step!(backend, loss, batch, ts;\n",
    "                                                        return_gradients = false)\n",
    "    end\n",
    "    if abs(curr - prev_loss) < tol\n",
    "        @info \"Converged at epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "        break\n",
    "    end\n",
    "    if epoch % 100 == 0\n",
    "        @info \"Epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "    end\n",
    "    prev_loss = curr\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "516b612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weight = Float32[0.85815114 -0.31487662; -0.28055164 0.31933126; … ; -0.16486089 0.30668002; 0.80982757 0.5465371],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W1 = ps.layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33a5458c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16×2 Matrix{Float32}:\n",
       "  0.858151   -0.314877\n",
       " -0.280552    0.319331\n",
       "  0.678927    0.06178\n",
       "  0.68123    -0.389811\n",
       " -0.0627615  -0.0377295\n",
       " -0.273092   -0.52679\n",
       "  0.300346   -0.729362\n",
       " -0.587113   -0.504232\n",
       " -0.929384   -0.199617\n",
       " -0.481189   -0.510561\n",
       "  0.0895388   0.568804\n",
       " -0.366615   -0.379062\n",
       " -0.1682     -0.393172\n",
       " -0.694414   -0.824898\n",
       " -0.164861    0.30668\n",
       "  0.809828    0.546537"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W1 = ps.layer_1.weight      # first layer weight   (Float32 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e12414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20-element Vector{Float32}:\n",
       " -0.015817653\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0\n",
       "  0.0018295145\n",
       "  0.0\n",
       " -0.0039392537\n",
       "  0.027774425\n",
       "  0.0008137302\n",
       " -0.01563457\n",
       "  0.0\n",
       "  0.0\n",
       "  0.008989939\n",
       " -0.002883925\n",
       " -0.0027699831\n",
       "  0.0012075545\n",
       "  0.0\n",
       " -0.021333545\n",
       " -0.0031455518"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- fitted coefficients / predictions ----------------\n",
    "ŷ, _      = model(X′, ts.parameters, ts.states)\n",
    "y_pred_lux_adam  = ŷ[:] # analogue of `βhat_flux_adam`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e6ad15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||y_pred_adam - y_pred_lux_adam||₂ = 4.300e-02\n",
      "\n",
      "||y_pred_sgd - y_pred_lux_adam||₂ = 4.300e-02\n"
     ]
    }
   ],
   "source": [
    "# @printf(\"\\n norm(y_pred_adma -y_prd_lux_adma))\n",
    "@printf(\"\\n||y_pred_adam - y_pred_lux_adam||₂ = %.3e\\n\", norm(vec(y_pred_adam) - y_pred_lux_adam))\n",
    "@printf(\"\\n||y_pred_sgd - y_pred_lux_adam||₂ = %.3e\\n\", norm(vec(y_pred_sgd) - y_pred_lux_adam))\n",
    "\n",
    "# @printf(\"\\n||β̂_NN_adam - β̂_lux_adam||₂ = %.3e\\n\", norm(βhat_nn_adam-βhat_lux))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c15618",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d319b42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module SaveLoadTxt.\n",
      "WARNING: using SaveLoadTxt.dump_txt in module Main conflicts with an existing identifier.\n",
      "WARNING: using SaveLoadTxt.load_txt in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "include(\"../../comp_translate/SaveLoadTxt.jl\") # Assuming you have a SaveLoadTxt.jl file with the necessary functions\n",
    "using .SaveLoadTxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc179e4",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1528a40",
   "metadata": {},
   "source": [
    "### Non linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0c9c4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "n = 20\n",
    "p = 50\n",
    "s = 5\n",
    "# Random.seed!(42)\n",
    "X = randn(n, p)\n",
    "# y = cos(x_1) + x_2^2 - x_3 + exp(x_4) - x_5 + 0.1 * N(0,1)\n",
    "y = cos.(X[:,1]) .+ X[:,2].^2 .- X[:,3] .+ exp.(X[:,4]) .- X[:,5] .+ 0.1 * randn(n)\n",
    "layers = [p,3,1]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "118fe1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round-trip OK \n"
     ]
    }
   ],
   "source": [
    "dump_txt(\"../../comp_translate/data/ANN_non_linear_sparse.txt\", [X, y, layers]; append = false)\n",
    "loaded = load_txt(\"../../comp_translate/data/ANN_non_linear_sparse.txt\")\n",
    "# @assert loaded == [X,y, layers]\n",
    "println(\"Round-trip OK \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838ad574",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd1f73",
   "metadata": {},
   "source": [
    "+ SGD & Adam & ISTA\n",
    "+ Initialisation : TODO\n",
    "+ Loss : 0.5 * MSE\n",
    "+ Batchsize : all data\n",
    "+ ReLU activations functions\n",
    "\n",
    "PLus tard : \n",
    "+ Penalty : $\\ell_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d1f15",
   "metadata": {},
   "source": [
    "On considere 0.5*MSE pour utilsier ISTA sans backtracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6ae986",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f85d91",
   "metadata": {},
   "source": [
    "#### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "92e39413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50000, Loss: 6.331675e+00\n",
      "Epoch 100/50000, Loss: 3.437376e+00\n",
      "Epoch 200/50000, Loss: 3.210058e+00\n",
      "Epoch 300/50000, Loss: 3.099914e+00\n",
      "Epoch 400/50000, Loss: 3.050248e+00\n",
      "Epoch 500/50000, Loss: 3.035163e+00\n",
      "Epoch 600/50000, Loss: 3.028725e+00\n",
      "Epoch 700/50000, Loss: 3.025806e+00\n",
      "Epoch 800/50000, Loss: 3.024462e+00\n",
      "Epoch 900/50000, Loss: 3.023839e+00\n",
      "Epoch 1000/50000, Loss: 3.023550e+00\n",
      "Epoch 1100/50000, Loss: 3.023416e+00\n",
      "Epoch 1200/50000, Loss: 3.023354e+00\n",
      "Epoch 1300/50000, Loss: 3.023324e+00\n",
      "Epoch 1400/50000, Loss: 3.023311e+00\n",
      "Epoch 1500/50000, Loss: 3.023305e+00\n",
      "Epoch 1600/50000, Loss: 3.023302e+00\n",
      "Epoch 1700/50000, Loss: 3.023300e+00\n",
      "Epoch 1800/50000, Loss: 3.023300e+00\n",
      "Epoch 1900/50000, Loss: 3.023299e+00\n",
      "Epoch 2000/50000, Loss: 3.023299e+00\n",
      "Epoch 2100/50000, Loss: 3.023299e+00\n",
      "Epoch 2200/50000, Loss: 3.023299e+00\n",
      "Epoch 2300/50000, Loss: 3.023299e+00\n",
      "Epoch 2400/50000, Loss: 3.023299e+00\n",
      "Epoch 2500/50000, Loss: 3.023299e+00\n",
      "Epoch 2600/50000, Loss: 3.023299e+00\n",
      "Epoch 2700/50000, Loss: 3.023299e+00\n",
      "Epoch 2800/50000, Loss: 3.023299e+00\n",
      "Converged at epoch 2896, Loss: 3.023299e+00\n"
     ]
    }
   ],
   "source": [
    "sizes = loaded[3]\n",
    "\n",
    "mse_loss = Loss(\n",
    "    (y_pred, y, model) -> 0.5 * sum((y_pred[1,:] .- y).^2)/ size(y,1), # flatten y, other option: vec(y_pred)\n",
    "    (y_pred, y) -> (y_pred .- y)/ size(y,1)\n",
    ")\n",
    "\n",
    "# Relu activaitosn\n",
    "net_sgd = build_mlp(sizes,\n",
    "    Pair{Function,Function}[\n",
    "        # (x -> relu_shift(x; α=0.0)) => (x -> drelu_shift(x; α=0.0)),\n",
    "        relu => drelu,\n",
    "        Identity => (x->ones(size(x)))\n",
    "    ])\n",
    "train!(net_sgd, X', y, SGD_scratch, sgd_scratch_update!, mse_loss ;tol=1e-12, epochs=50_000, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=100)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "18f0b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_sc_sgd_mse = forward(net_sgd, X')[:]\n",
    "βhat_sc_sgd_mse = net_sgd.layers[1].W[:];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d56623c",
   "metadata": {},
   "source": [
    "#### Lux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "70b58c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Number of observations less than batch-size, decreasing the batch-size to 20\n",
      "└ @ MLUtils C:\\Users\\Le R\\.julia\\packages\\MLUtils\\5jDrc\\src\\batchview.jl:104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element DataLoader(::Tuple{Adjoint{Float64, Matrix{Float64}}, Matrix{Float64}}, shuffle=true, batchsize=50)\n",
       "  with first element:\n",
       "  (50×20 Matrix{Float64}, 1×20 Matrix{Float64},)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu_shift(x) = NNlib.relu.(x)\n",
    "# σ = 1/sqrt(size(X,1))            # same std as scratch\n",
    "σ = 1.\n",
    "rng = Random.default_rng()\n",
    "\n",
    "initW(rng, out, in) = fill(1 / sqrt(in), out, in)\n",
    "initb(rng, out)     = zeros(Float32, out)\n",
    "\n",
    "model = Chain(\n",
    "    Dense(p => 3, relu_shift;  use_bias = true, init_weight = initW, init_bias = initb),\n",
    "    Dense(3 => 1, identity; use_bias = true, init_weight = initW, init_bias = initb)\n",
    ")\n",
    "\n",
    "η = 1f0 / opnorm(X)^2 # learning-rate heuristic\n",
    "\n",
    "ps, st = Lux.setup(rng, model)\n",
    "\n",
    "# opt = Optimisers.Adam(η)\n",
    "opt = Optimisers.Descent(η)\n",
    "\n",
    "# function l1(ps)                         # ∑|θ| over all leaves\n",
    "#     s = 0f0\n",
    "#     Functors.fmap(x -> x isa AbstractArray ? (s += sum(abs, x); x) : x, ps)\n",
    "#     return s\n",
    "# end\n",
    "\n",
    "function loss(model, ps, st, (x, y))\n",
    "    ŷ, st2 = model(x, ps, st)\n",
    "    mse    = 0.5f0 * sum((ŷ .- y).^2) / size(y,2)\n",
    "    return mse , st2, (; mse)\n",
    "end\n",
    "\n",
    "y′  = reshape(y, 1, :)\n",
    "batchsize    = size(X,2)\n",
    "loader       = MLUtils.DataLoader((X', y′); batchsize, shuffle = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "216e2825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Starting training…\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:7\n",
      "┌ Info: Epoch 1 | loss = 6.4456\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 100 | loss = 3.441\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 200 | loss = 3.2115\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 300 | loss = 3.1011\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 400 | loss = 3.0505\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 500 | loss = 3.0353\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 600 | loss = 3.0288\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 700 | loss = 3.0258\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 800 | loss = 3.0245\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 900 | loss = 3.0238\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1000 | loss = 3.0236\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1100 | loss = 3.0234\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1200 | loss = 3.0234\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1300 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1400 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1500 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1600 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1700 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1800 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1900 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2000 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2100 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2200 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2300 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2400 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2500 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2600 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2700 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2800 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 2900 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:18\n",
      "┌ Info: Converged at epoch 2933 | loss = 3.0233\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y133sZmlsZQ==.jl:14\n"
     ]
    }
   ],
   "source": [
    "backend      = Lux.AutoZygote()\n",
    "ts           = Lux.Training.TrainState(model, ps, st, opt) \n",
    "nepochs      = 30_000\n",
    "tol          = 1e-12\n",
    "prev_loss    = Inf\n",
    "\n",
    "@info \"Starting training…\"\n",
    "for epoch in 1:nepochs\n",
    "    curr = 0f0\n",
    "    for batch in loader\n",
    "        _, curr, _, ts = Lux.Training.single_train_step!(backend, loss, batch, ts;return_gradients = false)\n",
    "    end\n",
    "    if abs(curr - prev_loss) < tol\n",
    "        @info \"Converged at epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "        break\n",
    "    end\n",
    "    if epoch % 100 == 0 || epoch==1\n",
    "        @info \"Epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "    end\n",
    "    prev_loss = curr\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a7ad5bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Vector{Float64}:\n",
       "  0.15325549524848178\n",
       "  0.15325549524848178\n",
       "  0.15325549524848178\n",
       "  0.1409315851763199\n",
       "  0.1409315851763199\n",
       "  0.1409315851763199\n",
       " -0.03737682408715094\n",
       " -0.03737682408715094\n",
       " -0.03737682408715094\n",
       "  0.12473458875783115\n",
       "  ⋮\n",
       "  0.2508999402405783\n",
       "  0.2508999402405783\n",
       "  0.2508999402405783\n",
       "  0.1019488305179242\n",
       "  0.1019488305179242\n",
       "  0.1019488305179242\n",
       "  0.15083442841144232\n",
       "  0.15083442841144232\n",
       "  0.15083442841144232"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_p_lux_sgd_mse, _ = model(X', ts.parameters, ts.states)\n",
    "βhat_lux_sgd_mse = ts.parameters.layer_1.weight[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "0cba0a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||y_p_sc_sgd_mse - y_p_lux_sgd_mse||₂ = 1.2819273875945559e-5\n",
      "||βhat_sc_sgd_mse - βhat_lux_sgd_mse||₂ = 8.239194602466951e-7\n"
     ]
    }
   ],
   "source": [
    "println(\"||y_p_sc_sgd_mse - y_p_lux_sgd_mse||₂ = \", norm(vec(y_p_sc_sgd_mse) - vec(y_p_lux_sgd_mse)))\n",
    "println(\"||βhat_sc_sgd_mse - βhat_lux_sgd_mse||₂ = \", norm(βhat_sc_sgd_mse - βhat_lux_sgd_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60ce69",
   "metadata": {},
   "source": [
    "Parfait ça fonctionne. On regarde avec Adam à présent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d8f89",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed851cc5",
   "metadata": {},
   "source": [
    "#### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "570da400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000, Loss: 6.194063e+00\n",
      "Epoch 100/3000, Loss: 2.586303e+00\n",
      "Epoch 200/3000, Loss: 2.443698e+00\n",
      "Epoch 300/3000, Loss: 2.386096e+00\n",
      "Epoch 400/3000, Loss: 2.369109e+00\n",
      "Epoch 500/3000, Loss: 2.365720e+00\n",
      "Epoch 600/3000, Loss: 2.365283e+00\n",
      "Epoch 700/3000, Loss: 2.365243e+00\n",
      "Epoch 800/3000, Loss: 2.365241e+00\n",
      "Epoch 900/3000, Loss: 2.365240e+00\n",
      "Epoch 1000/3000, Loss: 2.365240e+00\n",
      "Converged at epoch 1033, Loss: 2.365240e+00\n"
     ]
    }
   ],
   "source": [
    "mse_loss = Loss(\n",
    "    (y_pred, y, model) -> 0.5 * sum((y_pred[1,:] .- y).^2)/ size(y,1), # flatten y, other option: vec(y_pred)\n",
    "    (y_pred, y) -> (y_pred .- y)/ size(y,1)\n",
    ")\n",
    "\n",
    "# Relu activaitosn\n",
    "net_adam = build_mlp(sizes,\n",
    "    Pair{Function,Function}[\n",
    "        # (x -> relu_shift(x; α=0.0)) => (x -> drelu_shift(x; α=0.0)),\n",
    "        relu => drelu,\n",
    "        Identity => (x->ones(size(x)))\n",
    "    ])\n",
    "train!(net_adam, X', y, Adam_scratch, adam_scratch_update!, mse_loss ;tol=1e-12, epochs=3000, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=100)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bfe0e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p_sc_adam_mse = forward(net_adam, X')[:]\n",
    "βhat_sc_adam_mse = net_adam.layers[1].W[:];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14493c3b",
   "metadata": {},
   "source": [
    "#### Lux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "37f53166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Number of observations less than batch-size, decreasing the batch-size to 20\n",
      "└ @ MLUtils C:\\Users\\Le R\\.julia\\packages\\MLUtils\\5jDrc\\src\\batchview.jl:104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element DataLoader(::Tuple{Adjoint{Float64, Matrix{Float64}}, Matrix{Float64}}, shuffle=true, batchsize=50)\n",
       "  with first element:\n",
       "  (50×20 Matrix{Float64}, 1×20 Matrix{Float64},)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(p => 3, relu_shift;  use_bias = true, init_weight = initW, init_bias = initb),\n",
    "    Dense(3 => 1, identity; use_bias = true, init_weight = initW, init_bias = initb)\n",
    ")\n",
    "ps, st = Lux.setup(rng, model)\n",
    "\n",
    "opt = Optimisers.Adam(η)\n",
    "\n",
    "function loss(model, ps, st, (x, y))\n",
    "    ŷ, st2 = model(x, ps, st)\n",
    "    mse    = 0.5f0 * sum((ŷ .- y).^2) / size(y,2)\n",
    "    return mse , st2, (; mse)\n",
    "end\n",
    "\n",
    "y′  = reshape(y, 1, :)\n",
    "loader       = MLUtils.DataLoader((X', y′); batchsize, shuffle = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1ac8f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Starting training…\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:7\n",
      "┌ Info: Epoch 1 | loss = 6.4456\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 100 | loss = 2.5883\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 200 | loss = 2.4446\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 300 | loss = 2.3864\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 400 | loss = 2.3692\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 500 | loss = 2.3657\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 600 | loss = 2.3653\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 700 | loss = 2.3652\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 800 | loss = 2.3652\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 900 | loss = 2.3652\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Epoch 1000 | loss = 2.3652\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:18\n",
      "┌ Info: Converged at epoch 1039 | loss = 2.3652\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y150sZmlsZQ==.jl:14\n"
     ]
    }
   ],
   "source": [
    "backend      = Lux.AutoZygote()\n",
    "ts           = Lux.Training.TrainState(model, ps, st, opt) \n",
    "nepochs      = 30_000\n",
    "tol          = 1e-12\n",
    "prev_loss    = Inf\n",
    "\n",
    "@info \"Starting training…\"\n",
    "for epoch in 1:nepochs\n",
    "    curr = 0f0\n",
    "    for batch in loader\n",
    "        _, curr, _, ts = Lux.Training.single_train_step!(backend, loss, batch, ts;return_gradients = false)\n",
    "    end\n",
    "    if abs(curr - prev_loss) < tol\n",
    "        @info \"Converged at epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "        break\n",
    "    end\n",
    "    if epoch % 100 == 0 || epoch==1\n",
    "        @info \"Epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "    end\n",
    "    prev_loss = curr\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "dfc5ca7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150-element Vector{Float64}:\n",
       "  0.23446048784984852\n",
       "  0.23446048784984852\n",
       "  0.23446048784984852\n",
       "  0.3030017295803509\n",
       "  0.3030017295803509\n",
       "  0.3030017295803509\n",
       " -0.02757013009242761\n",
       " -0.02757013009242761\n",
       " -0.02757013009242761\n",
       "  0.06496048646716612\n",
       "  ⋮\n",
       "  0.2872589202789973\n",
       "  0.2872589202789973\n",
       "  0.2872589202789973\n",
       "  0.054689801470101655\n",
       "  0.054689801470101655\n",
       "  0.054689801470101655\n",
       "  0.2531694380556684\n",
       "  0.2531694380556684\n",
       "  0.2531694380556684"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_p_lux_adam_mse, _ = model(X', ts.parameters, ts.states)\n",
    "βhat_lux_adam_mse = ts.parameters.layer_1.weight[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e05b005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||y_p_sc_adam_mse - y_p_lux_adam_mse||₂ = 4.454340912653943e-6\n",
      "||βhat_sc_adam_mse - βhat_lux_adam_mse||₂ = 4.366207290469105e-7\n"
     ]
    }
   ],
   "source": [
    "# println(norm(vec(y_p_sc_sgd_mse) - vec(y_p_lux_adam_mse))\n",
    "println(\"||y_p_sc_adam_mse - y_p_lux_adam_mse||₂ = \", norm(vec(y_p_sc_adam_mse) - vec(y_p_lux_adam_mse)))\n",
    "println(\"||βhat_sc_adam_mse - βhat_lux_adam_mse||₂ = \", norm(βhat_sc_adam_mse - βhat_lux_adam_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e16ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

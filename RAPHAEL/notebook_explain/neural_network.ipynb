{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67d50c1f",
   "metadata": {},
   "source": [
    "$\\textbf{GOAL}$ Create from scratch a fully connected neural network (ANN) and get exact result than library already integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fbf1a",
   "metadata": {},
   "source": [
    "**This notebook is nota at all the main subject of the intership, more and better explaination exists other than here. Its more a compilation of ideas on neural network and code test than a explicit course.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e232cc",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>TODO :</b></span> Je pense un probleme sur la matrice de poids du from scratch. \n",
    "\n",
    "On doit tout reprendre proprement avec des tests solides.\n",
    "\n",
    "Pas assez convergent sur le cas linéaire. \n",
    "Peut-être rester sur du Lux et après voir pourquoi le from scratch ne fonctionne pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24c5b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Random, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6ec3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../functions/ISTA.jl\")\n",
    "\n",
    "import .ISTA: ista_L, ista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "007cd7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Dense_scratch\n",
    "    W::Matrix{Float64}\n",
    "    b::Vector{Float64}\n",
    "    ς::Function\n",
    "    ς′::Function\n",
    "end\n",
    "\n",
    "mutable struct MLP\n",
    "    layers::Vector{Dense_scratch}\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47bfe4e",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd85ede",
   "metadata": {},
   "source": [
    "## Basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5c783",
   "metadata": {},
   "source": [
    "We need ANN because it allows to modelize non linear function from ther activation function which are non linear.\n",
    "\n",
    "**Definition.**\n",
    "For an input $x\\in\\mathbb R^{p_1}$ and an integer $l\\ge 1$ (total layers), a *fully-connected ANN* is the composite map\n",
    "\n",
    "$$\n",
    "\\mu_\\theta(x) =S_l\\circ S_{l-1}\\circ\\cdots\\circ S_{1}(x)\n",
    "$$\n",
    "\n",
    "with parameters\n",
    "$$\n",
    "\\theta = \\bigl(W_1, (W_2,\\dots,W_l,b_1, \\dots, b_l)\\bigr) =(\\theta^{(1)},\\theta^{(2)})$$\n",
    "\n",
    "where, for $k=\\llbracket 1, l-1 \\rrbracket$\n",
    "$$\n",
    "S_k(u)=\\varsigma\\!\\bigl(b_k+W_k u\\bigr),\n",
    "\\qquad\n",
    "S_l(u)=b_l+W_l u,\n",
    "$$\n",
    "\n",
    "$W_k\\in\\mathbb R^{p_{k+1}\\times p_k}$, $b_k\\in\\mathbb R^{p_{k+1}}$, and $p_{l+1}=1$ for scalar regression ($b_l = c$ the intercept)\n",
    "\n",
    "From ``lambda_qut.ipynb`` we need assumptions.  \n",
    "<span style=\"color:red\"><b>TODO :</b></span>  Les hypotheses sont plausibles mais pas sûr de ce que j'avance.  \n",
    "To keep the zero-function reachable and allow QUT-compatible sparsity we impose \n",
    "$\\varsigma\\in C^2$, unbounded, $L$-Lipschitz, $\\varsigma(0)=0$ and $\\varsigma'(0)>0$ like shifted-ReLU ($ReLU_\\alpha$), $ELU_\\alpha$, Softplus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfa6fc",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03915fa",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e72ba",
   "metadata": {},
   "source": [
    "Given a loss function we need to minimize the error made by the learner $\\mu$, so we will optimize it by **BackPropagation**\n",
    "\n",
    "On theLASSO problem (from chatGPT), for any weight $W_k$,\n",
    "$$\n",
    "\\partial_{W_k} \\tfrac12\\|y-\\mu_\\theta(X)\\|_2^2\n",
    "=\n",
    "\\Bigl(J_{S_{k+1:l}}(z_k)^\\top\\bigl(\\mu_\\theta(X)-y\\bigr)\\Bigr)z_{k-1}^\\top,\n",
    "$$\n",
    "\n",
    "where $z_{k-1}=S_{k-1}\\circ\\cdots\\circ S_1(X)$ and $J_{S_{k+1:l}}$ is the Jacobian of the suffix $S_l\\circ\\cdots\\circ S_{k+1}$. This is the usual chain-rule implemented by back-prop.\n",
    "\n",
    "\n",
    "\n",
    "I've written this few years ago, it can be helpful.  \n",
    "Let $n\\in \\mathbb{N}^*$ be the sample size of data, $X_i \\in \\mathbb{R}^{N_0}$ be one input data and $y_i, \\hat{y}_i \\in \\mathbb{R}^{N_L}$ be, respectively, the true output associated with $X_i$ and the prediction inferred by the model (in this case, the perceptron) to the $i$-th data element. \n",
    "\n",
    "\n",
    "\n",
    "We have via the chain rule,\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial \\hat{y}_i} \\cdot \\frac{\\partial \\hat{y}_i}{\\partial Z_i} \\cdot \\frac{\\partial Z_i}{\\partial w_{ij}}$$  \n",
    "\n",
    "\n",
    "We can rewrite it to get a general code later, for one parameter block the full chain rule by fixing a layer index $1\\le \\ell\\le L$.\n",
    "The loss is $\\mathcal L\\bigl(a^{(L)},y\\bigr)$ with $y$ the target.\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial W^{(\\ell)}} \n",
    "= \n",
    "\\frac{\\partial\\mathcal{L}}{\\partial a^{(L)}}\n",
    "\\cdot \\frac{\\partial a^{(L)}}{\\partial z^{(L)}}\n",
    "\\cdot \\frac{\\partial z^{(L)}}{\\partial a^{(L-1)}}\n",
    "\\cdots\n",
    "\\frac{\\partial a^{(\\ell)}}{\\partial z^{(\\ell)}}\n",
    "\\cdot \\frac{\\partial z^{(\\ell)}}{\\partial W^{(\\ell)}}\n",
    "$$\n",
    "\n",
    "Define the error signal,\n",
    "$$\n",
    "\\delta^{(\\ell)} := \\frac{\\partial \\mathcal L}{\\partial z^{(\\ell)}} \\in \\mathbb{R}^{n_\\ell}.\n",
    "$$\n",
    "\n",
    "By factorising the long product above, we obtain the recursive form,\n",
    "$$\n",
    "\\delta^{(L)} = \\frac{\\partial \\mathcal{L}}{\\partial a^{(L)}} \n",
    "\\odot \\sigma^{(L)\\prime}\\big(z^{(L)}\\big)\n",
    "$$\n",
    "\n",
    "$$\\delta^{(\\ell)} = \\bigl(W^{(\\ell+1)}\\bigr)^\\top \\delta^{(\\ell+1)}\n",
    "               \\odot \\sigma^{(\\ell)\\prime}\\bigl(z^{(\\ell)}\\bigr),\n",
    "               \\qquad \\ell = L-1,\\dots,1$$\n",
    "\n",
    "So, we got for all parameters,\n",
    "$$\n",
    "\n",
    "\\frac{\\partial\\mathcal L}{\\partial W^{(\\ell)}}= \\delta^{(\\ell)} \\, \\big(a^{(\\ell-1)}\\big)^\\top$$\n",
    "$$\\frac{\\partial\\mathcal L}{\\partial b^{(\\ell)}}\n",
    "  = \\delta^{(\\ell)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375e622",
   "metadata": {},
   "source": [
    "### Non convexity of ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8551df",
   "metadata": {},
   "source": [
    "The optimisation is not convexe in despite we have convex activation function.\n",
    "In some case we can expect convex problem but it's too restrictive\n",
    "> https://stats.stackexchange.com/questions/499120/are-there-any-convex-neural-networks  \n",
    "\n",
    "> Input Convex Neural Networks  \n",
    "Brandon Amos 1 Lei Xu 2 * J. Zico Kolter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb5d8d7",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95afc7",
   "metadata": {},
   "source": [
    "### Basic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bfa6f",
   "metadata": {},
   "source": [
    "$$\n",
    "W_{t+1} = W_t - \\eta \\nabla W_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c32975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sgd_scratch_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To keep the same syntax as below but i agree is too much for nothing\n",
    "\n",
    "mutable struct SGD_scratch\n",
    "end\n",
    "\n",
    "function SGD_scratch(model::MLP)\n",
    "    SGD_scratch()\n",
    "end\n",
    "\n",
    "function sgd_scratch_update!(model::MLP, grads, opt::SGD_scratch, η)\n",
    "    W, b = model.layers[1].W, model.layers[1].b\n",
    "    dW, db = grads[1]\n",
    "    W .-= η .* dW\n",
    "    b .-= η .* db\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976dbd5",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3fbfc",
   "metadata": {},
   "source": [
    "So we will use the optimizer Adam because it works very well, $W_t$ is the gradient of weights and biases at the $t$-th epochs  \n",
    "$\\eta$ is the learing rate\n",
    "\n",
    "$$m_0 = 0, \\quad v_0 = 0, \\quad 1 > \\beta_1, \\beta_2 \\geq 0, \\quad \\epsilon >0$$\n",
    "$$\n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\partial W_t\n",
    "$$\n",
    "$$\n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\partial W_t)^2\n",
    "$$\n",
    "$$\n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "   W_{t+1} = W_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t + \\epsilon}} \\hat{m}_t\n",
    "$$\n",
    "\n",
    "> https://arxiv.org/pdf/1412.6980   \n",
    "\n",
    "> https://arxiv.org/pdf/1904.09237   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d443fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "adam_scratch_update! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mutable struct Adam_scratch\n",
    "    m::Vector{Matrix}; v::Vector{Matrix}\n",
    "    mb::Vector{Vector}; vb::Vector{Vector}\n",
    "    β1::Float64; β2::Float64; ε::Float64; t::Int\n",
    "end\n",
    "\n",
    "function Adam_scratch(model::MLP; β1=.9, β2=.999, ε=1e-8)\n",
    "    L = length(model.layers)\n",
    "    Adam_scratch([zeros(size(L.W)) for L in model.layers],\n",
    "         [zeros(size(L.W)) for L in model.layers],\n",
    "         [zeros(size(L.b)) for L in model.layers],\n",
    "         [zeros(size(L.b)) for L in model.layers],\n",
    "         β1, β2, ε, 0)\n",
    "end\n",
    "\n",
    "function adam_scratch_update!(model::MLP, grads, opt::Adam_scratch, η)\n",
    "    opt.t += 1\n",
    "    for (ℓ,(dW,db)) in enumerate(grads)\n",
    "        L = model.layers[ℓ]\n",
    "\n",
    "        # weights\n",
    "        opt.m[ℓ] .= opt.β1 .* opt.m[ℓ] .+ (1-opt.β1).*dW\n",
    "        opt.v[ℓ] .= opt.β2 .* opt.v[ℓ] .+ (1-opt.β2).* (dW.^2)\n",
    "        m̂ = opt.m[ℓ] ./ (1 - opt.β1^opt.t)\n",
    "        v̂ = opt.v[ℓ] ./ (1 - opt.β2^opt.t)\n",
    "        L.W .-= η .* m̂ ./ (sqrt.(v̂) .+ opt.ε)\n",
    "\n",
    "        # biases\n",
    "        opt.mb[ℓ] .= opt.β1 .* opt.mb[ℓ] .+ (1-opt.β1).*db\n",
    "        opt.vb[ℓ] .= opt.β2 .* opt.vb[ℓ] .+ (1-opt.β2).* (db.^2)\n",
    "        m̂b = opt.mb[ℓ] ./ (1 - opt.β1^opt.t)\n",
    "        v̂b = opt.vb[ℓ] ./ (1 - opt.β2^opt.t)\n",
    "        L.b .-= η .* m̂b ./ (sqrt.(v̂b) .+ opt.ε)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09450268",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d593284",
   "metadata": {},
   "source": [
    "There are good ways to initialize weight matrices and bias vectors depending on the activations functions.  \n",
    "<span style=\"color:red\"><b>TODO :</b></span> comment on initialise dans notre cas ? car c'est pas ReLU nous masi $ReLU_\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0dc2fd",
   "metadata": {},
   "source": [
    "# From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d047ab",
   "metadata": {},
   "source": [
    "I define MLP at the start of the notebook because it allows to get Adam in the good section and execute all cells from the button \"run all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c16a73e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dIdentity (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu_shift(x, α=1.0) = max(0.0,  x - α)\n",
    "drelu_shift(x, α=1.0) = x > α ? 1.0 : 0.0 \n",
    "\n",
    "softplus(x)  = log1p(exp(x))\n",
    "dsoftplus(x) = 1/(1+exp(-x))\n",
    "\n",
    "elu(x, α=1.0)  = x ≥ 0 ? x : α*(exp(x)-1)\n",
    "delu(x, α=1.0) = x ≥ 0 ? 1.0 : α*exp(x)\n",
    "\n",
    "Identity(α=1.0) = (x) -> x\n",
    "dIdentity(α=1.0) = (x) -> 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae09c9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_mlp"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    build_mlp(sizes::Vector{Int}, activations::Vector{Pair})\n",
    "\n",
    "`activations[k] = σ=>σ′` for hidden layer k (1-based).  \n",
    "The output layer is linear (regression linear).\n",
    "\"\"\"\n",
    "function build_mlp(sizes::Vector{Int}, activations::Vector{Pair})\n",
    "    L = length(sizes)-1\n",
    "    layers = Vector{Dense_scratch}(undef, L)\n",
    "    for ℓ in 1:L\n",
    "        n_in, n_out = sizes[ℓ], sizes[ℓ+1]\n",
    "        # on each layer we take the corresponding ς and ς' functions except for the last layer \n",
    "        ς,ς′ = ℓ < L ? (activations[ℓ].first, activations[ℓ].second) : (identity, _->1.0)\n",
    "        scale = 1/sqrt(n_in) # TODO :  sqrt(2/n_in) mieux faire l'initialization (He/Kaiming)\n",
    "        # scale =0\n",
    "        layers[ℓ] = Dense_scratch(scale*randn(n_out,n_in), zeros(n_out), ς, ς′)\n",
    "    end\n",
    "    return MLP(layers)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fd5fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function forward(model::MLP, x)\n",
    "    a = x\n",
    "    for L in model.layers\n",
    "        a = L.ς.(L.W*a .+ L.b)\n",
    "    end\n",
    "    return a\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89ea927f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loss"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Loss(loss_fn, delta_fn)\n",
    "\n",
    "f  : (ŷ, y) → real scalar   (for logging / early–stop)\n",
    "d_f : (ŷ, y) → δ⁽ᴸ⁾          (seed for back-prop)\n",
    "\"\"\"\n",
    "struct Loss{F,G}\n",
    "    f :: F\n",
    "    d_f :: G\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a34c7cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    backprop(model, x, y, loss′)\n",
    "\n",
    "Input\n",
    "`x` is a column vector,\n",
    "`y` is the target output (column vector),\n",
    "`loss′` is the gradient of the loss function at the output layer (column vector).\n",
    "Returns `grads` : Vector of (dW, db) in layer-order.\n",
    "\"\"\"\n",
    "function backprop(model::MLP, x, y, loss::Loss)\n",
    "    a = x\n",
    "    A = [x]\n",
    "    Z = Vector{Vector}()    # pre-activations\n",
    "    for L in model.layers\n",
    "        z = L.W*a .+ L.b\n",
    "        push!(Z,z)\n",
    "\n",
    "        a = L.ς.(z)\n",
    "        push!(A,a)\n",
    "    end\n",
    "\n",
    "    δ = loss.d_f(A[end], y)\n",
    "    Ltot = length(model.layers)\n",
    "    grads = Vector{Tuple{Matrix,Vector}}(undef, Ltot)\n",
    "\n",
    "    for ℓ in Ltot:-1:1\n",
    "        L = model.layers[ℓ]\n",
    "\n",
    "        a_prev = A[ℓ]\n",
    "        δ = δ .* L.ς′.(Z[ℓ])\n",
    "        grads[ℓ] = (δ*a_prev', δ)\n",
    "        δ = L.W' * δ # propagation\n",
    "    end\n",
    "    return grads\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4b0ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function train!(model, X, y, opt, opt_update!, loss::Loss; tol=1e-6, epochs=200, batch=32, η=1e-3, verbose=false, print_freq=100)\n",
    "    optimiser = opt(model)\n",
    "    N = size(X,2)\n",
    "    val_loss_prev = Inf\n",
    "    for epoch in 1:epochs\n",
    "        perm = randperm(N)\n",
    "        for k in 1:batch:N\n",
    "            idx = perm[k:min(k+batch-1, N)]\n",
    "            grads_sum = nothing\n",
    "\n",
    "            for j in idx\n",
    "                grads = backprop(model, X[:,j], y[j], loss)\n",
    "\n",
    "                if grads_sum === nothing\n",
    "                    grads_sum = grads\n",
    "                else\n",
    "                    for ℓ in eachindex(grads)\n",
    "                        grads_sum[ℓ][1] .+= grads[ℓ][1]\n",
    "                        grads_sum[ℓ][2] .+= grads[ℓ][2]\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            # average\n",
    "            for ℓ in eachindex(grads_sum)\n",
    "                grads_sum[ℓ][1] ./= length(idx)\n",
    "                grads_sum[ℓ][2] ./= length(idx)\n",
    "            end\n",
    "            opt_update!(model, grads_sum, optimiser, η)\n",
    "        end\n",
    "\n",
    "        y_pred = forward(model, X)\n",
    "        val_loss = loss.f(y_pred, y, model)\n",
    "\n",
    "        if verbose && (epoch == 1 || epoch % print_freq == 0)\n",
    "            @printf(\"Epoch %d/%d, Loss: %.6e\\n\", epoch, epochs, val_loss)\n",
    "        end\n",
    "\n",
    "        if abs(val_loss - val_loss_prev) < tol\n",
    "            if verbose\n",
    "                @printf(\"Converged at epoch %d, Loss: %.6e\\n\", epoch, val_loss)\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "        val_loss_prev = val_loss\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a946a62",
   "metadata": {},
   "source": [
    "## Linear model (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea254617",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "n,p = 200,20\n",
    "X = randn(n,p)\n",
    "βtrue = randn(p) .* (rand(p) .< 0.3) # sparse\n",
    "y = X*βtrue + 0.05*randn(n)\n",
    "λ = 0. # lasso with lambda=0 is mse\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11f17a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 1.086216e+03\n",
      "Epoch 1/300, Loss: 1.209643e+03\n"
     ]
    }
   ],
   "source": [
    "sizes = [p,1] # zero hidden layer\n",
    "\n",
    "mse_loss = Loss(\n",
    "    (y_pred, y, model) -> 0.5 * sum((y_pred[1,:] .- y).^2), # flatten y, other option: vec(y_pred)\n",
    "    (y_pred, y) -> y_pred .- y\n",
    ")\n",
    "\n",
    "net_sgd  = build_mlp(sizes, Pair[]) # no activations\n",
    "net_adam  = build_mlp(sizes, Pair[]) # no activations\n",
    "train!(net_sgd, X', y, SGD_scratch, sgd_scratch_update!, mse_loss ;tol=1e-12, epochs=300, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    "train!(net_adam, X', y, Adam_scratch, adam_scratch_update!, mse_loss ;tol=1e-12, epochs=300, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "βhat_nn_sgd = net_sgd.layers[1].W[:]\n",
    "βhat_nn_adam = net_adam.layers[1].W[:]\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad4d2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ISTA] iter     1  cost=1.682e+02  diff=9.792e+02\n",
      "[ISTA] END iter    51  cost=2.183e-01  diff=8.733e-13\n"
     ]
    }
   ],
   "source": [
    "f(b) = 0.5*norm(y-X*b,2)^2\n",
    "∇f(b) = -X'*(y - X*b)\n",
    "g(x) = λ*sum(abs, x) # L1 norm\n",
    "prox_L(x, step) = sign.(x) .* max.(abs.(x) .- λ*step, zero(x))\n",
    "λ = 0\n",
    "beta0 = zeros(p)# initial conditions\n",
    "βhat_ista = ista_L(beta0, f, g, ∇f, opnorm(X)^2, prox_L, tol=1e-12)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb321258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||β̂_NN_sgd - β̂_ISTA||₂ = 1.663e-04\n",
      "\n",
      "||β̂_NN_adam - β̂_ISTA||₂ = 7.281e-03\n"
     ]
    }
   ],
   "source": [
    "@printf(\"\\n||β̂_NN_sgd - β̂_ISTA||₂ = %.3e\\n\", norm(βhat_nn_sgd-βhat_ista))\n",
    "@printf(\"\\n||β̂_NN_adam - β̂_ISTA||₂ = %.3e\\n\", norm(βhat_nn_adam-βhat_ista))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d561da5",
   "metadata": {},
   "source": [
    "It seems to be working but I can't success to get a better vector (1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944432a",
   "metadata": {},
   "source": [
    "## Others tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe7b4b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><b>TODO :</b></span> Mon code fonctionne mais c'est sur de petits détails qu'on remarque des différences donc j'ai pas d'idées pour voir si l'implémentation est bonne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e9056",
   "metadata": {},
   "source": [
    "### 3-layers, ReLU, same data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def7e4c",
   "metadata": {},
   "source": [
    "This test is only to see if the output is exactly the same than from Flux.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbd91dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "n,p = 200,20\n",
    "X = randn(n,p)\n",
    "βtrue = randn(p) .* (rand(p) .< 0.3) # sparse\n",
    "y = X*βtrue + 0.05*randn(n)\n",
    "λ = 0. # lasso with lambda=0 is mse\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4380bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30000, Loss: 1.145963e+03\n",
      "Epoch 1000/30000, Loss: 1.153971e+03\n",
      "Epoch 2000/30000, Loss: 1.155545e+03\n",
      "Epoch 3000/30000, Loss: 1.158048e+03\n",
      "Epoch 4000/30000, Loss: 1.160961e+03\n",
      "Epoch 5000/30000, Loss: 1.161404e+03\n",
      "Epoch 6000/30000, Loss: 1.161965e+03\n",
      "Epoch 7000/30000, Loss: 1.167089e+03\n",
      "Epoch 8000/30000, Loss: 1.171441e+03\n",
      "Epoch 9000/30000, Loss: 1.174654e+03\n",
      "Epoch 10000/30000, Loss: 1.175607e+03\n",
      "Epoch 11000/30000, Loss: 1.179241e+03\n",
      "Epoch 12000/30000, Loss: 1.177920e+03\n",
      "Epoch 13000/30000, Loss: 1.173187e+03\n",
      "Epoch 14000/30000, Loss: 1.170983e+03\n",
      "Epoch 15000/30000, Loss: 1.170333e+03\n",
      "Epoch 16000/30000, Loss: 1.170282e+03\n",
      "Epoch 17000/30000, Loss: 1.170295e+03\n",
      "Epoch 18000/30000, Loss: 1.170308e+03\n",
      "Epoch 19000/30000, Loss: 1.170305e+03\n",
      "Epoch 20000/30000, Loss: 1.170308e+03\n",
      "Epoch 21000/30000, Loss: 1.170311e+03\n",
      "Epoch 22000/30000, Loss: 1.170311e+03\n",
      "Epoch 23000/30000, Loss: 1.170308e+03\n",
      "Epoch 24000/30000, Loss: 1.170316e+03\n",
      "Epoch 25000/30000, Loss: 1.170314e+03\n",
      "Epoch 26000/30000, Loss: 1.170310e+03\n",
      "Epoch 27000/30000, Loss: 1.170312e+03\n",
      "Epoch 28000/30000, Loss: 1.170317e+03\n",
      "Epoch 29000/30000, Loss: 1.170311e+03\n",
      "Epoch 30000/30000, Loss: 1.170309e+03\n",
      "Epoch 1/30000, Loss: 1.099627e+03\n",
      "Epoch 1000/30000, Loss: 2.950673e-01\n",
      "Epoch 2000/30000, Loss: 4.940543e-01\n",
      "Epoch 3000/30000, Loss: 2.906254e-01\n",
      "Epoch 4000/30000, Loss: 3.070520e-01\n",
      "Epoch 5000/30000, Loss: 2.599139e-01\n",
      "Epoch 6000/30000, Loss: 4.854301e-01\n",
      "Epoch 7000/30000, Loss: 3.102016e-01\n",
      "Epoch 8000/30000, Loss: 2.649045e-01\n",
      "Epoch 9000/30000, Loss: 2.783041e-01\n",
      "Epoch 10000/30000, Loss: 2.894146e-01\n",
      "Epoch 11000/30000, Loss: 3.824587e-01\n",
      "Epoch 12000/30000, Loss: 2.730007e-01\n",
      "Epoch 13000/30000, Loss: 2.466967e-01\n",
      "Epoch 14000/30000, Loss: 2.269396e-01\n",
      "Epoch 15000/30000, Loss: 2.482152e-01\n",
      "Epoch 16000/30000, Loss: 2.620778e-01\n",
      "Epoch 17000/30000, Loss: 3.509031e-01\n",
      "Epoch 18000/30000, Loss: 2.293726e-01\n",
      "Epoch 19000/30000, Loss: 4.547682e-01\n",
      "Epoch 20000/30000, Loss: 2.656234e-01\n",
      "Epoch 21000/30000, Loss: 2.498464e-01\n",
      "Epoch 22000/30000, Loss: 2.821031e-01\n",
      "Epoch 23000/30000, Loss: 2.853460e-01\n",
      "Epoch 24000/30000, Loss: 2.429559e-01\n",
      "Epoch 25000/30000, Loss: 4.315833e-01\n",
      "Epoch 26000/30000, Loss: 2.751109e-01\n",
      "Epoch 27000/30000, Loss: 2.772852e-01\n",
      "Epoch 28000/30000, Loss: 2.311671e-01\n",
      "Epoch 29000/30000, Loss: 2.293517e-01\n",
      "Epoch 30000/30000, Loss: 3.044041e-01\n"
     ]
    }
   ],
   "source": [
    "sizes = [p,1,16,1] # zero hidden layer\n",
    "\n",
    "mse_loss = Loss(\n",
    "    (y_pred, y, model) -> 0.5 * sum((y_pred[1,:] .- y).^2), # flatten y, other option: vec(y_pred)\n",
    "    (y_pred, y) -> y_pred .- y\n",
    ")\n",
    "\n",
    "net_sgd  = build_mlp(sizes, Vector{Pair}([\n",
    "    relu_shift => drelu_shift,\n",
    "    relu_shift => drelu_shift,\n",
    "    Identity() => dIdentity()\n",
    "]))\n",
    "net_adam  = build_mlp(sizes, Vector{Pair}([\n",
    "    relu_shift => drelu_shift,\n",
    "    relu_shift => drelu_shift,\n",
    "    Identity() => dIdentity()\n",
    "]))\n",
    "\n",
    "train!(net_sgd, X', y, SGD_scratch, sgd_scratch_update!, mse_loss ;tol=1e-12, epochs=30_000, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    "train!(net_adam, X', y, Adam_scratch, adam_scratch_update!, mse_loss ;tol=1e-12, epochs=30_000, η=1/opnorm(X)^2, batch=size(X,2), verbose=true , print_freq=1000)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63a262ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×20 Matrix{Float64}:\n",
       " -0.325669  -0.0157537  0.00158554  …  -0.0176003  0.0800011  -0.0156674"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net_sgd.layers[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3c6531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "βhat_nn_sgd = net_sgd.layers[1].W[:]\n",
    "y_pred_sgd = net_sgd.layers[1].W * X' .+ net_sgd.layers[1].b\n",
    "\n",
    "βhat_nn_adam = net_adam.layers[1].W[:]\n",
    "y_pred_adam = net_adam.layers[1].W * X' .+ net_adam.layers[1].b\n",
    "\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ab0d5",
   "metadata": {},
   "source": [
    "# From Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7417966b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using NNlib.softplus in module Main conflicts with an existing identifier.\n",
      "WARNING: using NNlib.elu in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using Lux, Optimisers, MLUtils, Functors, Zygote, NNlib, Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0e235",
   "metadata": {},
   "source": [
    "## Linear model (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81ae74",
   "metadata": {},
   "source": [
    "I did only **ADAM** but **SGD** works also as precdent experiment !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad5fdb",
   "metadata": {},
   "source": [
    "### 3-layers, ReLU, same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "255a77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(42)\n",
    "n,p = 200,20\n",
    "X = randn(n,p)\n",
    "βtrue = randn(p) .* (rand(p) .< 0.3) # sparse\n",
    "y = X*βtrue + 0.05*randn(n)\n",
    "λ = 0. # lasso with lambda=0 is mse\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba0b4b",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97456a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7-element DataLoader(::Tuple{Matrix{Float32}, Matrix{Float32}}, shuffle=true, batchsize=32)\n",
       "  with first element:\n",
       "  (20×32 Matrix{Float32}, 1×32 Matrix{Float32},)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ---------------- synthetic data ----------------\n",
    "Random.seed!(42)\n",
    "n, p = 200, 20\n",
    "X     = randn(Float32, n, p)          # (obs, features)\n",
    "βtrue = randn(Float32, p) .* (rand(Float32, p) .< 0.3f0)\n",
    "y     = X * βtrue .+ 0.05f0 * randn(Float32, n)  # (obs,)\n",
    "λ     = 0f0                             # LASSO penalty (mse if λ = 0)\n",
    "\n",
    "X′  = permutedims(X)                    # Lux/Flux want (features, batch)\n",
    "y′  = reshape(y, 1, :)\n",
    "\n",
    "# ---------------- model ----------------\n",
    "relu_shift(x) = NNlib.relu.(x .+ 1f-8)\n",
    "\n",
    "# TODO : we have to use_bias !\n",
    "model = Chain(\n",
    "    Dense(p => 16, relu_shift;   use_bias = false),\n",
    "    Dense(16 => 16, relu_shift;  use_bias = false),\n",
    "    Dense(16 => 1,  identity;    use_bias = false)\n",
    ")     \n",
    "\n",
    "# ---------------- parameters & optimiser ----------------\n",
    "η          = 1f0 / opnorm(X)^2          # learning-rate heuristic\n",
    "rng        = Random.default_rng()\n",
    "ps, st     = Lux.setup(rng, model)\n",
    "\n",
    "opt        = Optimisers.Adam(η)\n",
    "\n",
    "# ---------------- helpers ----------------\n",
    "function l1(ps)                         # ∑|θ| over all leaves\n",
    "    s = 0f0\n",
    "    Functors.fmap(x -> x isa AbstractArray ? (s += sum(abs, x); x) : x, ps)\n",
    "    return s\n",
    "end\n",
    "\n",
    "function loss(model, ps, st, (x, y))\n",
    "    ŷ, st2 = model(x, ps, st)\n",
    "    mse    = 0.5f0 * sum((ŷ .- y).^2) / size(y,2)\n",
    "    return mse + λ*l1(ps), st2, (; mse)\n",
    "end\n",
    "\n",
    "# ---------------- data loader ----------------\n",
    "batchsize    = 32\n",
    "loader       = MLUtils.DataLoader((X′, y′); batchsize, shuffle = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21f35df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Starting training…\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y130sZmlsZQ==.jl:8\n",
      "┌ Info: Epoch 100 | loss = 0.0015609\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y130sZmlsZQ==.jl:20\n",
      "┌ Info: Epoch 200 | loss = 4.991e-5\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y130sZmlsZQ==.jl:20\n",
      "┌ Info: Converged at epoch 244 | loss = 1.5201e-5\n",
      "└ @ Main c:\\Users\\Le R\\Desktop\\Code\\Projets\\Geneve\\STAGE-GENEVE\\RAPHAEL\\notebook_explain\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_Y130sZmlsZQ==.jl:16\n"
     ]
    }
   ],
   "source": [
    "# ---------------- training loop ----------------\n",
    "backend      = Lux.AutoZygote()\n",
    "ts           = Lux.Training.TrainState(model, ps, st, opt) \n",
    "nepochs      = 30_000\n",
    "tol          = 1e-6\n",
    "prev_loss    = Inf\n",
    "\n",
    "@info \"Starting training…\"\n",
    "for epoch in 1:nepochs\n",
    "    curr = 0f0\n",
    "    for batch in loader\n",
    "        _, curr, _, ts = Lux.Training.single_train_step!(backend, loss, batch, ts;\n",
    "                                                        return_gradients = false)\n",
    "    end\n",
    "    if abs(curr - prev_loss) < tol\n",
    "        @info \"Converged at epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "        break\n",
    "    end\n",
    "    if epoch % 100 == 0\n",
    "        @info \"Epoch $epoch | loss = $(round(curr; sigdigits = 5))\"\n",
    "    end\n",
    "    prev_loss = curr\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "516b612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weight = Float32[-0.24433415 -0.4275184 … -0.6371743 -0.044091262; 0.098271295 -0.42824268 … -0.58313954 0.3422922; … ; -0.19983451 -0.17457394 … -0.32556838 0.08683801; -0.06578253 -0.61086965 … -0.4179089 0.016703814],)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W1 = ps.layer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33a5458c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16×20 Matrix{Float32}:\n",
       " -0.244334   -0.427518   -0.0216734  …   0.00456807  -0.637174   -0.0440913\n",
       "  0.0982713  -0.428243    0.0509947      0.168124    -0.58314     0.342292\n",
       "  0.0465844  -0.203886   -0.287295      -0.198544    -0.532725   -0.264628\n",
       "  0.120133   -0.0783193   0.40931        0.0826287   -0.346299    0.0647181\n",
       "  0.126306   -0.0285955  -0.132011       0.00921239  -0.313232   -0.700554\n",
       " -0.199879    0.285553   -0.129697   …  -0.0951012    0.514218   -0.157448\n",
       " -0.0688653   0.385883   -0.105749      -0.0143618    0.0325229   0.454902\n",
       "  0.30662     0.4856      0.34771       -0.011409     0.619588   -0.231679\n",
       "  0.0552083  -0.0506788  -0.0127704     -0.473215     0.518992   -0.2314\n",
       " -0.0107565   0.603927    0.0552842      0.209338     0.436884   -0.320865\n",
       "  0.12927     0.0790456   0.0801101  …  -0.0188691   -0.660306   -0.161895\n",
       " -0.112838   -0.15742    -0.0965732      0.134961     0.553193    0.398644\n",
       "  0.0624289   0.494245   -0.0175265      0.0325168   -0.281269    0.340725\n",
       "  0.0962787  -0.0111929  -0.172371       0.351136     0.0191042  -0.289802\n",
       " -0.199835   -0.174574   -0.325594       0.0925104   -0.325568    0.086838\n",
       " -0.0657825  -0.61087     0.248837   …  -0.0935028   -0.417909    0.0167038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W1 = ps.layer_1.weight      # first layer weight   (Float32 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e12414b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200-element Vector{Float32}:\n",
       "  0.32428136\n",
       " -0.22719732\n",
       "  0.328153\n",
       " -2.1337805\n",
       " -0.10493779\n",
       " -4.765954\n",
       "  0.82200533\n",
       "  3.0794828\n",
       "  0.44216186\n",
       " -0.48283583\n",
       "  ⋮\n",
       " -2.156866\n",
       " -1.5300952\n",
       "  2.3958976\n",
       " -2.9457557\n",
       " -1.0884045\n",
       "  0.14263847\n",
       "  0.7313972\n",
       " -0.48301864\n",
       " -1.6590878"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- fitted coefficients / predictions ----------------\n",
    "ŷ, _      = model(X′, ts.parameters, ts.states)\n",
    "y_pred_lux_adam  = ŷ[:] # analogue of `βhat_flux_adam`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e6ad15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||y_pred_adam - y_pred_lux_adam||₂ = 7.941e+01\n",
      "\n",
      "||y_pred_sgd - y_pred_lux_adam||₂ = 3.443e+01\n"
     ]
    }
   ],
   "source": [
    "# @printf(\"\\n norm(y_pred_adma -y_prd_lux_adma))\n",
    "@printf(\"\\n||y_pred_adam - y_pred_lux_adam||₂ = %.3e\\n\", norm(vec(y_pred_adam) - y_pred_lux_adam))\n",
    "@printf(\"\\n||y_pred_sgd - y_pred_lux_adam||₂ = %.3e\\n\", norm(vec(y_pred_sgd) - y_pred_lux_adam))\n",
    "\n",
    "# @printf(\"\\n||β̂_NN_adam - β̂_lux_adam||₂ = %.3e\\n\", norm(βhat_nn_adam-βhat_lux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fe1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.4",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
